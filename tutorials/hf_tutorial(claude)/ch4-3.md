# Hugging Face íŠœí† ë¦¬ì–¼ Part 4.3: Advanced RAG

## í•™ìŠµ ëª©í‘œ
- Cross-Encoder ê¸°ë°˜ Reranking ì´í•´ ë° êµ¬í˜„
- Hybrid Search (Dense + Sparse) ì „ëµ êµ¬ì¶•
- Self-RAG, Corrective RAG ë“± ìê¸° ê°œì„  RAG íŒ¨í„´
- Query Transformation ê¸°ë²• ì ìš©

---

## 1. Reranking (Cross-Encoder)

### 1.1 Bi-Encoder vs Cross-Encoder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Bi-Encoder (ì„ë² ë”© ëª¨ë¸)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query â”€â”€â–¶ [Encoder] â”€â”€â–¶ Vector â”€â”€â”                         â”‚
â”‚                                    â”œâ”€â”€â–¶ Cosine Similarity   â”‚
â”‚  Doc   â”€â”€â–¶ [Encoder] â”€â”€â–¶ Vector â”€â”€â”˜                         â”‚
â”‚                                                             â”‚
â”‚  âœ… ë¹ ë¦„ (ë…ë¦½ ì¸ì½”ë”©)                                        â”‚
â”‚  âœ… ëŒ€ê·œëª¨ ê²€ìƒ‰ ê°€ëŠ¥                                          â”‚
â”‚  âŒ ìƒí˜¸ ì‘ìš© ì œí•œ                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Cross-Encoder (Reranker)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Query, Doc] â”€â”€â–¶ [Encoder] â”€â”€â–¶ Relevance Score             â”‚
â”‚                                                             â”‚
â”‚  âœ… ë†’ì€ ì •í™•ë„ (Full Attention)                              â”‚
â”‚  âŒ ëŠë¦¼ (ëª¨ë“  ìŒ ê°œë³„ ì²˜ë¦¬)                                   â”‚
â”‚  âœ… Top-K ì¬ì •ë ¬ì— ì í•©                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| íŠ¹ì„± | Bi-Encoder | Cross-Encoder |
|------|------------|---------------|
| ì…ë ¥ | ê°œë³„ í…ìŠ¤íŠ¸ | í…ìŠ¤íŠ¸ ìŒ |
| ì¶œë ¥ | ë²¡í„° ì„ë² ë”© | ìœ ì‚¬ë„ ì ìˆ˜ |
| ì†ë„ | ë¹ ë¦„ | ëŠë¦¼ |
| ì •í™•ë„ | ë³´í†µ | ë†’ìŒ |
| í™•ì¥ì„± | ë†’ìŒ | ë‚®ìŒ |
| ìš©ë„ | ì´ˆê¸° ê²€ìƒ‰ | ì¬ì •ë ¬ (Reranking) |

### 1.2 2-Stage Retrieve & Rerank

```
Query â”€â”€â–¶ [Bi-Encoder] â”€â”€â–¶ Top-100 í›„ë³´ â”€â”€â–¶ [Cross-Encoder] â”€â”€â–¶ Top-5 ìµœì¢…
              (ë¹ ë¥¸ ê²€ìƒ‰)                        (ì •ë°€ ì¬ì •ë ¬)
```

### 1.3 ì„¤ì¹˜

```bash
pip install sentence-transformers FlagEmbedding
```

### 1.4 Sentence Transformers CrossEncoder

```python
from sentence_transformers import CrossEncoder

# Cross-Encoder ëª¨ë¸ ë¡œë“œ
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ì ìˆ˜ ê³„ì‚°
query = "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
documents = [
    "ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ í•™ìŠµê³¼ ì¶”ë¡ ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
]

# ì ìˆ˜ ì˜ˆì¸¡
pairs = [(query, doc) for doc in documents]
scores = reranker.predict(pairs)

print("Reranking ê²°ê³¼:")
for doc, score in sorted(zip(documents, scores), key=lambda x: x[1], reverse=True):
    print(f"  [{score:.4f}] {doc[:50]}...")
```

### 1.5 CrossEncoder.rank() ë©”ì„œë“œ

```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

query = "What is machine learning?"
passages = [
    "Machine learning is a subset of AI that enables systems to learn from data.",
    "The weather is nice today.",
    "Deep learning uses neural networks with multiple layers.",
    "Python is a popular programming language.",
]

# rank() ë©”ì„œë“œë¡œ ê°„í¸í•˜ê²Œ ì¬ì •ë ¬
results = reranker.rank(query, passages, return_documents=True, top_k=3)

print(f"Query: {query}\n")
for result in results:
    print(f"#{result['corpus_id']} (score: {result['score']:.4f}): {result['text']}")
```

### 1.6 ì¸ê¸° Reranker ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì–¸ì–´ | íŠ¹ì§• |
|------|---------|------|------|
| `cross-encoder/ms-marco-MiniLM-L-6-v2` | 22M | ì˜ì–´ | ê²½ëŸ‰, ë¹ ë¦„ |
| `cross-encoder/ms-marco-MiniLM-L-12-v2` | 33M | ì˜ì–´ | ê· í˜• |
| `BAAI/bge-reranker-base` | 278M | ë‹¤êµ­ì–´ | ê³ ì„±ëŠ¥ |
| `BAAI/bge-reranker-large` | 560M | ë‹¤êµ­ì–´ | ìµœê³  ì„±ëŠ¥ |
| `BAAI/bge-reranker-v2-m3` | 568M | ë‹¤êµ­ì–´ | ìµœì‹ , ë‹¤ê¸°ëŠ¥ |
| `jinaai/jina-reranker-v2-base-multilingual` | 278M | ë‹¤êµ­ì–´ | 1024 í† í° ì§€ì› |

### 1.7 BGE Reranker (FlagEmbedding)

```python
from FlagEmbedding import FlagReranker

# BGE Reranker ë¡œë“œ
reranker = FlagReranker(
    "BAAI/bge-reranker-v2-m3",
    use_fp16=True,  # FP16ìœ¼ë¡œ ì†ë„ í–¥ìƒ
)

# ì ìˆ˜ ê³„ì‚°
pairs = [
    ["ì¸ê³µì§€ëŠ¥ì´ë€?", "AIëŠ” ì¸ê°„ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤."],
    ["ì¸ê³µì§€ëŠ¥ì´ë€?", "ì˜¤ëŠ˜ ì ì‹¬ì€ ê¹€ì¹˜ì°Œê°œì…ë‹ˆë‹¤."],
]

scores = reranker.compute_score(pairs)
print(f"Scores: {scores}")

# 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (Sigmoid ì ìš©)
scores_normalized = reranker.compute_score(pairs, normalize=True)
print(f"Normalized Scores: {scores_normalized}")
```

### 1.8 RAGì— Reranking í†µí•©

```python
from sentence_transformers import SentenceTransformer, CrossEncoder
from typing import List, Tuple
import numpy as np


class RetrieveAndRerank:
    """2-Stage Retrieve & Rerank ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        embedding_model: str = "BAAI/bge-m3",
        reranker_model: str = "BAAI/bge-reranker-base",
    ):
        # Stage 1: Bi-Encoder (ë¹ ë¥¸ ê²€ìƒ‰)
        self.embedder = SentenceTransformer(embedding_model)
        
        # Stage 2: Cross-Encoder (ì •ë°€ ì¬ì •ë ¬)
        self.reranker = CrossEncoder(reranker_model)
        
        self.corpus = []
        self.corpus_embeddings = None
    
    def index(self, documents: List[str]) -> None:
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        self.corpus = documents
        self.corpus_embeddings = self.embedder.encode(
            documents,
            normalize_embeddings=True,
            show_progress_bar=True,
        )
    
    def search(
        self,
        query: str,
        top_k_retrieve: int = 20,
        top_k_rerank: int = 5,
    ) -> List[Tuple[str, float]]:
        """ê²€ìƒ‰ + ì¬ì •ë ¬"""
        # Stage 1: Bi-Encoderë¡œ Top-K í›„ë³´ ê²€ìƒ‰
        query_embedding = self.embedder.encode(
            query, normalize_embeddings=True
        )
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.dot(self.corpus_embeddings, query_embedding)
        top_indices = np.argsort(similarities)[::-1][:top_k_retrieve]
        
        # Stage 2: Cross-Encoderë¡œ ì¬ì •ë ¬
        candidates = [self.corpus[i] for i in top_indices]
        pairs = [(query, doc) for doc in candidates]
        rerank_scores = self.reranker.predict(pairs)
        
        # ì¬ì •ë ¬ëœ ê²°ê³¼
        reranked = sorted(
            zip(candidates, rerank_scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        return reranked[:top_k_rerank]


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rar = RetrieveAndRerank(
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        reranker_model="cross-encoder/ms-marco-MiniLM-L-6-v2",
    )
    
    # ë¬¸ì„œ ì¸ë±ì‹±
    documents = [
        "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” AI ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ ì–¸ì–´ë¥¼ ì´í•´í•˜ê²Œ í•©ë‹ˆë‹¤.",
        "ì»´í“¨í„° ë¹„ì „ì€ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤.",
        "ê°•í™”í•™ìŠµì€ ë³´ìƒì„ í†µí•´ ìµœì ì˜ í–‰ë™ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤.",
        "íŒŒì´ì¬ì€ AI ê°œë°œì— ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.",
        "íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” NLPì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.",
    ]
    rar.index(documents)
    
    # ê²€ìƒ‰
    query = "ë”¥ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    results = rar.search(query, top_k_retrieve=5, top_k_rerank=3)
    
    print(f"Query: {query}\n")
    for doc, score in results:
        print(f"  [{score:.4f}] {doc}")
```

---

## 2. Hybrid Search (Dense + Sparse)

### 2.1 ê²€ìƒ‰ ë°©ì‹ ë¹„êµ

| ë°©ì‹ | ì¥ì  | ë‹¨ì  |
|------|------|------|
| **Dense (Vector)** | ì˜ë¯¸ì  ìœ ì‚¬ì„± í¬ì°© | í‚¤ì›Œë“œ ì •í™•ë„ ë‚®ìŒ |
| **Sparse (BM25)** | í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­ | ë™ì˜ì–´/ì˜ë¯¸ ì´í•´ ë¶€ì¡± |
| **Hybrid** | ë‘˜ì˜ ì¥ì  ê²°í•© | êµ¬í˜„ ë³µì¡ë„ |

### 2.2 Hybrid Search ì•„í‚¤í…ì²˜

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Dense Search   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚         â”‚  (Vector DB)    â”‚        â”‚
          â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
Query â”€â”€â”€â”€â”¤                                    â”œâ”€â”€â–¶ Fusion â”€â”€â–¶ Results
          â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Sparse Search  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  (BM25)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 BM25 êµ¬í˜„

```python
from rank_bm25 import BM25Okapi
from typing import List, Tuple
import numpy as np


class BM25Retriever:
    """BM25 ê¸°ë°˜ Sparse Retriever"""
    
    def __init__(self):
        self.bm25 = None
        self.corpus = []
    
    def _tokenize(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €"""
        return text.lower().split()
    
    def fit(self, documents: List[str]) -> None:
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        self.corpus = documents
        tokenized_corpus = [self._tokenize(doc) for doc in documents]
        self.bm25 = BM25Okapi(tokenized_corpus)
    
    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """BM25 ê²€ìƒ‰"""
        tokenized_query = self._tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)
        
        # Top-K ì¸ë±ìŠ¤
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        return [(idx, scores[idx]) for idx in top_indices]


# ì‚¬ìš©
bm25 = BM25Retriever()
docs = [
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ í•™ìŠµí•©ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.",
]
bm25.fit(docs)

results = bm25.search("ì‹ ê²½ë§ ë”¥ëŸ¬ë‹", top_k=2)
for idx, score in results:
    print(f"[{score:.4f}] {docs[idx]}")
```

### 2.4 Reciprocal Rank Fusion (RRF)

```python
from typing import List, Dict, Tuple
from collections import defaultdict


def reciprocal_rank_fusion(
    rankings: List[List[Tuple[int, float]]],
    k: int = 60,
) -> List[Tuple[int, float]]:
    """
    RRFë¡œ ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ ë³‘í•©
    
    Args:
        rankings: ê° ê²€ìƒ‰ ë°©ì‹ì˜ (doc_id, score) ë¦¬ìŠ¤íŠ¸ë“¤
        k: RRF ìƒìˆ˜ (ê¸°ë³¸ 60)
    
    Returns:
        ë³‘í•©ëœ (doc_id, rrf_score) ë¦¬ìŠ¤íŠ¸
    """
    rrf_scores = defaultdict(float)
    
    for ranking in rankings:
        for rank, (doc_id, _) in enumerate(ranking):
            # RRF ê³µì‹: 1 / (k + rank)
            rrf_scores[doc_id] += 1.0 / (k + rank + 1)
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    sorted_results = sorted(
        rrf_scores.items(),
        key=lambda x: x[1],
        reverse=True
    )
    
    return sorted_results


# ì˜ˆì‹œ
dense_results = [(0, 0.95), (2, 0.85), (1, 0.80)]  # doc_id, score
sparse_results = [(1, 12.5), (0, 10.2), (3, 8.1)]

fused = reciprocal_rank_fusion([dense_results, sparse_results])
print("RRF Results:", fused)
```

### 2.5 ì™„ì „í•œ Hybrid Search êµ¬í˜„

```python
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from typing import List, Tuple, Dict
from collections import defaultdict
import numpy as np


class HybridSearcher:
    """Dense + Sparse Hybrid Search"""
    
    def __init__(
        self,
        embedding_model: str = "BAAI/bge-m3",
        dense_weight: float = 0.5,
        sparse_weight: float = 0.5,
    ):
        self.embedder = SentenceTransformer(embedding_model)
        self.dense_weight = dense_weight
        self.sparse_weight = sparse_weight
        
        self.corpus = []
        self.corpus_embeddings = None
        self.bm25 = None
    
    def _tokenize(self, text: str) -> List[str]:
        """í† í¬ë‚˜ì´ì €"""
        return text.lower().split()
    
    def index(self, documents: List[str]) -> None:
        """ë¬¸ì„œ ì¸ë±ì‹± (Dense + Sparse)"""
        self.corpus = documents
        
        # Dense: ì„ë² ë”© ìƒì„±
        print("Creating dense embeddings...")
        self.corpus_embeddings = self.embedder.encode(
            documents,
            normalize_embeddings=True,
            show_progress_bar=True,
        )
        
        # Sparse: BM25 ì¸ë±ìŠ¤ ìƒì„±
        print("Creating BM25 index...")
        tokenized = [self._tokenize(doc) for doc in documents]
        self.bm25 = BM25Okapi(tokenized)
    
    def _dense_search(self, query: str, top_k: int) -> List[Tuple[int, float]]:
        """Dense Vector Search"""
        query_embedding = self.embedder.encode(
            query, normalize_embeddings=True
        )
        scores = np.dot(self.corpus_embeddings, query_embedding)
        top_indices = np.argsort(scores)[::-1][:top_k]
        return [(idx, float(scores[idx])) for idx in top_indices]
    
    def _sparse_search(self, query: str, top_k: int) -> List[Tuple[int, float]]:
        """Sparse BM25 Search"""
        tokenized_query = self._tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)
        top_indices = np.argsort(scores)[::-1][:top_k]
        return [(idx, float(scores[idx])) for idx in top_indices]
    
    def _normalize_scores(
        self, results: List[Tuple[int, float]]
    ) -> List[Tuple[int, float]]:
        """ì ìˆ˜ ì •ê·œí™” (0-1)"""
        if not results:
            return results
        scores = [s for _, s in results]
        min_s, max_s = min(scores), max(scores)
        if max_s == min_s:
            return [(idx, 1.0) for idx, _ in results]
        return [
            (idx, (s - min_s) / (max_s - min_s))
            for idx, s in results
        ]
    
    def search(
        self,
        query: str,
        top_k: int = 10,
        fusion: str = "weighted",  # "weighted" or "rrf"
    ) -> List[Tuple[str, float, Dict]]:
        """Hybrid Search ì‹¤í–‰"""
        # ê° ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
        dense_results = self._dense_search(query, top_k * 2)
        sparse_results = self._sparse_search(query, top_k * 2)
        
        if fusion == "rrf":
            # RRF Fusion
            fused = self._rrf_fusion([dense_results, sparse_results])
        else:
            # Weighted Fusion
            fused = self._weighted_fusion(dense_results, sparse_results)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for doc_id, score in fused[:top_k]:
            results.append((
                self.corpus[doc_id],
                score,
                {"doc_id": doc_id}
            ))
        
        return results
    
    def _weighted_fusion(
        self,
        dense_results: List[Tuple[int, float]],
        sparse_results: List[Tuple[int, float]],
    ) -> List[Tuple[int, float]]:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ Fusion"""
        # ì •ê·œí™”
        dense_norm = self._normalize_scores(dense_results)
        sparse_norm = self._normalize_scores(sparse_results)
        
        # ì ìˆ˜ í•©ì‚°
        scores = defaultdict(float)
        for doc_id, score in dense_norm:
            scores[doc_id] += score * self.dense_weight
        for doc_id, score in sparse_norm:
            scores[doc_id] += score * self.sparse_weight
        
        return sorted(scores.items(), key=lambda x: x[1], reverse=True)
    
    def _rrf_fusion(
        self,
        rankings: List[List[Tuple[int, float]]],
        k: int = 60,
    ) -> List[Tuple[int, float]]:
        """RRF Fusion"""
        rrf_scores = defaultdict(float)
        for ranking in rankings:
            for rank, (doc_id, _) in enumerate(ranking):
                rrf_scores[doc_id] += 1.0 / (k + rank + 1)
        return sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # Hybrid Searcher ì´ˆê¸°í™”
    searcher = HybridSearcher(
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        dense_weight=0.6,
        sparse_weight=0.4,
    )
    
    # ë¬¸ì„œ ì¸ë±ì‹±
    documents = [
        "BERTëŠ” êµ¬ê¸€ì—ì„œ ê°œë°œí•œ ì‚¬ì „ í›ˆë ¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.",
        "GPTëŠ” OpenAIì—ì„œ ê°œë°œí•œ ìƒì„±í˜• AI ëª¨ë¸ì…ë‹ˆë‹¤.",
        "íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ NLPëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ëŠ” AI ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹ MLì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ DLì€ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
    ]
    searcher.index(documents)
    
    # Hybrid Search
    query = "GPT ì–¸ì–´ ëª¨ë¸"
    
    print(f"\nQuery: {query}")
    print("\n[Weighted Fusion]")
    for doc, score, meta in searcher.search(query, top_k=3, fusion="weighted"):
        print(f"  [{score:.4f}] {doc[:50]}...")
    
    print("\n[RRF Fusion]")
    for doc, score, meta in searcher.search(query, top_k=3, fusion="rrf"):
        print(f"  [{score:.4f}] {doc[:50]}...")
```

### 2.6 BGE-M3 Multi-Vector Retrieval

```python
from FlagEmbedding import BGEM3FlagModel

# BGE-M3: Dense + Sparse + ColBERT í†µí•© ëª¨ë¸
model = BGEM3FlagModel(
    "BAAI/bge-m3",
    use_fp16=True,
)

# ë¬¸ì„œ ì¸ì½”ë”© (Dense + Sparse ë™ì‹œ ìƒì„±)
sentences = [
    "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤.",
    "LLMì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.",
]

# return_dense=True, return_sparse=Trueë¡œ ë‘ ë²¡í„° ë™ì‹œ íšë“
embeddings = model.encode(
    sentences,
    return_dense=True,
    return_sparse=True,
)

print(f"Dense shape: {embeddings['dense_vecs'].shape}")
print(f"Sparse keys: {list(embeddings['lexical_weights'][0].keys())[:5]}...")
```

---

## 3. Self-RAG & Corrective RAG

### 3.1 Self-RAG ê°œìš”

- **ë…¼ë¬¸**: "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection" (ICLR 2024)
- **í•µì‹¬ ì•„ì´ë””ì–´**: LLMì´ ìŠ¤ìŠ¤ë¡œ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨, ìƒì„± í’ˆì§ˆ í‰ê°€

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Self-RAG ì›Œí¬í”Œë¡œìš°                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Query â”€â”€â–¶ [ê²€ìƒ‰ í•„ìš”?] â”€â”€â–¶ Yes â”€â”€â–¶ Retrieve â”€â”€â–¶ Generate   â”‚
â”‚                 â”‚                                           â”‚
â”‚                 â–¼ No                                        â”‚
â”‚            Direct Generate                                  â”‚
â”‚                                                             â”‚
â”‚  Generated â”€â”€â–¶ [í’ˆì§ˆ í‰ê°€] â”€â”€â–¶ [Relevant?] â”€â”€â–¶ Output      â”‚
â”‚                    â”‚                                        â”‚
â”‚                    â–¼ Low Quality                            â”‚
â”‚               Regenerate or Refine                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Corrective RAG (CRAG) ê°œìš”

- **í•µì‹¬**: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í’ˆì§ˆ í‰ê°€ í›„ ë³´ì • ì „ëµ ì ìš©
- **3ë‹¨ê³„ ì‹ ë¢°ë„**: Correct / Ambiguous / Incorrect

```
Retrieved Docs â”€â”€â–¶ [Relevance Grader] â”€â”€â”¬â”€â”€â–¶ Correct â”€â”€â–¶ Use docs
                                        â”‚
                                        â”œâ”€â”€â–¶ Ambiguous â”€â”€â–¶ Refine + Web Search
                                        â”‚
                                        â””â”€â”€â–¶ Incorrect â”€â”€â–¶ Web Search Only
```

### 3.3 Document Grader êµ¬í˜„

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFacePipeline
from pydantic import BaseModel, Field
from typing import Literal


class GradeResult(BaseModel):
    """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼"""
    score: Literal["relevant", "irrelevant"]
    reason: str = ""


def create_document_grader(llm):
    """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì²´ì¸ ìƒì„±"""
    
    system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
ì£¼ì–´ì§„ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
ë¬¸ì„œê°€ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë‚˜ ì˜ë¯¸ì™€ ê´€ë ¨ì´ ìˆìœ¼ë©´ 'relevant'ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
ì—„ê²©í•  í•„ìš” ì—†ì´, ê´€ë ¨ ì •ë³´ê°€ ì¡°ê¸ˆì´ë¼ë„ ìˆìœ¼ë©´ 'relevant'ì…ë‹ˆë‹¤.

ì˜¤ì§ 'relevant' ë˜ëŠ” 'irrelevant' ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•˜ì„¸ìš”."""

    grade_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", """ë¬¸ì„œ: {document}

ì§ˆë¬¸: {question}

í‰ê°€ (relevant/irrelevant):"""),
    ])
    
    chain = grade_prompt | llm | StrOutputParser()
    return chain


# ì‚¬ìš© ì˜ˆì‹œ
def grade_documents(grader, question: str, documents: list) -> list:
    """ë¬¸ì„œë“¤ì˜ ê´€ë ¨ì„± í‰ê°€"""
    relevant_docs = []
    
    for doc in documents:
        result = grader.invoke({
            "question": question,
            "document": doc,
        })
        
        if "relevant" in result.lower():
            relevant_docs.append(doc)
            print(f"âœ… RELEVANT: {doc[:50]}...")
        else:
            print(f"âŒ IRRELEVANT: {doc[:50]}...")
    
    return relevant_docs
```

### 3.4 Query Rewriter êµ¬í˜„

```python
def create_query_rewriter(llm):
    """ì¿¼ë¦¬ ì¬ì‘ì„± ì²´ì¸ ìƒì„±"""
    
    system_prompt = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë„ë¡ ì¬ì‘ì„±í•˜ì„¸ìš”.
- í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìœ ì§€í•˜ë©´ì„œ
- ê²€ìƒ‰ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜
- ë™ì˜ì–´ë‚˜ ê´€ë ¨ ìš©ì–´ ì¶”ê°€ ê°€ëŠ¥

ì¬ì‘ì„±ëœ ì¿¼ë¦¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

    rewrite_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", """ì›ë˜ ì§ˆë¬¸: {question}

ì¬ì‘ì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬:"""),
    ])
    
    chain = rewrite_prompt | llm | StrOutputParser()
    return chain


# ì‚¬ìš© ì˜ˆì‹œ
# rewriter = create_query_rewriter(llm)
# new_query = rewriter.invoke({"question": "RAGê°€ ë­ì•¼?"})
# print(new_query)  # "RAG Retrieval Augmented Generation ê²€ìƒ‰ ì¦ê°• ìƒì„± ì •ì˜"
```

### 3.5 Corrective RAG ì „ì²´ êµ¬í˜„

```python
from typing import List, Dict, Any, TypedDict
from enum import Enum


class DocumentGrade(Enum):
    CORRECT = "correct"
    AMBIGUOUS = "ambiguous"
    INCORRECT = "incorrect"


class CRAGState(TypedDict):
    """CRAG ìƒíƒœ"""
    question: str
    documents: List[str]
    grades: List[str]
    web_search_needed: bool
    rewritten_query: str
    final_documents: List[str]
    answer: str


class CorrectiveRAG:
    """Corrective RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, retriever, grader, rewriter, generator, web_searcher=None):
        self.retriever = retriever
        self.grader = grader
        self.rewriter = rewriter
        self.generator = generator
        self.web_searcher = web_searcher
    
    def retrieve(self, question: str) -> List[str]:
        """Step 1: ë¬¸ì„œ ê²€ìƒ‰"""
        docs = self.retriever.invoke(question)
        return [doc.page_content for doc in docs]
    
    def grade_documents(self, question: str, documents: List[str]) -> Dict[str, Any]:
        """Step 2: ë¬¸ì„œ í‰ê°€"""
        relevant_docs = []
        irrelevant_count = 0
        
        for doc in documents:
            result = self.grader.invoke({
                "question": question,
                "document": doc,
            })
            
            if "relevant" in result.lower():
                relevant_docs.append(doc)
            else:
                irrelevant_count += 1
        
        # í‰ê°€ ê²°ê³¼ íŒë‹¨
        if len(relevant_docs) == len(documents):
            grade = DocumentGrade.CORRECT
        elif len(relevant_docs) > 0:
            grade = DocumentGrade.AMBIGUOUS
        else:
            grade = DocumentGrade.INCORRECT
        
        return {
            "grade": grade,
            "relevant_docs": relevant_docs,
            "irrelevant_count": irrelevant_count,
        }
    
    def rewrite_query(self, question: str) -> str:
        """Step 3: ì¿¼ë¦¬ ì¬ì‘ì„±"""
        return self.rewriter.invoke({"question": question})
    
    def web_search(self, query: str) -> List[str]:
        """Step 4: ì›¹ ê²€ìƒ‰ (í•„ìš”ì‹œ)"""
        if self.web_searcher:
            results = self.web_searcher.invoke(query)
            return [r["content"] for r in results]
        return []
    
    def generate(self, question: str, documents: List[str]) -> str:
        """Step 5: ë‹µë³€ ìƒì„±"""
        context = "\n\n".join(documents)
        return self.generator.invoke({
            "question": question,
            "context": context,
        })
    
    def __call__(self, question: str) -> Dict[str, Any]:
        """CRAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print(f"ğŸ“ Question: {question}\n")
        
        # Step 1: ê²€ìƒ‰
        print("ğŸ” Step 1: Retrieving documents...")
        documents = self.retrieve(question)
        print(f"   Retrieved {len(documents)} documents")
        
        # Step 2: í‰ê°€
        print("\nğŸ“Š Step 2: Grading documents...")
        grade_result = self.grade_documents(question, documents)
        print(f"   Grade: {grade_result['grade'].value}")
        print(f"   Relevant: {len(grade_result['relevant_docs'])}/{len(documents)}")
        
        final_docs = grade_result["relevant_docs"]
        
        # Step 3-4: í•„ìš”ì‹œ ì¿¼ë¦¬ ì¬ì‘ì„± & ì›¹ ê²€ìƒ‰
        if grade_result["grade"] in [DocumentGrade.AMBIGUOUS, DocumentGrade.INCORRECT]:
            print("\nğŸ”„ Step 3: Rewriting query...")
            new_query = self.rewrite_query(question)
            print(f"   Original: {question}")
            print(f"   Rewritten: {new_query}")
            
            if self.web_searcher and grade_result["grade"] == DocumentGrade.INCORRECT:
                print("\nğŸŒ Step 4: Web searching...")
                web_docs = self.web_search(new_query)
                final_docs.extend(web_docs)
                print(f"   Found {len(web_docs)} web results")
        
        # Step 5: ìƒì„±
        print("\nâœ¨ Step 5: Generating answer...")
        answer = self.generate(question, final_docs)
        
        return {
            "question": question,
            "grade": grade_result["grade"].value,
            "num_relevant_docs": len(grade_result["relevant_docs"]),
            "final_docs": final_docs,
            "answer": answer,
        }
```

### 3.6 LangGraph ê¸°ë°˜ CRAG (ì„ íƒì )

```python
# LangGraphë¥¼ ì‚¬ìš©í•œ CRAG ì›Œí¬í”Œë¡œìš° (ê°œë… ì½”ë“œ)
"""
from langgraph.graph import StateGraph, END

# ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    question: str
    documents: List[str]
    grade: str
    generation: str
    web_search: bool

# ë…¸ë“œ í•¨ìˆ˜ë“¤
def retrieve(state):
    docs = retriever.invoke(state["question"])
    return {"documents": docs}

def grade_documents(state):
    # ë¬¸ì„œ í‰ê°€ ë¡œì§
    ...
    return {"grade": grade, "web_search": need_search}

def transform_query(state):
    new_query = rewriter.invoke(state["question"])
    return {"question": new_query}

def web_search(state):
    docs = web_tool.invoke(state["question"])
    return {"documents": docs}

def generate(state):
    answer = generator.invoke(state)
    return {"generation": answer}

# ì¡°ê±´ë¶€ ì—£ì§€
def decide_to_generate(state):
    if state["web_search"]:
        return "transform_query"
    return "generate"

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(GraphState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("transform_query", transform_query)
workflow.add_node("web_search", web_search)
workflow.add_node("generate", generate)

workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate",
    }
)
workflow.add_edge("transform_query", "web_search")
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", END)

app = workflow.compile()
"""
```

---

## 4. Query Transformation ê¸°ë²•

### 4.1 Query Transformation ì¢…ë¥˜

| ê¸°ë²• | ì„¤ëª… | ìš©ë„ |
|------|------|------|
| **Query Rewriting** | ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ì¬ì‘ì„± | ëª¨í˜¸í•œ ì¿¼ë¦¬ ê°œì„  |
| **Query Expansion** | ë™ì˜ì–´, ê´€ë ¨ ìš©ì–´ ì¶”ê°€ | ê²€ìƒ‰ ë²”ìœ„ í™•ì¥ |
| **Query Decomposition** | ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ í•˜ìœ„ ì¿¼ë¦¬ë¡œ ë¶„í•´ | Multi-hop QA |
| **HyDE** | ê°€ìƒ ë‹µë³€ ìƒì„± í›„ ê²€ìƒ‰ | ì˜ë¯¸ ê°­ í•´ì†Œ |
| **Step-back Prompting** | ì¶”ìƒì  ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜ | ë§¥ë½ í™•ì¥ |

### 4.2 Query Expansion

```python
def create_query_expander(llm):
    """ì¿¼ë¦¬ í™•ì¥ ì²´ì¸"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ì£¼ì–´ì§„ ê²€ìƒ‰ ì¿¼ë¦¬ì— ê´€ë ¨ í‚¤ì›Œë“œì™€ ë™ì˜ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ í™•ì¥í•˜ì„¸ìš”.
ì›ë˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„“íˆëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹: í™•ì¥ëœ ì¿¼ë¦¬ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œë“¤)"""),
        ("human", "ì›ë˜ ì¿¼ë¦¬: {query}\n\ní™•ì¥ëœ ì¿¼ë¦¬:"),
    ])
    
    return prompt | llm | StrOutputParser()


# ì˜ˆì‹œ
# expander = create_query_expander(llm)
# expanded = expander.invoke({"query": "RAG ì‹œìŠ¤í…œ"})
# print(expanded)  # "RAG ê²€ìƒ‰ ì¦ê°• ìƒì„± Retrieval Augmented Generation LLM ì‹œìŠ¤í…œ"
```

### 4.3 Query Decomposition (Multi-Query)

```python
def create_query_decomposer(llm):
    """ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ í•˜ìœ„ ì¿¼ë¦¬ë¡œ ë¶„í•´"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë³µì¡í•œ ì§ˆë¬¸ì„ ë‹µë³€ì— í•„ìš”í•œ í•˜ìœ„ ì§ˆë¬¸ë“¤ë¡œ ë¶„í•´í•˜ì„¸ìš”.
ê° í•˜ìœ„ ì§ˆë¬¸ì€ ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹:
1. í•˜ìœ„ ì§ˆë¬¸ 1
2. í•˜ìœ„ ì§ˆë¬¸ 2
3. í•˜ìœ„ ì§ˆë¬¸ 3"""),
        ("human", "ì›ë˜ ì§ˆë¬¸: {question}\n\ní•˜ìœ„ ì§ˆë¬¸ë“¤:"),
    ])
    
    return prompt | llm | StrOutputParser()


def parse_sub_queries(output: str) -> List[str]:
    """í•˜ìœ„ ì¿¼ë¦¬ íŒŒì‹±"""
    lines = output.strip().split("\n")
    queries = []
    for line in lines:
        # "1. ì§ˆë¬¸" í˜•ì‹ì—ì„œ ì§ˆë¬¸ ì¶”ì¶œ
        if line.strip():
            query = line.split(".", 1)[-1].strip()
            if query:
                queries.append(query)
    return queries


# ì‚¬ìš© ì˜ˆì‹œ
"""
question = "RAGì™€ Fine-tuningì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì´ê³ , ê°ê° ì–¸ì œ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?"

decomposer = create_query_decomposer(llm)
output = decomposer.invoke({"question": question})
sub_queries = parse_sub_queries(output)

# sub_queries:
# - "RAGë€ ë¬´ì—‡ì¸ê°€?"
# - "Fine-tuningì´ë€ ë¬´ì—‡ì¸ê°€?"
# - "RAGì˜ ì¥ë‹¨ì ì€?"
# - "Fine-tuningì˜ ì¥ë‹¨ì ì€?"
# - "RAGë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ìƒí™©ì€?"
# - "Fine-tuningì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ìƒí™©ì€?"

# ê° í•˜ìœ„ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ í›„ ê²°ê³¼ í†µí•©
all_docs = []
for sub_q in sub_queries:
    docs = retriever.invoke(sub_q)
    all_docs.extend(docs)
"""
```

### 4.4 HyDE (Hypothetical Document Embeddings)

```python
def create_hyde_generator(llm):
    """ê°€ìƒ ë‹µë³€ ìƒì„±ê¸°"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ì§ˆë¬¸ì— ëŒ€í•œ ê°€ìƒì˜ ë‹µë³€ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ì‹¤ì œ ì •í™•í•œ ì •ë³´ê°€ ì•„ë‹ˆì–´ë„ ë©ë‹ˆë‹¤.
ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ê°€ê°€ ì‘ì„±í•  ë²•í•œ ë¬¸ì„œ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”."""),
        ("human", "ì§ˆë¬¸: {question}\n\nê°€ìƒ ë‹µë³€ ë¬¸ì„œ:"),
    ])
    
    return prompt | llm | StrOutputParser()


class HyDERetriever:
    """HyDE ê¸°ë°˜ Retriever"""
    
    def __init__(self, llm, embedder, vectorstore):
        self.hyde_generator = create_hyde_generator(llm)
        self.embedder = embedder
        self.vectorstore = vectorstore
    
    def retrieve(self, question: str, top_k: int = 5) -> List:
        # Step 1: ê°€ìƒ ë‹µë³€ ìƒì„±
        hypothetical_doc = self.hyde_generator.invoke({"question": question})
        print(f"Generated hypothetical document:\n{hypothetical_doc[:200]}...")
        
        # Step 2: ê°€ìƒ ë‹µë³€ìœ¼ë¡œ ê²€ìƒ‰ (ë” ë‚˜ì€ ì˜ë¯¸ ë§¤ì¹­)
        docs = self.vectorstore.similarity_search(hypothetical_doc, k=top_k)
        
        return docs
```

### 4.5 Step-back Prompting

```python
def create_stepback_transformer(llm):
    """Step-back ì§ˆë¬¸ ìƒì„±ê¸°"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ì£¼ì–´ì§„ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì—ì„œ í•œ ë‹¨ê³„ ë’¤ë¡œ ë¬¼ëŸ¬ë‚˜,
ë” ë„“ì€ ë§¥ë½ì˜ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”.

ì˜ˆì‹œ:
- ì›ë˜: "Python 3.9ì—ì„œ match ë¬¸ë²•ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?"
- Step-back: "Pythonì˜ íŒ¨í„´ ë§¤ì¹­ ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."

- ì›ë˜: "GPT-4ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
- Step-back: "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë€ ë¬´ì—‡ì¸ê°€ìš”?"
"""),
        ("human", "ì›ë˜ ì§ˆë¬¸: {question}\n\nStep-back ì§ˆë¬¸:"),
    ])
    
    return prompt | llm | StrOutputParser()


# ì‚¬ìš©
"""
stepback = create_stepback_transformer(llm)
original = "Qwen2.5-7B ëª¨ë¸ì˜ ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ëŠ”?"
broader = stepback.invoke({"question": original})
# "ì˜¤í”ˆì†ŒìŠ¤ LLMì˜ ì„±ëŠ¥ í‰ê°€ ë°©ë²•ê³¼ ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

# ë‘ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ ê²°í•©
docs1 = retriever.invoke(original)  # êµ¬ì²´ì  ì •ë³´
docs2 = retriever.invoke(broader)   # ë°°ê²½ ì§€ì‹
all_docs = docs1 + docs2
"""
```

---

## 5. í†µí•© Advanced RAG ì‹œìŠ¤í…œ

### 5.1 ì „ì²´ ì•„í‚¤í…ì²˜

```python
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum


@dataclass
class RAGConfig:
    """RAG ì„¤ì •"""
    # Retrieval
    top_k_retrieve: int = 20
    top_k_rerank: int = 5
    use_hybrid: bool = True
    dense_weight: float = 0.6
    
    # Reranking
    use_reranker: bool = True
    reranker_model: str = "BAAI/bge-reranker-base"
    
    # Corrective
    use_correction: bool = True
    relevance_threshold: float = 0.5
    
    # Query Transform
    use_query_rewrite: bool = True
    use_hyde: bool = False


class AdvancedRAG:
    """í†µí•© Advanced RAG ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        embedding_model: str,
        llm,
        config: RAGConfig = None,
    ):
        from sentence_transformers import SentenceTransformer, CrossEncoder
        
        self.config = config or RAGConfig()
        self.llm = llm
        
        # Embedding & Reranker
        self.embedder = SentenceTransformer(embedding_model)
        if self.config.use_reranker:
            self.reranker = CrossEncoder(self.config.reranker_model)
        
        # Components
        self.corpus = []
        self.corpus_embeddings = None
        self.bm25 = None
        
        # Chains
        self._setup_chains()
    
    def _setup_chains(self):
        """LLM ì²´ì¸ ì„¤ì •"""
        # Document Grader
        self.grader = create_document_grader(self.llm)
        
        # Query Rewriter
        if self.config.use_query_rewrite:
            self.rewriter = create_query_rewriter(self.llm)
        
        # HyDE Generator
        if self.config.use_hyde:
            self.hyde = create_hyde_generator(self.llm)
    
    def index(self, documents: List[str]) -> None:
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # Dense embeddings
        self.corpus_embeddings = self.embedder.encode(
            documents,
            normalize_embeddings=True,
            show_progress_bar=True,
        )
        
        # Sparse (BM25)
        if self.config.use_hybrid:
            tokenized = [doc.lower().split() for doc in documents]
            self.bm25 = BM25Okapi(tokenized)
    
    def _retrieve(self, query: str) -> List[Dict]:
        """ê²€ìƒ‰ (Hybrid + Rerank)"""
        import numpy as np
        
        # Dense search
        query_emb = self.embedder.encode(query, normalize_embeddings=True)
        dense_scores = np.dot(self.corpus_embeddings, query_emb)
        
        # Hybrid: combine with BM25
        if self.config.use_hybrid and self.bm25:
            sparse_scores = self.bm25.get_scores(query.lower().split())
            
            # Normalize and combine
            dense_norm = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min() + 1e-8)
            sparse_norm = (sparse_scores - sparse_scores.min()) / (sparse_scores.max() - sparse_scores.min() + 1e-8)
            
            combined_scores = (
                self.config.dense_weight * dense_norm +
                (1 - self.config.dense_weight) * sparse_norm
            )
        else:
            combined_scores = dense_scores
        
        # Top-K candidates
        top_indices = np.argsort(combined_scores)[::-1][:self.config.top_k_retrieve]
        candidates = [
            {"content": self.corpus[i], "score": float(combined_scores[i]), "idx": i}
            for i in top_indices
        ]
        
        # Rerank
        if self.config.use_reranker:
            pairs = [(query, c["content"]) for c in candidates]
            rerank_scores = self.reranker.predict(pairs)
            
            for c, score in zip(candidates, rerank_scores):
                c["rerank_score"] = float(score)
            
            candidates.sort(key=lambda x: x["rerank_score"], reverse=True)
        
        return candidates[:self.config.top_k_rerank]
    
    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆì˜ ì‘ë‹µ"""
        result = {
            "question": question,
            "steps": [],
        }
        
        # Step 1: Query transformation (optional)
        search_query = question
        if self.config.use_query_rewrite:
            search_query = self.rewriter.invoke({"question": question})
            result["steps"].append({
                "step": "query_rewrite",
                "original": question,
                "rewritten": search_query,
            })
        
        # Step 2: Retrieve
        candidates = self._retrieve(search_query)
        result["steps"].append({
            "step": "retrieve",
            "num_candidates": len(candidates),
        })
        
        # Step 3: Corrective grading (optional)
        if self.config.use_correction:
            relevant_docs = []
            for c in candidates:
                grade = self.grader.invoke({
                    "question": question,
                    "document": c["content"],
                })
                c["relevant"] = "relevant" in grade.lower()
                if c["relevant"]:
                    relevant_docs.append(c)
            
            result["steps"].append({
                "step": "grade",
                "relevant_count": len(relevant_docs),
                "total_count": len(candidates),
            })
            
            # Use only relevant docs
            final_docs = relevant_docs if relevant_docs else candidates[:3]
        else:
            final_docs = candidates
        
        # Step 4: Generate
        context = "\n\n".join([d["content"] for d in final_docs])
        
        gen_prompt = f"""ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ë§¥:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        answer = self.llm.invoke(gen_prompt)
        
        result["context"] = [d["content"] for d in final_docs]
        result["answer"] = answer
        
        return result
```

---

## 6. ì‹¤ìŠµ ê³¼ì œ

### ê³¼ì œ 1: Retrieve & Rerank êµ¬í˜„
1. `sentence-transformers/all-MiniLM-L6-v2`ë¡œ ë¬¸ì„œ ì„ë² ë”©
2. `cross-encoder/ms-marco-MiniLM-L-6-v2`ë¡œ ì¬ì •ë ¬
3. Top-50 ê²€ìƒ‰ â†’ Top-5 ì¬ì •ë ¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
4. Reranking ì „í›„ ì •í™•ë„ ë¹„êµ

### ê³¼ì œ 2: Hybrid Search ì‹¤í—˜
1. Dense-only, Sparse-only, Hybrid ê²€ìƒ‰ êµ¬í˜„
2. ë™ì¼ ì¿¼ë¦¬ì…‹ìœ¼ë¡œ ê° ë°©ì‹ ê²€ìƒ‰
3. RRFì™€ Weighted Fusion ë¹„êµ
4. ê²°ê³¼ ë¶„ì„ ë° ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰

### ê³¼ì œ 3: Corrective RAG íŒŒì´í”„ë¼ì¸
1. Document Grader êµ¬í˜„
2. Query Rewriter êµ¬í˜„
3. ë‚®ì€ í’ˆì§ˆ ë¬¸ì„œ ê°ì§€ ì‹œ ì¬ê²€ìƒ‰ ë¡œì§
4. ì „ì²´ CRAG ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸

---

## 7. ìš”ì•½ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Reranking
- [ ] Bi-Encoder vs Cross-Encoder ì°¨ì´ ì´í•´
- [ ] CrossEncoder ì‚¬ìš©ë²• (predict, rank)
- [ ] BGE Reranker í™œìš©
- [ ] 2-Stage Retrieve & Rerank êµ¬í˜„

### Hybrid Search
- [ ] Dense + Sparse ê²€ìƒ‰ ê²°í•©
- [ ] BM25 êµ¬í˜„
- [ ] RRF (Reciprocal Rank Fusion)
- [ ] Weighted Score Fusion

### Self-RAG & Corrective RAG
- [ ] Document Grading
- [ ] Query Rewriting
- [ ] Corrective ì›Œí¬í”Œë¡œìš°
- [ ] LangGraph ê¸°ë°˜ êµ¬í˜„ (ì„ íƒ)

### Query Transformation
- [ ] Query Expansion
- [ ] Query Decomposition
- [ ] HyDE
- [ ] Step-back Prompting

---

## ì°¸ê³  ìë£Œ

### Reranking
- **Sentence Transformers Cross-Encoder**: https://www.sbert.net/examples/cross_encoder/applications/README.html
- **BGE Reranker**: https://huggingface.co/BAAI/bge-reranker-v2-m3
- **Training Rerankers (v4)**: https://huggingface.co/blog/train-reranker

### Hybrid Search
- **BGE-M3 Multi-Vector**: https://huggingface.co/BAAI/bge-m3
- **Weaviate Hybrid Search**: https://weaviate.io/blog/hybrid-search-explained
- **VectorHub Hybrid Search**: https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking

### Advanced RAG
- **Self-RAG Paper**: https://arxiv.org/abs/2310.11511
- **Corrective RAG (LangGraph)**: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/
- **RAG Survey**: https://www.promptingguide.ai/research/rag

---

## ë‹¤ìŒ ë‹¨ê³„

**Part 5.1: HF Agents/smolagents**ì—ì„œ ë‹¤ë£° ë‚´ìš©:
- Tool ì •ì˜ ë° ì‚¬ìš©
- CodeAgent vs ToolCallingAgent
- RAGë¥¼ Agentë¡œ í™•ì¥