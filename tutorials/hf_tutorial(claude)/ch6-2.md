# Hugging Face íŠœí† ë¦¬ì–¼ Part 6.2: MLOps & Deployment

## í•™ìŠµ ëª©í‘œ
- HuggingFace Inference Endpoints ë°°í¬
- Text Generation Inference (TGI) ì„¤ì •
- Docker ì»¨í…Œì´ë„ˆí™” ë° Kubernetes ë°°í¬
- CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (GitHub Actions)
- í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

---

## 1. HuggingFace Inference Endpoints

### 1.1 Inference Endpoints ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            HuggingFace Inference Endpoints                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   [HF Hub Model] â”€â”€â–¶ [ë°°í¬ ì„¤ì •] â”€â”€â–¶ [API Endpoint]        â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  â€¢ í´ë¼ìš°ë“œ: AWS / Azure / GCP                  â”‚      â”‚
â”‚   â”‚  â€¢ ì¶”ë¡  ì—”ì§„: vLLM / TGI / SGLang / Custom     â”‚      â”‚
â”‚   â”‚  â€¢ í•˜ë“œì›¨ì–´: CPU / GPU (T4, L4, A10G, A100)    â”‚      â”‚
â”‚   â”‚  â€¢ ë³´ì•ˆ: Public / Protected / Private          â”‚      â”‚
â”‚   â”‚  â€¢ ì˜¤í† ìŠ¤ì¼€ì¼ë§: 0-N ì¸ìŠ¤í„´ìŠ¤                  â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚   ì¥ì :                                                     â”‚
â”‚   â€¢ ì¸í”„ë¼ ê´€ë¦¬ ë¶ˆí•„ìš”                                      â”‚
â”‚   â€¢ Scale-to-Zero (ë¹„ìš© ì ˆê°)                              â”‚
â”‚   â€¢ ìë™ ëª¨ë¸ ìµœì í™”                                        â”‚
â”‚   â€¢ SOC2 Type 2 ì¸ì¦                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 UIë¡œ Endpoint ìƒì„±

1. **HuggingFace Hub ì ‘ì†**: https://huggingface.co
2. **ëª¨ë¸ í˜ì´ì§€** â†’ **Deploy** â†’ **Inference Endpoints** í´ë¦­
3. **ì„¤ì •**:
   - Endpoint Name: `my-llm-endpoint`
   - Cloud Provider: AWS / Azure
   - Region: `us-east-1` (ë°ì´í„° ìœ„ì¹˜ ê³ ë ¤)
   - Instance Type: `GPU [small] - 1x Nvidia T4`
   - Container Type: `vLLM` (ê¶Œì¥)
4. **Security Level**:
   - `Protected`: í† í° ì¸ì¦ í•„ìš” (ê¶Œì¥)
   - `Public`: ì¸ì¦ ì—†ì´ ì ‘ê·¼
   - `Private`: VPC ë‚´ë¶€ì—ì„œë§Œ ì ‘ê·¼
5. **Create Endpoint** í´ë¦­

### 1.3 Python SDKë¡œ Endpoint ìƒì„±

```python
from huggingface_hub import create_inference_endpoint

# Endpoint ìƒì„±
endpoint = create_inference_endpoint(
    name="llama-8b-endpoint",
    repository="meta-llama/Llama-3.1-8B-Instruct",
    framework="pytorch",
    task="text-generation",
    accelerator="gpu",
    instance_size="x1",  # 1x GPU
    instance_type="nvidia-l4",
    region="us-east-1",
    vendor="aws",
    type="protected",  # ë³´ì•ˆ ë ˆë²¨
    custom_image={
        "health_route": "/health",
        "env": {
            "MAX_INPUT_LENGTH": "4096",
            "MAX_TOTAL_TOKENS": "8192",
        },
        "url": "ghcr.io/huggingface/text-generation-inference:latest",
    },
)

# ìƒíƒœ í™•ì¸ (ë°°í¬ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°)
endpoint.wait()
print(f"Endpoint URL: {endpoint.url}")
print(f"Status: {endpoint.status}")
```

### 1.4 Endpoint ì‚¬ìš©

**InferenceClient ì‚¬ìš©:**

```python
from huggingface_hub import InferenceClient

# Endpointì— ì—°ê²°
client = InferenceClient(
    model="https://xxx.us-east-1.aws.endpoints.huggingface.cloud",
    token="hf_xxxxx",  # HF í† í°
)

# í…ìŠ¤íŠ¸ ìƒì„±
response = client.text_generation(
    prompt="What is machine learning?",
    max_new_tokens=100,
    temperature=0.7,
)
print(response)

# Chat Completion (OpenAI í˜¸í™˜)
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"},
]

response = client.chat_completion(
    messages=messages,
    max_tokens=100,
)
print(response.choices[0].message.content)
```

**OpenAI SDK ì‚¬ìš©:**

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://xxx.us-east-1.aws.endpoints.huggingface.cloud/v1",
    api_key="hf_xxxxx",  # HF í† í°
)

response = client.chat.completions.create(
    model="tgi",  # ëª¨ë¸ ì´ë¦„ (TGI/vLLMì—ì„œëŠ” ë¬´ì‹œë¨)
    messages=[
        {"role": "user", "content": "Explain quantum computing."}
    ],
    max_tokens=200,
    stream=True,
)

for chunk in response:
    print(chunk.choices[0].delta.content or "", end="")
```

### 1.5 Endpoint ê´€ë¦¬

```python
from huggingface_hub import (
    get_inference_endpoint,
    list_inference_endpoints,
    pause_inference_endpoint,
    resume_inference_endpoint,
    delete_inference_endpoint,
)

# ëª¨ë“  Endpoint ì¡°íšŒ
endpoints = list_inference_endpoints()
for ep in endpoints:
    print(f"{ep.name}: {ep.status}")

# íŠ¹ì • Endpoint ì¡°íšŒ
endpoint = get_inference_endpoint("llama-8b-endpoint")

# ì¼ì‹œì •ì§€ (ë¹„ìš© ì ˆê°)
pause_inference_endpoint("llama-8b-endpoint")

# ì¬ê°œ
resume_inference_endpoint("llama-8b-endpoint")

# ì‚­ì œ
delete_inference_endpoint("llama-8b-endpoint")
```

---

## 2. Text Generation Inference (TGI)

> âš ï¸ **ì°¸ê³ **: TGIëŠ” 2025ë…„ 12ì›”ë¶€í„° ìœ ì§€ë³´ìˆ˜ ëª¨ë“œ. ìƒˆ í”„ë¡œì íŠ¸ëŠ” **vLLM** ë˜ëŠ” **SGLang** ê¶Œì¥.

### 2.1 TGI ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TGI íŠ¹ì§•                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   â€¢ Flash Attention + Paged Attention                      â”‚
â”‚   â€¢ Continuous Batching                                     â”‚
â”‚   â€¢ Tensor Parallelism (ë‹¤ì¤‘ GPU)                          â”‚
â”‚   â€¢ Token Streaming                                         â”‚
â”‚   â€¢ GPTQ / AWQ / EETQ ì–‘ìí™” ì§€ì›                          â”‚
â”‚   â€¢ Guidance (JSON/Grammar ì¶œë ¥ ê°•ì œ)                      â”‚
â”‚   â€¢ OpenAI í˜¸í™˜ API                                        â”‚
â”‚                                                             â”‚
â”‚   ì§€ì› ëª¨ë¸:                                                â”‚
â”‚   Llama, Mistral, Falcon, StarCoder, BLOOM, T5, ë“±        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Dockerë¡œ TGI ì‹¤í–‰

```bash
# ê¸°ë³¸ ì‹¤í–‰
model=meta-llama/Llama-3.1-8B-Instruct
volume=$PWD/data
token=<your_hf_token>

docker run --gpus all --shm-size 1g \
    -p 8080:80 \
    -v $volume:/data \
    -e HF_TOKEN=$token \
    ghcr.io/huggingface/text-generation-inference:3.3.5 \
    --model-id $model

# ì–‘ìí™” ëª¨ë¸ ì‹¤í–‰
docker run --gpus all --shm-size 1g \
    -p 8080:80 \
    -v $volume:/data \
    -e HF_TOKEN=$token \
    ghcr.io/huggingface/text-generation-inference:3.3.5 \
    --model-id TheBloke/Llama-2-7B-GPTQ \
    --quantize gptq

# ë‹¤ì¤‘ GPU (Tensor Parallelism)
docker run --gpus all --shm-size 1g \
    -p 8080:80 \
    -v $volume:/data \
    -e HF_TOKEN=$token \
    ghcr.io/huggingface/text-generation-inference:3.3.5 \
    --model-id meta-llama/Llama-3.1-70B-Instruct \
    --num-shard 4  # 4ê°œ GPU ì‚¬ìš©
```

### 2.3 TGI ì„¤ì • ì˜µì…˜

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--model-id` | HF Hub ëª¨ë¸ ID | í•„ìˆ˜ |
| `--quantize` | ì–‘ìí™” ë°©ë²• (gptq, awq, eetq, bitsandbytes) | None |
| `--num-shard` | Tensor Parallelism GPU ìˆ˜ | 1 |
| `--max-input-length` | ìµœëŒ€ ì…ë ¥ í† í° | 1024 |
| `--max-total-tokens` | ìµœëŒ€ ì´ í† í° (ì…ë ¥+ì¶œë ¥) | 2048 |
| `--max-batch-prefill-tokens` | Prefill ë°°ì¹˜ ìµœëŒ€ í† í° | 4096 |
| `--max-concurrent-requests` | ìµœëŒ€ ë™ì‹œ ìš”ì²­ | 128 |
| `--dtype` | ë°ì´í„° íƒ€ì… (float16, bfloat16) | auto |

### 2.4 TGI API ì‚¬ìš©

**Generate Endpoint:**

```python
import requests

API_URL = "http://localhost:8080/generate"
headers = {"Content-Type": "application/json"}

data = {
    "inputs": "What is the capital of France?",
    "parameters": {
        "max_new_tokens": 50,
        "temperature": 0.7,
        "top_p": 0.9,
        "do_sample": True,
    }
}

response = requests.post(API_URL, headers=headers, json=data)
print(response.json()["generated_text"])
```

**Chat Completion (OpenAI í˜¸í™˜):**

```bash
curl http://localhost:8080/v1/chat/completions \
    -X POST \
    -H "Content-Type: application/json" \
    -d '{
        "model": "tgi",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ],
        "max_tokens": 100,
        "stream": true
    }'
```

**Streaming:**

```python
from huggingface_hub import InferenceClient

client = InferenceClient(model="http://localhost:8080")

for token in client.text_generation(
    "Once upon a time",
    max_new_tokens=100,
    stream=True,
):
    print(token, end="", flush=True)
```

---

## 3. Docker ì»¨í…Œì´ë„ˆí™”

### 3.1 LLM ì¶”ë¡  ì„œë²„ Dockerfile

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„±
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„±
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY src/ ./src/
COPY models/ ./models/

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8080

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s \
    CMD curl -f http://localhost:8080/health || exit 1

# ì‹¤í–‰
CMD ["python", "src/serve.py"]
```

**requirements.txt:**

```text
transformers>=4.40.0
torch>=2.0.0
accelerate>=0.27.0
fastapi>=0.110.0
uvicorn>=0.27.0
pydantic>=2.0.0
```

### 3.2 FastAPI ì¶”ë¡  ì„œë²„

```python
# src/serve.py
import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer
from contextlib import asynccontextmanager

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
    global model, tokenizer
    model_id = "meta-llama/Llama-3.1-8B-Instruct"
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    print(f"Model loaded: {model_id}")
    yield
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    del model, tokenizer

app = FastAPI(lifespan=lifespan)

class GenerateRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 100
    temperature: float = 0.7

class GenerateResponse(BaseModel):
    generated_text: str

@app.get("/health")
async def health():
    return {"status": "healthy"}

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    try:
        inputs = tokenizer(request.prompt, return_tensors="pt").to(model.device)
        
        outputs = model.generate(
            **inputs,
            max_new_tokens=request.max_new_tokens,
            temperature=request.temperature,
            do_sample=True,
        )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return GenerateResponse(generated_text=generated_text)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

### 3.3 Docker ë¹Œë“œ ë° ì‹¤í–‰

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t llm-inference:latest .

# GPUì™€ í•¨ê»˜ ì‹¤í–‰
docker run --gpus all \
    -p 8080:8080 \
    -e HF_TOKEN=$HF_TOKEN \
    -v $PWD/cache:/root/.cache/huggingface \
    llm-inference:latest

# í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8080/generate \
    -H "Content-Type: application/json" \
    -d '{"prompt": "Hello, how are you?", "max_new_tokens": 50}'
```

### 3.4 vLLM Docker ì»¨í…Œì´ë„ˆ

```dockerfile
# Dockerfile.vllm
FROM vllm/vllm-openai:latest

# í™˜ê²½ ë³€ìˆ˜
ENV MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
ENV MAX_MODEL_LEN=8192
ENV GPU_MEMORY_UTILIZATION=0.9

# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server", \
            "--model", "${MODEL_NAME}", \
            "--max-model-len", "${MAX_MODEL_LEN}", \
            "--gpu-memory-utilization", "${GPU_MEMORY_UTILIZATION}", \
            "--host", "0.0.0.0", \
            "--port", "8000"]
```

---

## 4. Kubernetes ë°°í¬

### 4.1 Kubernetes ë¦¬ì†ŒìŠ¤ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Kubernetes LLM ë°°í¬ êµ¬ì¡°                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   [Ingress] â”€â”€â–¶ [Service] â”€â”€â–¶ [Deployment]                â”‚
â”‚                                     â”‚                       â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                              â”‚   Pods      â”‚               â”‚
â”‚                              â”‚  (GPU)      â”‚               â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                     â”‚                       â”‚
â”‚                              [PVC / ConfigMap / Secret]    â”‚
â”‚                                                             â”‚
â”‚   ì¶”ê°€ êµ¬ì„±ìš”ì†Œ:                                            â”‚
â”‚   â€¢ HPA (Horizontal Pod Autoscaler)                        â”‚
â”‚   â€¢ PodMonitor (Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘)                   â”‚
â”‚   â€¢ NetworkPolicy (ë³´ì•ˆ)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Deployment YAML

```yaml
# llm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-inference
  labels:
    app: llm-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-inference
  template:
    metadata:
      labels:
        app: llm-inference
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - "--model"
          - "meta-llama/Llama-3.1-8B-Instruct"
          - "--host"
          - "0.0.0.0"
          - "--port"
          - "8000"
          - "--gpu-memory-utilization"
          - "0.9"
        ports:
        - containerPort: 8000
          name: http
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-secret
              key: token
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
          limits:
            memory: "32Gi"
            cpu: "8"
            nvidia.com/gpu: "1"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
        volumeMounts:
        - name: model-cache
          mountPath: /root/.cache/huggingface
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      nodeSelector:
        accelerator: nvidia-gpu
```

### 4.3 Service ë° Ingress

```yaml
# llm-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: llm-inference
spec:
  selector:
    app: llm-inference
  ports:
  - port: 80
    targetPort: 8000
    name: http
  type: ClusterIP

---
# llm-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-inference
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  ingressClassName: nginx
  rules:
  - host: llm.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llm-inference
            port:
              number: 80
  tls:
  - hosts:
    - llm.example.com
    secretName: llm-tls-secret
```

### 4.4 HPA (Horizontal Pod Autoscaler)

```yaml
# llm-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-inference-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-inference
  minReplicas: 1
  maxReplicas: 4
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: vllm_requests_running
      target:
        type: AverageValue
        averageValue: "10"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 120
```

### 4.5 Secret ë° ConfigMap

```yaml
# hf-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: hf-secret
type: Opaque
stringData:
  token: "hf_xxxxx"

---
# llm-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-config
data:
  MAX_MODEL_LEN: "8192"
  GPU_MEMORY_UTILIZATION: "0.9"
  MAX_NUM_SEQS: "256"
```

### 4.6 Helm Chart ì‚¬ìš©

```bash
# llm-d Helm Chart ì‚¬ìš© (ê¶Œì¥)
helm repo add llm-d https://llm-d.github.io/llm-d
helm repo update

# ì„¤ì¹˜
helm install my-llm llm-d/llm-d \
    --set model.name=meta-llama/Llama-3.1-8B-Instruct \
    --set resources.gpu=1 \
    --set hf.token=$HF_TOKEN

# ë˜ëŠ” values.yaml ì‚¬ìš©
helm install my-llm llm-d/llm-d -f values.yaml
```

**values.yaml ì˜ˆì‹œ:**

```yaml
# values.yaml
model:
  name: meta-llama/Llama-3.1-8B-Instruct
  quantization: awq  # ë˜ëŠ” gptq, null

inference:
  engine: vllm
  maxModelLen: 8192
  gpuMemoryUtilization: 0.9

resources:
  gpu: 1
  memory: 32Gi
  cpu: 8

autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 4
  targetCPU: 70

monitoring:
  enabled: true
  prometheus:
    enabled: true
  grafana:
    enabled: true

hf:
  token: ""  # ë˜ëŠ” Secret ì°¸ì¡°
  tokenSecret: hf-secret
```

---

## 5. CI/CD íŒŒì´í”„ë¼ì¸

### 5.1 GitHub Actions ì›Œí¬í”Œë¡œìš°

```yaml
# .github/workflows/deploy-llm.yaml
name: Deploy LLM Service

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'Dockerfile'
      - 'k8s/**'
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/llm-inference

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=sha,prefix=
          type=ref,event=branch
          latest

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig

    - name: Update deployment image
      run: |
        kubectl set image deployment/llm-inference \
          vllm=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \
          --namespace=llm

    - name: Wait for rollout
      run: |
        kubectl rollout status deployment/llm-inference \
          --namespace=llm \
          --timeout=600s

    - name: Run smoke tests
      run: |
        # ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
        sleep 30
        curl -f https://llm.example.com/health || exit 1
```

### 5.2 HuggingFace Spaces CI/CD

```yaml
# .github/workflows/sync-to-hf-space.yaml
name: Sync to Hugging Face Space

on:
  push:
    branches: [main]

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        lfs: true

    - name: Check large files
      uses: ActionsDesk/lfs-warning@v2.0
      with:
        filesizelimit: 10485760  # 10MB

    - name: Push to Hugging Face Space
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        git remote add hf https://user:$HF_TOKEN@huggingface.co/spaces/${{ secrets.HF_USERNAME }}/${{ secrets.HF_SPACE_NAME }}
        git push hf main --force
```

### 5.3 ëª¨ë¸ ë²„ì „ ê´€ë¦¬

```yaml
# .github/workflows/model-versioning.yaml
name: Model Version Management

on:
  workflow_dispatch:
    inputs:
      model_version:
        description: 'Model version tag'
        required: true
        type: string
      action:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - deploy
          - rollback

jobs:
  manage_model:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install huggingface_hub

    - name: Deploy/Rollback Model
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        python scripts/manage_endpoint.py \
          --action ${{ github.event.inputs.action }} \
          --version ${{ github.event.inputs.model_version }}
```

**scripts/manage_endpoint.py:**

```python
import argparse
from huggingface_hub import (
    get_inference_endpoint,
    update_inference_endpoint,
)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--action", choices=["deploy", "rollback"])
    parser.add_argument("--version", required=True)
    args = parser.parse_args()

    endpoint = get_inference_endpoint("my-llm-endpoint")
    
    if args.action == "deploy":
        # ìƒˆ ë²„ì „ ë°°í¬
        update_inference_endpoint(
            "my-llm-endpoint",
            repository=f"my-org/my-model",
            revision=args.version,
        )
        print(f"Deployed version: {args.version}")
    
    elif args.action == "rollback":
        # ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
        update_inference_endpoint(
            "my-llm-endpoint",
            revision=args.version,
        )
        print(f"Rolled back to: {args.version}")

if __name__ == "__main__":
    main()
```

---

## 6. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 6.1 LLM Observability ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LLM Observability Stack                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Metrics    â”‚    â”‚   Traces    â”‚    â”‚    Logs     â”‚   â”‚
â”‚   â”‚ (Prometheus)â”‚    â”‚  (Jaeger)   â”‚    â”‚   (Loki)    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚                  â”‚                  â”‚           â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                             â–¼                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                    â”‚    Grafana      â”‚                     â”‚
â”‚                    â”‚   Dashboards    â”‚                     â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                             â”‚
â”‚   ì£¼ìš” ë©”íŠ¸ë¦­:                                              â”‚
â”‚   â€¢ ìš”ì²­ ì§€ì—°ì‹œê°„ (P50, P95, P99)                          â”‚
â”‚   â€¢ í† í° ì²˜ë¦¬ëŸ‰ (tokens/sec)                               â”‚
â”‚   â€¢ GPU ì‚¬ìš©ë¥                                              â”‚
â”‚   â€¢ í† í° ë¹„ìš©                                              â”‚
â”‚   â€¢ ì—ëŸ¬ìœ¨                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘

**prometheus.yml:**

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'vllm-server'
    static_configs:
      - targets: ['vllm-server:8000']
    metrics_path: /metrics

  - job_name: 'llm-app'
    static_configs:
      - targets: ['llm-app:8001']
```

**vLLM ë©”íŠ¸ë¦­ ì˜ˆì‹œ:**

| ë©”íŠ¸ë¦­ | ì„¤ëª… |
|--------|------|
| `vllm:num_requests_running` | ì‹¤í–‰ ì¤‘ì¸ ìš”ì²­ ìˆ˜ |
| `vllm:num_requests_waiting` | ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ìˆ˜ |
| `vllm:gpu_cache_usage_perc` | GPU KV ìºì‹œ ì‚¬ìš©ë¥  |
| `vllm:avg_prompt_throughput_toks_per_s` | í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ëŸ‰ |
| `vllm:avg_generation_throughput_toks_per_s` | ìƒì„± ì²˜ë¦¬ëŸ‰ |
| `vllm:time_to_first_token_seconds` | ì²« í† í° ì§€ì—°ì‹œê°„ |
| `vllm:time_per_output_token_seconds` | í† í°ë‹¹ ì‹œê°„ |

### 6.3 ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ êµ¬í˜„

```python
# metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# ë©”íŠ¸ë¦­ ì •ì˜
REQUEST_COUNT = Counter(
    'llm_requests_total',
    'Total LLM requests',
    ['model', 'status']
)

REQUEST_LATENCY = Histogram(
    'llm_request_latency_seconds',
    'Request latency in seconds',
    ['model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

TOKEN_COUNT = Counter(
    'llm_tokens_total',
    'Total tokens processed',
    ['model', 'type']  # type: input/output
)

GPU_MEMORY = Gauge(
    'llm_gpu_memory_bytes',
    'GPU memory usage',
    ['gpu_id']
)

class LLMMetrics:
    def __init__(self, port=8001):
        start_http_server(port)
    
    def record_request(self, model: str, status: str, latency: float,
                       input_tokens: int, output_tokens: int):
        REQUEST_COUNT.labels(model=model, status=status).inc()
        REQUEST_LATENCY.labels(model=model).observe(latency)
        TOKEN_COUNT.labels(model=model, type='input').inc(input_tokens)
        TOKEN_COUNT.labels(model=model, type='output').inc(output_tokens)

# ì‚¬ìš© ì˜ˆì‹œ
metrics = LLMMetrics()

start_time = time.time()
# ... LLM ì¶”ë¡  ...
latency = time.time() - start_time

metrics.record_request(
    model="llama-8b",
    status="success",
    latency=latency,
    input_tokens=100,
    output_tokens=50,
)
```

### 6.4 OpenTelemetry í†µí•©

```python
# otel_setup.py
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# OpenLIT (LLM ìë™ ê³„ì¸¡)
import openlit

def setup_telemetry(app):
    # Tracer ì„¤ì •
    provider = TracerProvider()
    processor = BatchSpanProcessor(
        OTLPSpanExporter(endpoint="http://jaeger:4317")
    )
    provider.add_span_processor(processor)
    trace.set_tracer_provider(provider)
    
    # FastAPI ê³„ì¸¡
    FastAPIInstrumentor.instrument_app(app)
    
    # OpenLIT ì´ˆê¸°í™” (LLM ìë™ ê³„ì¸¡)
    openlit.init(
        otlp_endpoint="http://otel-collector:4317",
        application_name="llm-service",
    )
```

### 6.5 Grafana ëŒ€ì‹œë³´ë“œ

```json
{
  "dashboard": {
    "title": "LLM Inference Monitoring",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(llm_requests_total[5m])",
            "legendFormat": "{{model}} - {{status}}"
          }
        ]
      },
      {
        "title": "Latency P95",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(llm_request_latency_seconds_bucket[5m]))",
            "legendFormat": "P95 Latency"
          }
        ]
      },
      {
        "title": "Token Throughput",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(llm_tokens_total[5m])",
            "legendFormat": "{{type}} tokens/sec"
          }
        ]
      },
      {
        "title": "GPU Memory Usage",
        "type": "gauge",
        "targets": [
          {
            "expr": "llm_gpu_memory_bytes / 1e9",
            "legendFormat": "GPU {{gpu_id}}"
          }
        ]
      }
    ]
  }
}
```

### 6.6 ì•Œë¦¼ ì„¤ì •

```yaml
# alerting-rules.yaml
groups:
- name: llm-alerts
  rules:
  - alert: HighLatency
    expr: histogram_quantile(0.95, rate(llm_request_latency_seconds_bucket[5m])) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High LLM latency detected"
      description: "P95 latency is {{ $value }}s"

  - alert: HighErrorRate
    expr: rate(llm_requests_total{status="error"}[5m]) / rate(llm_requests_total[5m]) > 0.05
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High error rate in LLM service"
      description: "Error rate is {{ $value | humanizePercentage }}"

  - alert: GPUMemoryHigh
    expr: llm_gpu_memory_bytes / 85899345920 > 0.9  # 80GB ê¸°ì¤€
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "GPU memory usage is high"
      description: "GPU {{ $labels.gpu_id }} memory at {{ $value | humanizePercentage }}"
```

### 6.7 LangFuse í†µí•© (LLM ì „ìš© ê´€ì¸¡ì„±)

```python
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context

# Langfuse ì´ˆê¸°í™”
langfuse = Langfuse(
    public_key="pk-lf-xxx",
    secret_key="sk-lf-xxx",
    host="https://cloud.langfuse.com",
)

@observe(name="llm_generation")
def generate_response(prompt: str, model: str = "llama-8b"):
    # LLM í˜¸ì¶œ
    response = llm.generate(prompt)
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    langfuse_context.update_current_observation(
        input=prompt,
        output=response,
        model=model,
        metadata={
            "temperature": 0.7,
            "max_tokens": 100,
        },
        usage={
            "input_tokens": len(prompt.split()),
            "output_tokens": len(response.split()),
        },
    )
    
    return response
```

---

## 7. ì‹¤ìŠµ ê³¼ì œ

### ê³¼ì œ 1: Inference Endpoint ë°°í¬
1. HuggingFace Inference Endpointsë¡œ LLM ë°°í¬
2. Python SDKë¡œ Endpoint í˜¸ì¶œ
3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„

### ê³¼ì œ 2: Docker + Kubernetes ë°°í¬
1. vLLM Docker ì´ë¯¸ì§€ ë¹Œë“œ
2. Kubernetes Deployment ì‘ì„±
3. HPA ì„¤ì • ë° í…ŒìŠ¤íŠ¸

### ê³¼ì œ 3: CI/CD íŒŒì´í”„ë¼ì¸
1. GitHub Actions ì›Œí¬í”Œë¡œìš° ì‘ì„±
2. ìë™ ë¹Œë“œ ë° ë°°í¬
3. ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„

### ê³¼ì œ 4: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
1. Prometheus + Grafana ì„¤ì •
2. LLM ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ ìƒì„±
3. ì•Œë¦¼ ê·œì¹™ ì„¤ì •

---

## 8. ìš”ì•½ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Inference Endpoints
- [ ] UI/SDKë¡œ Endpoint ìƒì„±
- [ ] vLLM/TGI ì»¨í…Œì´ë„ˆ ì„¤ì •
- [ ] InferenceClient / OpenAI SDK ì‚¬ìš©
- [ ] ì˜¤í† ìŠ¤ì¼€ì¼ë§ ë° Scale-to-Zero

### TGI/vLLM
- [ ] Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰
- [ ] ì–‘ìí™” ëª¨ë¸ ì„œë¹™
- [ ] Tensor Parallelism (ë‹¤ì¤‘ GPU)

### Docker & Kubernetes
- [ ] Dockerfile ì‘ì„±
- [ ] Deployment/Service/Ingress YAML
- [ ] HPA ì„¤ì •
- [ ] Helm Chart ì‚¬ìš©

### CI/CD
- [ ] GitHub Actions ì›Œí¬í”Œë¡œìš°
- [ ] ìë™ ë¹Œë“œ/ë°°í¬
- [ ] ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜

### ëª¨ë‹ˆí„°ë§
- [ ] Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] Grafana ëŒ€ì‹œë³´ë“œ
- [ ] ì•Œë¦¼ ê·œì¹™
- [ ] OpenTelemetry/LangFuse í†µí•©

---

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- **HF Inference Endpoints**: https://huggingface.co/docs/inference-endpoints
- **TGI**: https://huggingface.co/docs/text-generation-inference
- **vLLM**: https://docs.vllm.ai/
- **llm-d (Kubernetes)**: https://llm-d.ai/

### ë„êµ¬
- **Prometheus**: https://prometheus.io/
- **Grafana**: https://grafana.com/
- **OpenTelemetry**: https://opentelemetry.io/
- **LangFuse**: https://langfuse.com/
- **OpenLIT**: https://github.com/openlit/openlit

### ë¸”ë¡œê·¸/íŠœí† ë¦¬ì–¼
- **HF Blog - Inference Endpoints**: https://huggingface.co/blog/inference-endpoints
- **HF Blog - Deploy LLMs**: https://huggingface.co/blog/inference-endpoints-llm
- **Grafana LLM Observability**: https://grafana.com/blog/a-complete-guide-to-llm-observability-with-opentelemetry-and-grafana-cloud/

---

## íŠœí† ë¦¬ì–¼ ì™„ë£Œ! ğŸ‰

**ì „ì²´ ì»¤ë¦¬í˜ëŸ¼ ì™„ë£Œ:**

| íŒŒíŠ¸ | ì£¼ì œ | ìƒíƒœ |
|------|------|------|
| 1.1-1.2 | Hub, Transformers, Datasets, Tokenizers | âœ… |
| 2.1-2.2 | Vision Transformers, VLM | âœ… |
| 3.1-3.2 | Small LLM ì„ íƒ/ë°°í¬ | âœ… |
| 4.1-4.3 | Embedding, RAG, Advanced RAG | âœ… |
| 5.1-5.3 | smolagents ê¸°ì´ˆ/ê³ ê¸‰/í”„ë¡œë•ì…˜ | âœ… |
| 6.1 | Model Optimization (Quantization, ONNX, vLLM) | âœ… |
| 6.2 | MLOps & Deployment | âœ… |

**ë‹¤ìŒ í•™ìŠµ ë°©í–¥:**
- íŠ¹ì • ë„ë©”ì¸ íŒŒì¸íŠœë‹ (PEFT, LoRA)
- ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (Vision-Language-Action)
- ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì‹¬í™” (MCP, Tool Use)