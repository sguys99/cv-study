# Part 2.1: Vision Transformer 실습

---

## 학습 목표
- ViT, DeiT, Swin Transformer 아키텍처 비교 이해
- Image Classification 파이프라인 구축
- Object Detection (DETR, YOLOS) 실습
- `AutoImageProcessor` 활용법 마스터

---

## 1. Vision Transformer 개요

### 1.1 ViT (Vision Transformer) 핵심 개념

```
이미지 → 패치 분할 → Linear Embedding → Transformer Encoder → Classification Head
```

**핵심 아이디어:**
- 이미지를 고정 크기 패치(16x16 또는 14x14)로 분할
- 각 패치를 토큰처럼 취급
- NLP Transformer 아키텍처 그대로 적용

### 1.2 주요 Vision Transformer 비교

| 모델 | 특징 | 장점 | 단점 |
|------|------|------|------|
| **ViT** | 순수 Transformer | 단순한 구조, 확장성 | 대규모 데이터 필요 |
| **DeiT** | Data-efficient ViT | 적은 데이터로 학습 가능, Knowledge Distillation | ViT 대비 약간 복잡 |
| **Swin** | Shifted Window Attention | 선형 복잡도, 다양한 해상도 지원 | 윈도우 크기 의존성 |

### 1.3 Inductive Bias 차이점

| CNN | Vision Transformer |
|-----|-------------------|
| Locality (지역성) 가정 | 전역 관계 학습 |
| Translation Equivariance | Position Embedding 필요 |
| 적은 데이터로 학습 가능 | 대규모 데이터에서 강점 |

---

## 2. 환경 설정

```bash
pip install transformers torch torchvision
pip install pillow requests matplotlib
pip install datasets  # 데이터셋 로딩용
```

---

## 3. Image Classification

### 3.1 Pipeline으로 빠른 추론

```python
from transformers import pipeline
from PIL import Image
import requests

# 이미지 분류 파이프라인
classifier = pipeline(
    task="image-classification",
    model="google/vit-base-patch16-224"
)

# 이미지 로드
url = "https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 추론
results = classifier(image)

for result in results[:5]:
    print(f"{result['label']:40} | Score: {result['score']:.4f}")
```

**출력 예시:**
```
Egyptian_cat                             | Score: 0.9234
tabby, tabby_cat                         | Score: 0.0521
tiger_cat                                | Score: 0.0198
...
```

### 3.2 AutoImageProcessor + AutoModel 활용

```python
from transformers import AutoImageProcessor, AutoModelForImageClassification
from PIL import Image
import requests
import torch

model_name = "google/vit-base-patch16-224"

# 이미지 프로세서 & 모델 로드
image_processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForImageClassification.from_pretrained(model_name)

# 이미지 로드
url = "https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# 전처리
inputs = image_processor(images=image, return_tensors="pt")
print(f"Input shape: {inputs['pixel_values'].shape}")
# torch.Size([1, 3, 224, 224])

# 추론
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# Top-5 예측
probs = torch.softmax(logits, dim=-1)
top5_probs, top5_indices = torch.topk(probs, 5)

print("\n=== Top 5 Predictions ===")
for prob, idx in zip(top5_probs[0], top5_indices[0]):
    label = model.config.id2label[idx.item()]
    print(f"{label:40} | {prob.item():.4f}")
```

### 3.3 다양한 ViT 모델 비교

```python
from transformers import AutoImageProcessor, AutoModelForImageClassification
from PIL import Image
import requests
import torch
import time

# 비교할 모델 목록
models = {
    "ViT-Base": "google/vit-base-patch16-224",
    "ViT-Large": "google/vit-large-patch16-224",
    "DeiT-Base": "facebook/deit-base-patch16-224",
    "DeiT-Small": "facebook/deit-small-patch16-224",
    "Swin-Tiny": "microsoft/swin-tiny-patch4-window7-224",
    "Swin-Base": "microsoft/swin-base-patch4-window7-224",
}

# 테스트 이미지
url = "https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

def compare_models(models: dict, image: Image.Image):
    results = {}
    
    for name, model_id in models.items():
        print(f"\n=== {name} ===")
        
        # 로드
        processor = AutoImageProcessor.from_pretrained(model_id)
        model = AutoModelForImageClassification.from_pretrained(model_id)
        model.eval()
        
        # 전처리
        inputs = processor(images=image, return_tensors="pt")
        
        # 추론 시간 측정
        start = time.time()
        with torch.no_grad():
            outputs = model(**inputs)
        inference_time = time.time() - start
        
        # 예측
        probs = torch.softmax(outputs.logits, dim=-1)
        top_prob, top_idx = torch.max(probs, dim=-1)
        label = model.config.id2label[top_idx.item()]
        
        results[name] = {
            "label": label,
            "confidence": top_prob.item(),
            "inference_time": inference_time,
            "params": sum(p.numel() for p in model.parameters()) / 1e6
        }
        
        print(f"  Prediction: {label}")
        print(f"  Confidence: {top_prob.item():.4f}")
        print(f"  Inference: {inference_time:.4f}s")
        print(f"  Parameters: {results[name]['params']:.1f}M")
    
    return results

# 실행 (일부 모델만 테스트)
results = compare_models(
    {k: v for k, v in list(models.items())[:3]},  # 처음 3개만
    image
)
```

### 3.4 Swin Transformer 상세

```python
from transformers import AutoImageProcessor, SwinForImageClassification
from PIL import Image
import requests
import torch

model_name = "microsoft/swin-base-patch4-window7-224"

# 로드
processor = AutoImageProcessor.from_pretrained(model_name)
model = SwinForImageClassification.from_pretrained(model_name)

# 설정 확인
print("=== Swin Configuration ===")
print(f"Image Size: {model.config.image_size}")
print(f"Patch Size: {model.config.patch_size}")
print(f"Window Size: {model.config.window_size}")
print(f"Embed Dim: {model.config.embed_dim}")
print(f"Depths: {model.config.depths}")
print(f"Num Heads: {model.config.num_heads}")

# 추론
url = "https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

inputs = processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

# Hidden states 분석 (옵션)
outputs_with_hidden = model(**inputs, output_hidden_states=True)
print(f"\nNumber of hidden states: {len(outputs_with_hidden.hidden_states)}")
for i, hs in enumerate(outputs_with_hidden.hidden_states):
    print(f"  Layer {i}: {hs.shape}")
```

---

## 4. AutoImageProcessor 활용

### 4.1 기본 사용법

```python
from transformers import AutoImageProcessor
from PIL import Image
import requests

# 프로세서 로드
processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")

# 설정 확인
print("=== Processor Configuration ===")
print(f"Image Size: {processor.size}")
print(f"Image Mean: {processor.image_mean}")
print(f"Image Std: {processor.image_std}")
print(f"Do Resize: {processor.do_resize}")
print(f"Do Normalize: {processor.do_normalize}")
print(f"Do Rescale: {processor.do_rescale}")

# 이미지 로드
url = "https://example.com/image.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# 전처리
inputs = processor(
    images=image,
    return_tensors="pt"
)
print(f"\nOutput keys: {inputs.keys()}")
print(f"Pixel values shape: {inputs['pixel_values'].shape}")
```

### 4.2 배치 처리

```python
from transformers import AutoImageProcessor
from PIL import Image
import requests

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")

# 여러 이미지
urls = [
    "https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg",
    "https://upload.wikimedia.org/wikipedia/commons/d/d9/Collage_of_Nine_Dogs.jpg",
]

images = [
    Image.open(requests.get(url, stream=True).raw).convert("RGB")
    for url in urls
]

# 배치 전처리
batch = processor(
    images=images,
    return_tensors="pt"
)

print(f"Batch shape: {batch['pixel_values'].shape}")
# torch.Size([2, 3, 224, 224])
```

### 4.3 커스텀 전처리 (Data Augmentation 포함)

```python
from transformers import AutoImageProcessor
from torchvision.transforms import (
    Compose, RandomResizedCrop, RandomHorizontalFlip,
    ToTensor, Normalize, ColorJitter
)
from PIL import Image

# 기본 프로세서 로드
processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")

# 학습용 Augmentation
train_transforms = Compose([
    RandomResizedCrop(processor.size["height"]),
    RandomHorizontalFlip(p=0.5),
    ColorJitter(brightness=0.2, contrast=0.2),
    ToTensor(),
    Normalize(mean=processor.image_mean, std=processor.image_std)
])

# 검증용 Transform (augmentation 없음)
val_transforms = Compose([
    ToTensor(),
])

def preprocess_train(example):
    """학습 데이터 전처리 (augmentation 포함)"""
    image = example["image"].convert("RGB")
    example["pixel_values"] = train_transforms(image)
    return example

def preprocess_val(example):
    """검증 데이터 전처리"""
    image = example["image"].convert("RGB")
    # AutoImageProcessor 사용
    inputs = processor(images=image, return_tensors="pt")
    example["pixel_values"] = inputs["pixel_values"].squeeze()
    return example
```

### 4.4 Fast Image Processor (GPU 가속)

```python
from transformers import AutoImageProcessor
import torch

# Fast 프로세서 (torchvision 기반, GPU 가속 지원)
processor = AutoImageProcessor.from_pretrained(
    "google/vit-base-patch16-224",
    use_fast=True
)

# GPU에서 처리
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 이미 텐서인 경우 GPU에서 직접 처리 가능
image_tensor = torch.randn(3, 256, 256).to(device)
inputs = processor(images=image_tensor, return_tensors="pt", device=device)
```

---

## 5. Object Detection

### 5.1 DETR (DEtection TRansformer)

```python
from transformers import pipeline
from PIL import Image
import requests

# Object Detection 파이프라인
detector = pipeline(
    task="object-detection",
    model="facebook/detr-resnet-50"
)

# 이미지 로드
url = "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/1200px-Cat_November_2010-1a.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 탐지
results = detector(image)

print("=== Detected Objects ===")
for obj in results:
    print(f"  {obj['label']:15} | Score: {obj['score']:.4f} | Box: {obj['box']}")
```

### 5.2 DETR 상세 사용법

```python
from transformers import AutoImageProcessor, AutoModelForObjectDetection
from PIL import Image, ImageDraw
import requests
import torch

model_name = "facebook/detr-resnet-50"

# 로드
processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForObjectDetection.from_pretrained(model_name)

# 이미지 로드
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 전처리
inputs = processor(images=image, return_tensors="pt")

# 추론
with torch.no_grad():
    outputs = model(**inputs)

# 후처리 (threshold 적용)
target_sizes = torch.tensor([image.size[::-1]])  # (height, width)
results = processor.post_process_object_detection(
    outputs,
    target_sizes=target_sizes,
    threshold=0.9
)[0]

# 결과 출력
print("=== Detection Results ===")
for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    label_name = model.config.id2label[label.item()]
    print(f"  {label_name:15} | Score: {score.item():.4f} | Box: {box}")

# 시각화
def visualize_detection(image, results, model):
    draw = ImageDraw.Draw(image)
    
    for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
        box = box.tolist()
        label_name = model.config.id2label[label.item()]
        
        # 박스 그리기
        draw.rectangle(box, outline="red", width=3)
        
        # 라벨 텍스트
        text = f"{label_name}: {score.item():.2f}"
        draw.text((box[0], box[1] - 15), text, fill="red")
    
    return image

# 결과 이미지
result_image = visualize_detection(image.copy(), results, model)
result_image.save("detection_result.jpg")
```

### 5.3 YOLOS (ViT 기반 Detection)

```python
from transformers import AutoImageProcessor, AutoModelForObjectDetection
from PIL import Image
import requests
import torch

model_name = "hustvl/yolos-small"

# 로드
processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForObjectDetection.from_pretrained(model_name)

# 이미지 로드
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 전처리
inputs = processor(images=image, return_tensors="pt")

# 추론
with torch.no_grad():
    outputs = model(**inputs)

# 후처리
target_sizes = torch.tensor([image.size[::-1]])
results = processor.post_process_object_detection(
    outputs,
    target_sizes=target_sizes,
    threshold=0.8
)[0]

print("=== YOLOS Detection Results ===")
for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    label_name = model.config.id2label[label.item()]
    print(f"  {label_name:15} | Score: {score.item():.4f}")
```

### 5.4 RT-DETR (Real-Time DETR)

```python
from transformers import AutoImageProcessor, AutoModelForObjectDetection
from PIL import Image
import requests
import torch

model_name = "PekingU/rtdetr_r50vd"

# 로드
processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForObjectDetection.from_pretrained(model_name)

# 이미지 로드
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 전처리 & 추론
inputs = processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

# 후처리
target_sizes = torch.tensor([image.size[::-1]])
results = processor.post_process_object_detection(
    outputs,
    target_sizes=target_sizes,
    threshold=0.5
)[0]

print("=== RT-DETR Detection Results ===")
for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    label_name = model.config.id2label[label.item()]
    print(f"  {label_name:15} | Score: {score.item():.4f}")
```

### 5.5 Object Detection 모델 비교

| 모델 | 백본 | 특징 | 속도 | 정확도 |
|------|------|------|------|--------|
| **DETR** | ResNet | End-to-end, No NMS | 느림 | 높음 |
| **Conditional DETR** | ResNet | 빠른 수렴 | 중간 | 높음 |
| **Deformable DETR** | ResNet | 작은 객체 개선 | 중간 | 매우 높음 |
| **YOLOS** | ViT | 순수 ViT 기반 | 빠름 | 중간 |
| **RT-DETR** | ResNet/HGNetv2 | Real-time | 매우 빠름 | 높음 |

---

## 6. Fine-tuning 예시

### 6.1 Image Classification Fine-tuning

```python
from transformers import (
    AutoImageProcessor,
    AutoModelForImageClassification,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
import torch
import numpy as np

# 데이터셋 로드 (Food-101 일부)
dataset = load_dataset("food101", split="train[:1000]")
dataset = dataset.train_test_split(test_size=0.2)

# 라벨 매핑
labels = dataset["train"].features["label"].names
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for i, label in enumerate(labels)}

# 프로세서 & 모델
model_name = "google/vit-base-patch16-224-in21k"
processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForImageClassification.from_pretrained(
    model_name,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True  # 분류 헤드 크기 다름
)

# 전처리 함수
def preprocess(examples):
    images = [img.convert("RGB") for img in examples["image"]]
    inputs = processor(images, return_tensors="pt")
    inputs["labels"] = examples["label"]
    return inputs

# 데이터셋 전처리
train_dataset = dataset["train"].with_transform(preprocess)
eval_dataset = dataset["test"].with_transform(preprocess)

# 평가 메트릭
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

# 학습 설정
training_args = TrainingArguments(
    output_dir="./vit-food101",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    warmup_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    logging_steps=50,
    remove_unused_columns=False,  # 중요!
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

# 학습
trainer.train()

# Hub에 업로드
trainer.push_to_hub("my-vit-food101")
```

---

## 7. 실습 과제

### 과제 1: 멀티 모델 앙상블
```python
"""
목표: 여러 Vision Transformer의 예측을 앙상블
요구사항:
1. ViT, DeiT, Swin 모델 로드
2. 동일 이미지에 대해 각 모델 추론
3. 예측 확률 평균으로 최종 예측
4. 개별 모델 vs 앙상블 성능 비교
"""

def ensemble_predict(image, model_ids: list[str]):
    # TODO: 구현하기
    pass
```

### 과제 2: Object Detection 시각화 도구
```python
"""
목표: Detection 결과를 시각화하는 유틸리티 함수
요구사항:
1. 바운딩 박스 그리기
2. 클래스 라벨 + 신뢰도 표시
3. 다양한 색상으로 클래스 구분
4. 이미지 저장 기능
"""

def visualize_detections(
    image,
    boxes,
    labels,
    scores,
    id2label,
    threshold=0.5,
    save_path=None
):
    # TODO: 구현하기
    pass
```

---

## 8. 참고 링크

### 공식 문서
- [ViT Documentation](https://huggingface.co/docs/transformers/model_doc/vit)
- [Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)
- [DeiT Documentation](https://huggingface.co/docs/transformers/model_doc/deit)
- [DETR Documentation](https://huggingface.co/docs/transformers/model_doc/detr)
- [YOLOS Documentation](https://huggingface.co/docs/transformers/model_doc/yolos)
- [Image Classification Task](https://huggingface.co/docs/transformers/tasks/image_classification)
- [Object Detection Task](https://huggingface.co/docs/transformers/tasks/object_detection)
- [Image Processor Guide](https://huggingface.co/docs/transformers/image_processors)

### HuggingFace Course
- [Computer Vision Course](https://huggingface.co/learn/computer-vision-course)
- [Vision Transformers for Image Classification](https://huggingface.co/learn/computer-vision-course/en/unit3/vision-transformers/vision-transformers-for-image-classification)

### 모델 Hub
- [Vision Models](https://huggingface.co/models?pipeline_tag=image-classification)
- [Object Detection Models](https://huggingface.co/models?pipeline_tag=object-detection)

---

## 9. 핵심 요약

| 작업 | 모델 | Pipeline Task | AutoModel |
|------|------|---------------|-----------|
| 이미지 분류 | ViT, DeiT, Swin | `image-classification` | `AutoModelForImageClassification` |
| 객체 탐지 | DETR, YOLOS | `object-detection` | `AutoModelForObjectDetection` |
| 이미지 전처리 | - | - | `AutoImageProcessor` |

### 모델 선택 가이드

| 상황 | 추천 모델 |
|------|----------|
| 범용 분류 (정확도 우선) | Swin-Base, ViT-Large |
| 빠른 추론 필요 | DeiT-Small, ViT-Base |
| 적은 데이터 | DeiT (Knowledge Distillation) |
| 다양한 해상도 | Swin Transformer |
| Real-time Detection | RT-DETR, YOLOS |
| 높은 정확도 Detection | Deformable DETR |