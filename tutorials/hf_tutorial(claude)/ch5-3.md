# Hugging Face íŠœí† ë¦¬ì–¼ Part 5.3: Production Agent Projects

## í•™ìŠµ ëª©í‘œ
- ì‹¤ì „ Agent í”„ë¡œì íŠ¸ êµ¬ì¶• (Research Agent, Coding Assistant)
- Gradio UI ë° FastAPI ë°°í¬
- OpenTelemetry ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ (Langfuse, Phoenix)
- ì—ëŸ¬ í•¸ë“¤ë§ ë° í´ë°± ì „ëµ
- HuggingFace Spaces ë°°í¬
- ì„±ëŠ¥ ìµœì í™” ë° ë¹„ìš© ê´€ë¦¬

---

## 1. ì‹¤ì „ í”„ë¡œì íŠ¸: Research Agent

### 1.1 Research Agent ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Research Agent System                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   [ì‚¬ìš©ì ì§ˆì˜] â”€â”€â–¶ [Manager Agent]                        â”‚
â”‚                          â”‚                                  â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚          â–¼               â–¼               â–¼                 â”‚
â”‚   [Web Search]    [Document RAG]   [Summarizer]            â”‚
â”‚      Agent           Agent           Agent                 â”‚
â”‚          â”‚               â”‚               â”‚                 â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â–¼                                  â”‚
â”‚                  [ë¦¬í¬íŠ¸ ìƒì„±]                              â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚               [Interactive Report]                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
research_agent/
â”œâ”€â”€ app.py                 # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ search_agent.py    # ì›¹ ê²€ìƒ‰ Agent
â”‚   â”œâ”€â”€ rag_agent.py       # ë¬¸ì„œ RAG Agent
â”‚   â””â”€â”€ manager_agent.py   # ë§¤ë‹ˆì € Agent
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ search_tools.py    # ê²€ìƒ‰ ë„êµ¬
â”‚   â””â”€â”€ document_tools.py  # ë¬¸ì„œ ì²˜ë¦¬ ë„êµ¬
â”œâ”€â”€ config.py              # ì„¤ì •
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

### 1.3 ê²€ìƒ‰ Agent êµ¬í˜„

```python
# agents/search_agent.py
from smolagents import (
    ToolCallingAgent, 
    DuckDuckGoSearchTool, 
    VisitWebpageTool,
    InferenceClientModel,
)


def create_search_agent(model_id: str = "Qwen/Qwen2.5-72B-Instruct"):
    """ì›¹ ê²€ìƒ‰ ì „ë¬¸ Agent ìƒì„±"""
    
    model = InferenceClientModel(model_id=model_id)
    
    agent = ToolCallingAgent(
        tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],
        model=model,
        name="search_agent",
        description="""
        ì›¹ ê²€ìƒ‰ ì „ë¬¸ Agentì…ë‹ˆë‹¤.
        - ìµœì‹  ì •ë³´ ê²€ìƒ‰
        - ì›¹í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
        - ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì •ë³´ ìˆ˜ì§‘
        """,
        max_steps=5,
    )
    
    return agent
```

### 1.4 RAG Agent êµ¬í˜„

```python
# agents/rag_agent.py
from smolagents import Tool, CodeAgent, InferenceClientModel
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter


class DocumentRetrieverTool(Tool):
    """ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
    
    name = "document_retriever"
    description = """
    ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì¿¼ë¦¬ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    inputs = {
        "query": {
            "type": "string",
            "description": "ê²€ìƒ‰ ì¿¼ë¦¬",
        },
        "top_k": {
            "type": "integer",
            "description": "ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸: 5)",
            "nullable": True,
        }
    }
    output_type = "string"
    
    def __init__(self, documents: list = None, **kwargs):
        super().__init__(**kwargs)
        
        # ì„ë² ë”© ëª¨ë¸
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # ë¬¸ì„œê°€ ì œê³µëœ ê²½ìš° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        if documents:
            self._build_vectorstore(documents)
        else:
            self.vectorstore = None
    
    def _build_vectorstore(self, documents: list):
        """ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
        )
        docs = splitter.split_documents(documents)
        self.vectorstore = FAISS.from_documents(docs, self.embeddings)
    
    def add_documents(self, documents: list):
        """ë¬¸ì„œ ì¶”ê°€"""
        self._build_vectorstore(documents)
    
    def forward(self, query: str, top_k: int = 5) -> str:
        if self.vectorstore is None:
            return "ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        results = self.vectorstore.similarity_search(query, k=top_k)
        
        if not results:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        output = f"ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):\n"
        for i, doc in enumerate(results):
            output += f"\n[{i+1}] {doc.page_content[:300]}...\n"
        
        return output


def create_rag_agent(documents: list = None, model_id: str = "Qwen/Qwen2.5-72B-Instruct"):
    """RAG ì „ë¬¸ Agent ìƒì„±"""
    
    retriever_tool = DocumentRetrieverTool(documents=documents)
    
    model = InferenceClientModel(model_id=model_id)
    
    agent = CodeAgent(
        tools=[retriever_tool],
        model=model,
        name="rag_agent",
        description="""
        ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ Agentì…ë‹ˆë‹¤.
        - ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì •ë³´ ê²€ìƒ‰
        - ë¬¸ì„œ ìš”ì•½
        - ì§ˆì˜ì‘ë‹µ
        """,
        max_steps=4,
    )
    
    return agent, retriever_tool
```

### 1.5 Manager Agent êµ¬í˜„

```python
# agents/manager_agent.py
from smolagents import CodeAgent, InferenceClientModel, ManagedAgent


def create_manager_agent(
    search_agent,
    rag_agent,
    model_id: str = "Qwen/Qwen2.5-72B-Instruct"
):
    """Research Manager Agent ìƒì„±"""
    
    model = InferenceClientModel(model_id=model_id)
    
    # Managed Agents ë˜í•‘
    managed_search = ManagedAgent(
        agent=search_agent,
        name="web_searcher",
        description="ìµœì‹  ì›¹ ì •ë³´ ê²€ìƒ‰ ì „ë¬¸ê°€",
    )
    
    managed_rag = ManagedAgent(
        agent=rag_agent,
        name="document_analyst", 
        description="ë¬¸ì„œ ë¶„ì„ ë° ê²€ìƒ‰ ì „ë¬¸ê°€",
    )
    
    manager = CodeAgent(
        tools=[],
        model=model,
        managed_agents=[managed_search, managed_rag],
        planning_interval=3,
        max_steps=10,
        verbosity_level=2,
    )
    
    return manager
```

### 1.6 ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜

```python
# app.py
import os
from smolagents import GradioUI
from agents.search_agent import create_search_agent
from agents.rag_agent import create_rag_agent
from agents.manager_agent import create_manager_agent


def create_research_system():
    """Research Agent ì‹œìŠ¤í…œ ìƒì„±"""
    
    # ê°œë³„ Agent ìƒì„±
    search_agent = create_search_agent()
    rag_agent, retriever_tool = create_rag_agent()
    
    # Manager Agent ìƒì„±
    manager = create_manager_agent(
        search_agent=search_agent,
        rag_agent=rag_agent,
    )
    
    return manager, retriever_tool


def main():
    manager, retriever_tool = create_research_system()
    
    # Gradio UI ì‹¤í–‰
    ui = GradioUI(
        manager,
        file_upload_folder="uploads",
        reset_agent_memory=False,
    )
    ui.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
    )


if __name__ == "__main__":
    main()
```

---

## 2. ì‹¤ì „ í”„ë¡œì íŠ¸: Coding Assistant

### 2.1 Coding Assistant ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Coding Assistant                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   [ì‚¬ìš©ì ì½”ë“œ/ì§ˆë¬¸]                                        â”‚
â”‚          â”‚                                                  â”‚
â”‚          â–¼                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚   â”‚   Code Agent    â”‚                                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚            â”‚                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â–¼                 â–¼            â–¼            â–¼           â”‚
â”‚ [ì½”ë“œ ì‹¤í–‰]    [ë¬¸ì„œ ê²€ìƒ‰]   [ì›¹ ê²€ìƒ‰]   [íŒŒì¼ ê´€ë¦¬]       â”‚
â”‚   Tool           Tool        Tool        Tool              â”‚
â”‚   â”‚                 â”‚            â”‚            â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                    â–¼                                        â”‚
â”‚            [ì½”ë“œ ìƒì„±/ìˆ˜ì •/ë””ë²„ê¹…]                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ì½”ë“œ ì‹¤í–‰ Tool

```python
# tools/code_tools.py
from smolagents import tool
import subprocess
import tempfile
import os


@tool
def execute_python(code: str, timeout: int = 30) -> str:
    """
    Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        code: ì‹¤í–‰í•  Python ì½”ë“œ
        timeout: ì‹¤í–‰ ì œí•œ ì‹œê°„ (ì´ˆ)
    
    Returns:
        ì‹¤í–‰ ê²°ê³¼ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€
    """
    try:
        # ì„ì‹œ íŒŒì¼ì— ì½”ë“œ ì €ì¥
        with tempfile.NamedTemporaryFile(
            mode='w',
            suffix='.py',
            delete=False
        ) as f:
            f.write(code)
            temp_file = f.name
        
        # ì½”ë“œ ì‹¤í–‰
        result = subprocess.run(
            ['python', temp_file],
            capture_output=True,
            text=True,
            timeout=timeout,
        )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_file)
        
        if result.returncode == 0:
            return f"ì‹¤í–‰ ì„±ê³µ:\n{result.stdout}"
        else:
            return f"ì‹¤í–‰ ì˜¤ë¥˜:\n{result.stderr}"
            
    except subprocess.TimeoutExpired:
        return f"íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ ë‚´ì— ì‹¤í–‰ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"ì˜ˆì™¸ ë°œìƒ: {str(e)}"


@tool
def lint_python(code: str) -> str:
    """
    Python ì½”ë“œì˜ ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ê²€ì‚¬í•©ë‹ˆë‹¤.
    
    Args:
        code: ê²€ì‚¬í•  Python ì½”ë“œ
    
    Returns:
        ë¦°íŠ¸ ê²°ê³¼
    """
    import ast
    
    try:
        ast.parse(code)
        return "âœ“ ë¬¸ë²• ì˜¤ë¥˜ ì—†ìŒ"
    except SyntaxError as e:
        return f"âœ— ë¬¸ë²• ì˜¤ë¥˜ ë°œê²¬:\n  ë¼ì¸ {e.lineno}: {e.msg}"


@tool
def explain_error(error_message: str) -> str:
    """
    Python ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ê³  í•´ê²°ì±…ì„ ì œì•ˆí•©ë‹ˆë‹¤.
    
    Args:
        error_message: ë¶„ì„í•  ì—ëŸ¬ ë©”ì‹œì§€
    
    Returns:
        ì—ëŸ¬ ì„¤ëª… ë° í•´ê²°ì±…
    """
    common_errors = {
        "NameError": "ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³€ìˆ˜ëª… ì² ìë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "TypeError": "ì˜ëª»ëœ íƒ€ì…ìœ¼ë¡œ ì—°ì‚°ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•˜ì„¸ìš”.",
        "IndexError": "ë¦¬ìŠ¤íŠ¸ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ ê°’ì„ í™•ì¸í•˜ì„¸ìš”.",
        "KeyError": "ë”•ì…”ë„ˆë¦¬ì— í•´ë‹¹ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.",
        "ValueError": "ì˜ëª»ëœ ê°’ì´ ì „ë‹¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ í™•ì¸í•˜ì„¸ìš”.",
        "ImportError": "ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "AttributeError": "ê°ì²´ì— í•´ë‹¹ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤. ë©”ì„œë“œ/ì†ì„±ëª…ì„ í™•ì¸í•˜ì„¸ìš”.",
        "ZeroDivisionError": "0ìœ¼ë¡œ ë‚˜ëˆ„ê¸°ë¥¼ ì‹œë„í–ˆìŠµë‹ˆë‹¤. ë¶„ëª¨ ê°’ì„ í™•ì¸í•˜ì„¸ìš”.",
    }
    
    explanation = "ì—ëŸ¬ ë¶„ì„:\n"
    for error_type, desc in common_errors.items():
        if error_type in error_message:
            explanation += f"- {error_type}: {desc}\n"
            break
    else:
        explanation += "- ì¼ë°˜ì ì¸ ì—ëŸ¬ì…ë‹ˆë‹¤. ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
    
    return explanation
```

### 2.3 ë¬¸ì„œ ê²€ìƒ‰ Tool

```python
# tools/doc_tools.py
from smolagents import tool
import requests


@tool  
def search_python_docs(query: str) -> str:
    """
    Python ê³µì‹ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ˆ: "list", "dict", "async")
    
    Returns:
        ê´€ë ¨ ë¬¸ì„œ ì •ë³´
    """
    # ì£¼ìš” Python ë¬¸ì„œ ì •ë³´ (ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ ë˜ëŠ” ë¡œì»¬ DB ì‚¬ìš©)
    docs = {
        "list": """
        list - ê°€ë³€ ì‹œí€€ìŠ¤ íƒ€ì…
        
        ìƒì„±: my_list = [1, 2, 3]
        ë©”ì„œë“œ:
        - append(x): ëì— ìš”ì†Œ ì¶”ê°€
        - extend(iterable): ì—¬ëŸ¬ ìš”ì†Œ ì¶”ê°€
        - insert(i, x): ìœ„ì¹˜ì— ì‚½ì…
        - remove(x): ì²« ë²ˆì§¸ x ì œê±°
        - pop([i]): ìœ„ì¹˜ì˜ ìš”ì†Œ ì œê±° í›„ ë°˜í™˜
        - sort(): ì •ë ¬
        - reverse(): ì—­ìˆœ
        
        ë¬¸ì„œ: https://docs.python.org/3/library/stdtypes.html#list
        """,
        
        "dict": """
        dict - í‚¤-ê°’ ë§¤í•‘ íƒ€ì…
        
        ìƒì„±: my_dict = {"key": "value"}
        ë©”ì„œë“œ:
        - keys(): í‚¤ ëª©ë¡
        - values(): ê°’ ëª©ë¡
        - items(): (í‚¤, ê°’) íŠœí”Œ ëª©ë¡
        - get(key, default): í‚¤ë¡œ ì¡°íšŒ
        - update(other): ë‹¤ë¥¸ ë”•ì…”ë„ˆë¦¬ë¡œ ì—…ë°ì´íŠ¸
        - pop(key): í‚¤ ì œê±° í›„ ê°’ ë°˜í™˜
        
        ë¬¸ì„œ: https://docs.python.org/3/library/stdtypes.html#dict
        """,
        
        "async": """
        async/await - ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°
        
        async def fetch_data():
            await asyncio.sleep(1)
            return "data"
        
        asyncio.run(fetch_data())
        
        ì£¼ìš” í•¨ìˆ˜:
        - asyncio.run(): ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        - asyncio.gather(): ì—¬ëŸ¬ ì½”ë£¨í‹´ ë™ì‹œ ì‹¤í–‰
        - asyncio.create_task(): íƒœìŠ¤í¬ ìƒì„±
        
        ë¬¸ì„œ: https://docs.python.org/3/library/asyncio.html
        """,
    }
    
    query_lower = query.lower()
    for key, doc in docs.items():
        if key in query_lower:
            return doc
    
    return f"'{query}'ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•˜ì„¸ìš”."
```

### 2.4 Coding Assistant Agent

```python
# coding_assistant.py
from smolagents import CodeAgent, InferenceClientModel, DuckDuckGoSearchTool
from tools.code_tools import execute_python, lint_python, explain_error
from tools.doc_tools import search_python_docs


def create_coding_assistant(
    model_id: str = "Qwen/Qwen2.5-Coder-32B-Instruct",
    enable_web_search: bool = True,
):
    """Coding Assistant Agent ìƒì„±"""
    
    tools = [
        execute_python,
        lint_python,
        explain_error,
        search_python_docs,
    ]
    
    if enable_web_search:
        tools.append(DuckDuckGoSearchTool())
    
    model = InferenceClientModel(model_id=model_id)
    
    agent = CodeAgent(
        tools=tools,
        model=model,
        additional_authorized_imports=[
            "numpy", "pandas", "requests", "json",
            "datetime", "os", "sys", "re", "math",
        ],
        max_steps=10,
        verbosity_level=2,
    )
    
    return agent


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from smolagents import GradioUI
    
    assistant = create_coding_assistant()
    
    # Gradio UIë¡œ ì‹¤í–‰
    ui = GradioUI(assistant, reset_agent_memory=True)
    ui.launch()
```

---

## 3. Gradio UI ë°°í¬

### 3.1 ê¸°ë³¸ GradioUI ì„¤ì •

```python
from smolagents import CodeAgent, InferenceClientModel, GradioUI


# Agent ìƒì„±
model = InferenceClientModel(model_id="Qwen/Qwen2.5-72B-Instruct")
agent = CodeAgent(tools=[], model=model)

# Gradio UI ì„¤ì •
ui = GradioUI(
    agent=agent,
    file_upload_folder="uploads",      # íŒŒì¼ ì—…ë¡œë“œ í´ë”
    reset_agent_memory=False,          # ëŒ€í™” ê¸°ë¡ ìœ ì§€
)

# ì‹¤í–‰
ui.launch(
    server_name="0.0.0.0",             # ëª¨ë“  IPì—ì„œ ì ‘ê·¼ í—ˆìš©
    server_port=7860,                   # ê¸°ë³¸ í¬íŠ¸
    share=True,                         # ê³µìœ  ë§í¬ ìƒì„±
)
```

### 3.2 ì»¤ìŠ¤í…€ Gradio Interface

```python
import gradio as gr
from smolagents import CodeAgent, InferenceClientModel


def create_custom_interface():
    """ì»¤ìŠ¤í…€ Gradio ì¸í„°í˜ì´ìŠ¤"""
    
    model = InferenceClientModel()
    agent = CodeAgent(tools=[], model=model)
    
    def process_query(message, history):
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬"""
        try:
            response = agent.run(message)
            return str(response)
        except Exception as e:
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    # ì»¤ìŠ¤í…€ Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(title="AI Assistant") as demo:
        gr.Markdown("# ğŸ¤– AI Assistant")
        gr.Markdown("smolagents ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")
        
        chatbot = gr.Chatbot(height=400)
        msg = gr.Textbox(
            placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
            label="ë©”ì‹œì§€"
        )
        
        with gr.Row():
            submit = gr.Button("ì „ì†¡", variant="primary")
            clear = gr.Button("ì´ˆê¸°í™”")
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        def respond(message, chat_history):
            bot_message = process_query(message, chat_history)
            chat_history.append((message, bot_message))
            return "", chat_history
        
        msg.submit(respond, [msg, chatbot], [msg, chatbot])
        submit.click(respond, [msg, chatbot], [msg, chatbot])
        clear.click(lambda: None, None, chatbot, queue=False)
    
    return demo


if __name__ == "__main__":
    demo = create_custom_interface()
    demo.launch(server_name="0.0.0.0", server_port=7860)
```

### 3.3 ai-gradio í†µí•©

```python
# ai-gradioë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ì„¤ì •
import gradio as gr
import ai_gradio

# smolagents ëª¨ë¸ ë¡œë“œ
gr.load(
    name='smolagents:meta-llama/Llama-3.1-8B-Instruct',
    src=ai_gradio.registry
).launch()
```

---

## 4. FastAPI ë°°í¬

### 4.1 FastAPI ì„œë²„ êµ¬í˜„

```python
# server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import uvicorn

from smolagents import CodeAgent, InferenceClientModel, DuckDuckGoSearchTool


app = FastAPI(
    title="smolagents API",
    description="smolagents ê¸°ë°˜ AI Agent API",
    version="1.0.0",
)

# Agent ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
model = InferenceClientModel(model_id="Qwen/Qwen2.5-72B-Instruct")
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    max_steps=5,
)


class QueryRequest(BaseModel):
    query: str
    max_steps: Optional[int] = 5


class QueryResponse(BaseModel):
    result: str
    success: bool
    error: Optional[str] = None


@app.get("/")
async def root():
    return {"message": "smolagents API Server", "status": "running"}


@app.get("/health")
async def health_check():
    return {"status": "healthy"}


@app.post("/agent/run", response_model=QueryResponse)
async def run_agent(request: QueryRequest):
    """Agent ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        result = agent.run(request.query)
        return QueryResponse(
            result=str(result),
            success=True,
        )
    except Exception as e:
        return QueryResponse(
            result="",
            success=False,
            error=str(e),
        )


@app.post("/agent/chat")
async def chat(request: QueryRequest):
    """ëŒ€í™”í˜• ì—”ë“œí¬ì¸íŠ¸"""
    try:
        result = agent.run(request.query)
        return {
            "query": request.query,
            "response": str(result),
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 4.2 ë¹„ë™ê¸° Agent ì²˜ë¦¬

```python
# async_server.py
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
import asyncio
import uuid
from typing import Dict

from smolagents import CodeAgent, InferenceClientModel


app = FastAPI()

# ì‘ì—… ì €ì¥ì†Œ
tasks: Dict[str, dict] = {}

# Agent ì´ˆê¸°í™”
model = InferenceClientModel()
agent = CodeAgent(tools=[], model=model)


class TaskRequest(BaseModel):
    query: str


class TaskStatus(BaseModel):
    task_id: str
    status: str  # pending, running, completed, failed
    result: str = None
    error: str = None


def run_agent_task(task_id: str, query: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ Agent ì‹¤í–‰"""
    tasks[task_id]["status"] = "running"
    
    try:
        result = agent.run(query)
        tasks[task_id]["status"] = "completed"
        tasks[task_id]["result"] = str(result)
    except Exception as e:
        tasks[task_id]["status"] = "failed"
        tasks[task_id]["error"] = str(e)


@app.post("/agent/submit")
async def submit_task(
    request: TaskRequest,
    background_tasks: BackgroundTasks
):
    """ë¹„ë™ê¸° ì‘ì—… ì œì¶œ"""
    task_id = str(uuid.uuid4())
    
    tasks[task_id] = {
        "status": "pending",
        "query": request.query,
        "result": None,
        "error": None,
    }
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¶”ê°€
    background_tasks.add_task(run_agent_task, task_id, request.query)
    
    return {"task_id": task_id, "message": "Task submitted"}


@app.get("/agent/status/{task_id}", response_model=TaskStatus)
async def get_task_status(task_id: str):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    if task_id not in tasks:
        return TaskStatus(
            task_id=task_id,
            status="not_found",
        )
    
    task = tasks[task_id]
    return TaskStatus(
        task_id=task_id,
        status=task["status"],
        result=task.get("result"),
        error=task.get("error"),
    )
```

---

## 5. OpenTelemetry ëª¨ë‹ˆí„°ë§

### 5.1 Langfuse ì—°ë™

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  OpenTelemetry ëª¨ë‹ˆí„°ë§                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   [Agent ì‹¤í–‰] â”€â”€â–¶ [SmolagentsInstrumentor]                â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚                   [OTLP Exporter]                          â”‚
â”‚                           â”‚                                 â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚           â–¼               â–¼               â–¼                â”‚
â”‚      [Langfuse]      [Phoenix]      [ìì²´ ë°±ì—”ë“œ]          â”‚
â”‚                                                             â”‚
â”‚   ì¶”ì  ê°€ëŠ¥ ì •ë³´:                                           â”‚
â”‚   â€¢ LLM í˜¸ì¶œ íšŸìˆ˜/í† í°                                     â”‚
â”‚   â€¢ Tool ì‚¬ìš© ë‚´ì—­                                         â”‚
â”‚   â€¢ ì‹¤í–‰ ì‹œê°„/ë¹„ìš©                                         â”‚
â”‚   â€¢ ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 ì„¤ì¹˜ ë° ì„¤ì •

```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install opentelemetry-sdk opentelemetry-exporter-otlp
pip install openinference-instrumentation-smolagents
pip install langfuse
```

### 5.3 Langfuse í†µí•© ì½”ë“œ

```python
# monitoring.py
import os
import base64
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from opentelemetry import trace
from openinference.instrumentation.smolagents import SmolagentsInstrumentor


def setup_langfuse_monitoring():
    """Langfuse ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
    
    # Langfuse ì¸ì¦
    public_key = os.environ.get("LANGFUSE_PUBLIC_KEY")
    secret_key = os.environ.get("LANGFUSE_SECRET_KEY")
    base_url = os.environ.get("LANGFUSE_BASE_URL", "https://cloud.langfuse.com")
    
    auth_string = base64.b64encode(
        f"{public_key}:{secret_key}".encode()
    ).decode()
    
    # OpenTelemetry ì„¤ì •
    os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = f"{base_url}/api/public/otel"
    os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {auth_string}"
    
    # TracerProvider ì„¤ì •
    trace_provider = TracerProvider()
    trace_provider.add_span_processor(
        SimpleSpanProcessor(OTLPSpanExporter())
    )
    trace.set_tracer_provider(trace_provider)
    
    # smolagents Instrumentor í™œì„±í™”
    SmolagentsInstrumentor().instrument()
    
    print("âœ“ Langfuse ëª¨ë‹ˆí„°ë§ í™œì„±í™”ë¨")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    setup_langfuse_monitoring()
    
    # Agent ì‹¤í–‰
    from smolagents import CodeAgent, InferenceClientModel
    
    model = InferenceClientModel()
    agent = CodeAgent(tools=[], model=model)
    
    # ì´ ì‹¤í–‰ì€ Langfuseì— ì¶”ì ë¨
    result = agent.run("1ë¶€í„° 10ê¹Œì§€ì˜ í•©ì„ ê³„ì‚°í•´ì¤˜")
    print(result)
```

### 5.4 Phoenix í†µí•©

```python
# phoenix_monitoring.py
from phoenix.otel import register
from openinference.instrumentation.smolagents import SmolagentsInstrumentor


def setup_phoenix_monitoring():
    """Phoenix ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
    
    # Phoenix ë“±ë¡ (ë¡œì»¬ ìˆ˜ì§‘ê¸° ì‹œì‘ í•„ìš”)
    # í„°ë¯¸ë„ì—ì„œ: python -m phoenix.server.main serve
    register()
    
    # smolagents Instrumentor í™œì„±í™”
    SmolagentsInstrumentor().instrument()
    
    print("âœ“ Phoenix ëª¨ë‹ˆí„°ë§ í™œì„±í™”ë¨")
    print("  Phoenix UI: http://localhost:6006")


# ì‚¬ìš©
if __name__ == "__main__":
    setup_phoenix_monitoring()
    
    from smolagents import CodeAgent, InferenceClientModel
    
    agent = CodeAgent(tools=[], model=InferenceClientModel())
    result = agent.run("Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜")
```

### 5.5 ì»¤ìŠ¤í…€ ì†ì„± ì¶”ê°€

```python
from langfuse import observe, propagate_attributes, get_client


langfuse = get_client()


@observe()
def run_agent_with_tracking(agent, query: str, user_id: str = None):
    """ì¶”ì  ì†ì„±ì´ í¬í•¨ëœ Agent ì‹¤í–‰"""
    
    with propagate_attributes(
        user_id=user_id,
        session_id=f"session_{user_id}",
        tags=["production", "research-agent"],
        metadata={"source": "api"},
    ):
        result = agent.run(query)
        
        # íŠ¸ë ˆì´ìŠ¤ ì—…ë°ì´íŠ¸
        langfuse.update_current_trace(
            input=query,
            output=str(result),
        )
        
        return result
```

---

## 6. ì—ëŸ¬ í•¸ë“¤ë§ ë° í´ë°± ì „ëµ

### 6.1 ì¬ì‹œë„ ë¡œì§

```python
import time
from functools import wraps
from typing import Callable, Any


def retry_with_backoff(
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exponential_base: float = 2.0,
):
    """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    if attempt < max_retries - 1:
                        delay = min(
                            base_delay * (exponential_base ** attempt),
                            max_delay
                        )
                        print(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                        print(f"{delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                        time.sleep(delay)
            
            raise last_exception
        
        return wrapper
    return decorator


# ì‚¬ìš© ì˜ˆì‹œ
@retry_with_backoff(max_retries=3)
def run_agent_safely(agent, query: str):
    return agent.run(query)
```

### 6.2 í´ë°± ëª¨ë¸ ì „ëµ

```python
from smolagents import CodeAgent, InferenceClientModel, LiteLLMModel


class AgentWithFallback:
    """í´ë°± ëª¨ë¸ì„ ì§€ì›í•˜ëŠ” Agent"""
    
    def __init__(
        self,
        primary_model_id: str = "Qwen/Qwen2.5-72B-Instruct",
        fallback_model_id: str = "gpt-4o-mini",
        tools: list = None,
    ):
        self.tools = tools or []
        
        # ì£¼ ëª¨ë¸
        self.primary_model = InferenceClientModel(model_id=primary_model_id)
        self.primary_agent = CodeAgent(
            tools=self.tools,
            model=self.primary_model,
        )
        
        # í´ë°± ëª¨ë¸
        self.fallback_model = LiteLLMModel(model_id=fallback_model_id)
        self.fallback_agent = CodeAgent(
            tools=self.tools,
            model=self.fallback_model,
        )
    
    def run(self, query: str, max_retries: int = 2) -> str:
        """í´ë°±ì„ ì§€ì›í•˜ëŠ” ì‹¤í–‰"""
        
        # ì£¼ ëª¨ë¸ ì‹œë„
        for attempt in range(max_retries):
            try:
                result = self.primary_agent.run(query)
                return result
            except Exception as e:
                print(f"ì£¼ ëª¨ë¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
        
        # í´ë°± ëª¨ë¸ ì‹œë„
        print("í´ë°± ëª¨ë¸ë¡œ ì „í™˜...")
        try:
            result = self.fallback_agent.run(query)
            return result
        except Exception as e:
            raise RuntimeError(f"ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
agent = AgentWithFallback()
result = agent.run("ì˜¤ëŠ˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì¤˜")
```

### 6.3 íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, TimeoutError


class AgentWithTimeout:
    """íƒ€ì„ì•„ì›ƒì„ ì§€ì›í•˜ëŠ” Agent"""
    
    def __init__(self, agent, default_timeout: int = 60):
        self.agent = agent
        self.default_timeout = default_timeout
        self.executor = ThreadPoolExecutor(max_workers=2)
    
    def run(self, query: str, timeout: int = None) -> str:
        """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” ì‹¤í–‰"""
        timeout = timeout or self.default_timeout
        
        future = self.executor.submit(self.agent.run, query)
        
        try:
            result = future.result(timeout=timeout)
            return result
        except TimeoutError:
            future.cancel()
            return f"âš ï¸ íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ ë‚´ì— ì‘ë‹µí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"âš ï¸ ì˜¤ë¥˜: {str(e)}"


# ì‚¬ìš© ì˜ˆì‹œ
from smolagents import CodeAgent, InferenceClientModel

base_agent = CodeAgent(tools=[], model=InferenceClientModel())
safe_agent = AgentWithTimeout(base_agent, default_timeout=30)

result = safe_agent.run("ë³µì¡í•œ ê³„ì‚°ì„ ìˆ˜í–‰í•´ì¤˜")
```

### 6.4 ì—ëŸ¬ ë¡œê¹…

```python
import logging
from datetime import datetime


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("agent.log"),
        logging.StreamHandler(),
    ]
)
logger = logging.getLogger("smolagents")


class LoggedAgent:
    """ë¡œê¹…ì´ í¬í•¨ëœ Agent ë˜í¼"""
    
    def __init__(self, agent):
        self.agent = agent
    
    def run(self, query: str) -> str:
        start_time = datetime.now()
        request_id = f"req_{start_time.strftime('%Y%m%d_%H%M%S')}"
        
        logger.info(f"[{request_id}] ìš”ì²­ ì‹œì‘: {query[:50]}...")
        
        try:
            result = self.agent.run(query)
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"[{request_id}] ì™„ë£Œ ({duration:.2f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            logger.error(
                f"[{request_id}] ì‹¤íŒ¨ ({duration:.2f}ì´ˆ): {str(e)}",
                exc_info=True
            )
            raise
```

---

## 7. HuggingFace Spaces ë°°í¬

### 7.1 Gradio Space ë°°í¬

```yaml
# README.md (Space ì„¤ì •)
---
title: Research Agent
emoji: ğŸ”¬
colorFrom: blue
colorTo: purple
sdk: gradio
sdk_version: 4.44.0
app_file: app.py
pinned: false
---
```

```python
# app.py
import os
from smolagents import (
    CodeAgent,
    InferenceClientModel,
    DuckDuckGoSearchTool,
    GradioUI,
)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í° ê°€ì ¸ì˜¤ê¸°
HF_TOKEN = os.environ.get("HF_TOKEN")

# Agent ì„¤ì •
model = InferenceClientModel(
    model_id="Qwen/Qwen2.5-72B-Instruct",
    token=HF_TOKEN,
)

agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
    max_steps=5,
)

# Gradio UI ì‹¤í–‰
if __name__ == "__main__":
    ui = GradioUI(agent)
    ui.launch()
```

```text
# requirements.txt
smolagents[gradio]>=1.0.0
huggingface_hub
```

### 7.2 Docker Space ë°°í¬

```yaml
# README.md
---
title: Agent API
emoji: ğŸ¤–
colorFrom: green
colorTo: blue
sdk: docker
app_port: 7860
---
```

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY . .

# í¬íŠ¸ ì„¤ì •
EXPOSE 7860

# ì‹¤í–‰
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]
```

```python
# app.py (FastAPI)
from fastapi import FastAPI
from pydantic import BaseModel
import os

from smolagents import CodeAgent, InferenceClientModel


app = FastAPI()

# Agent ì´ˆê¸°í™”
model = InferenceClientModel(
    model_id="Qwen/Qwen2.5-72B-Instruct",
    token=os.environ.get("HF_TOKEN"),
)
agent = CodeAgent(tools=[], model=model)


class Query(BaseModel):
    text: str


@app.get("/")
def root():
    return {"status": "running"}


@app.post("/chat")
def chat(query: Query):
    result = agent.run(query.text)
    return {"response": str(result)}
```

### 7.3 Space í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# HuggingFace CLIë¡œ ì‹œí¬ë¦¿ ì„¤ì •
huggingface-cli login
huggingface-cli repo-secrets add HF_TOKEN <your-token> --repo-type space --repo <username/space-name>
```

---

## 8. ì„±ëŠ¥ ìµœì í™”

### 8.1 ëª¨ë¸ ìºì‹±

```python
from functools import lru_cache
from smolagents import InferenceClientModel, CodeAgent


@lru_cache(maxsize=1)
def get_model(model_id: str):
    """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìºì‹±"""
    return InferenceClientModel(model_id=model_id)


@lru_cache(maxsize=1)
def get_agent(model_id: str = "Qwen/Qwen2.5-72B-Instruct"):
    """Agent ì¸ìŠ¤í„´ìŠ¤ ìºì‹±"""
    model = get_model(model_id)
    return CodeAgent(tools=[], model=model)


# ì‚¬ìš© - ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©
agent = get_agent()
result = agent.run("ì§ˆë¬¸ 1")
result = agent.run("ì§ˆë¬¸ 2")  # ê°™ì€ Agent ì¬ì‚¬ìš©
```

### 8.2 ìŠ¤í… ìˆ˜ ìµœì í™”

```python
from smolagents import CodeAgent, InferenceClientModel


def create_optimized_agent(complexity: str = "medium"):
    """ë³µì¡ë„ì— ë”°ë¥¸ ìµœì í™”ëœ Agent ìƒì„±"""
    
    configs = {
        "simple": {"max_steps": 3, "planning_interval": None},
        "medium": {"max_steps": 5, "planning_interval": 3},
        "complex": {"max_steps": 10, "planning_interval": 2},
    }
    
    config = configs.get(complexity, configs["medium"])
    
    model = InferenceClientModel()
    agent = CodeAgent(
        tools=[],
        model=model,
        **config,
    )
    
    return agent
```

### 8.3 ë¹„ìš© ëª¨ë‹ˆí„°ë§

```python
class CostTracker:
    """LLM í˜¸ì¶œ ë¹„ìš© ì¶”ì """
    
    # ëŒ€ëµì ì¸ í† í°ë‹¹ ë¹„ìš© (ëª¨ë¸ë³„ë¡œ ë‹¤ë¦„)
    COST_PER_1K_TOKENS = {
        "gpt-4": 0.03,
        "gpt-4o": 0.005,
        "gpt-4o-mini": 0.00015,
        "claude-3-opus": 0.015,
        "claude-3-sonnet": 0.003,
    }
    
    def __init__(self):
        self.total_tokens = 0
        self.total_cost = 0.0
        self.calls = 0
    
    def track(self, tokens: int, model: str = "gpt-4o"):
        """ì‚¬ìš©ëŸ‰ ì¶”ì """
        self.total_tokens += tokens
        self.calls += 1
        
        cost_rate = self.COST_PER_1K_TOKENS.get(model, 0.01)
        cost = (tokens / 1000) * cost_rate
        self.total_cost += cost
        
        return cost
    
    def report(self) -> str:
        """ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸"""
        return f"""
        === ë¹„ìš© ë¦¬í¬íŠ¸ ===
        ì´ í˜¸ì¶œ ìˆ˜: {self.calls}
        ì´ í† í° ìˆ˜: {self.total_tokens:,}
        ì˜ˆìƒ ë¹„ìš©: ${self.total_cost:.4f}
        """


# ì‚¬ìš© ì˜ˆì‹œ
tracker = CostTracker()
# Agent ì‹¤í–‰ í›„ ì¶”ì 
tracker.track(tokens=1500, model="gpt-4o")
print(tracker.report())
```

---

## 9. ì‹¤ìŠµ ê³¼ì œ

### ê³¼ì œ 1: ë‰´ìŠ¤ ë¶„ì„ Agent
1. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
2. ë‰´ìŠ¤ ìš”ì•½ ë° ê°ì„± ë¶„ì„
3. ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¡œ ì¶œë ¥

### ê³¼ì œ 2: ì½”ë“œ ë¦¬ë·° Agent
1. GitHub URLì—ì„œ ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
2. ì½”ë“œ í’ˆì§ˆ ë¶„ì„ (ë¦°íŠ¸, ë³µì¡ë„)
3. ê°œì„  ì œì•ˆ ë¦¬í¬íŠ¸ ìƒì„±

### ê³¼ì œ 3: ë°°í¬ íŒŒì´í”„ë¼ì¸
1. Gradio UI êµ¬í˜„
2. HuggingFace Spaces ë°°í¬
3. Langfuse ëª¨ë‹ˆí„°ë§ ì—°ë™

### ê³¼ì œ 4: í”„ë¡œë•ì…˜ Agent ì‹œìŠ¤í…œ
1. FastAPI ì„œë²„ êµ¬í˜„
2. í´ë°± ëª¨ë¸ ì „ëµ ì ìš©
3. ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…
4. Docker ì»¨í…Œì´ë„ˆí™”

---

## 10. ìš”ì•½ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹¤ì „ í”„ë¡œì íŠ¸
- [ ] Research Agent: Multi-Agent ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] Coding Assistant: ì½”ë“œ ì‹¤í–‰/ë¶„ì„ Tool êµ¬í˜„
- [ ] Manager Agentë¡œ Agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

### ë°°í¬
- [ ] GradioUIë¡œ ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- [ ] FastAPIë¡œ REST API êµ¬í˜„
- [ ] HuggingFace Spaces ë°°í¬ (Gradio/Docker)
- [ ] í™˜ê²½ ë³€ìˆ˜ ë° ì‹œí¬ë¦¿ ê´€ë¦¬

### ëª¨ë‹ˆí„°ë§
- [ ] OpenTelemetry ì„¤ì •
- [ ] Langfuse ì—°ë™
- [ ] SmolagentsInstrumentor í™œì„±í™”
- [ ] ì»¤ìŠ¤í…€ ì†ì„± ì¶”ê°€

### ì•ˆì •ì„±
- [ ] ì¬ì‹œë„ ë¡œì§ (ì§€ìˆ˜ ë°±ì˜¤í”„)
- [ ] í´ë°± ëª¨ë¸ ì „ëµ
- [ ] íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
- [ ] ì—ëŸ¬ ë¡œê¹…

### ìµœì í™”
- [ ] ëª¨ë¸/Agent ì¸ìŠ¤í„´ìŠ¤ ìºì‹±
- [ ] ìŠ¤í… ìˆ˜ ìµœì í™”
- [ ] ë¹„ìš© ëª¨ë‹ˆí„°ë§

---

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- **smolagents ë¬¸ì„œ**: https://huggingface.co/docs/smolagents
- **Gradio ë¬¸ì„œ**: https://gradio.app/docs
- **HF Spaces**: https://huggingface.co/docs/hub/spaces

### ëª¨ë‹ˆí„°ë§
- **Langfuse**: https://langfuse.com
- **Phoenix (Arize)**: https://github.com/Arize-ai/phoenix
- **OpenTelemetry**: https://opentelemetry.io

### íŠœí† ë¦¬ì–¼
- **HF Agents Course**: https://huggingface.co/learn/agents-course
- **DeepLearning.AI smolagents Course**: https://learn.deeplearning.ai/courses/building-code-agents-with-hugging-face-smolagents

---

## ë‹¤ìŒ ë‹¨ê³„

**Part 6.1: Model Optimization**ì—ì„œ ë‹¤ë£° ë‚´ìš©:
- Quantization (ì–‘ìí™”) ê¸°ë²•
- ONNX ë³€í™˜ ë° ìµœì í™”
- vLLM ì„œë¹™
- ì¶”ë¡  ìµœì í™” ì „ëµ