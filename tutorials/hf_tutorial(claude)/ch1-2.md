# Part 1.2: Transformers 라이브러리 핵심

---

## 학습 목표
- Pipeline API로 빠른 프로토타이핑
- AutoClass 패턴 이해 (AutoModel, AutoTokenizer, AutoProcessor)
- `from_pretrained` / `push_to_hub` 워크플로우 마스터

---

## 1. 환경 설정

### 설치
```bash
# 기본 설치
pip install transformers

# PyTorch 백엔드 (권장)
pip install transformers torch

# TensorFlow 백엔드
pip install transformers tensorflow

# 전체 설치 (모든 옵션)
pip install transformers[torch,tf,sentencepiece,vision,audio]
```

### 버전 확인
```python
import transformers
print(transformers.__version__)  # 4.40+ 권장
```

---

## 2. Pipeline API - 빠른 프로토타이핑

### 2.1 Pipeline 개념

Pipeline = **전처리 → 모델 추론 → 후처리**를 하나로 묶은 고수준 API

```
Input Text → Tokenizer → Model → Post-processing → Output
```

### 2.2 기본 사용법

```python
from transformers import pipeline

# Task 이름만으로 파이프라인 생성 (기본 모델 자동 선택)
classifier = pipeline("sentiment-analysis")

# 추론
result = classifier("I love this product!")
print(result)
# [{'label': 'POSITIVE', 'score': 0.9998}]

# 배치 처리
results = classifier([
    "I love this product!",
    "This is terrible."
])
print(results)
# [{'label': 'POSITIVE', 'score': 0.9998},
#  {'label': 'NEGATIVE', 'score': 0.9996}]
```

### 2.3 지원 Task 목록

#### NLP Tasks
| Task | 설명 | Pipeline 이름 |
|------|------|---------------|
| Text Classification | 감성분석, 주제분류 | `text-classification`, `sentiment-analysis` |
| Token Classification | NER, POS 태깅 | `ner`, `token-classification` |
| Question Answering | 문맥 기반 QA | `question-answering` |
| Text Generation | 텍스트 생성 | `text-generation` |
| Summarization | 요약 | `summarization` |
| Translation | 번역 | `translation` |
| Fill-Mask | 마스크 토큰 예측 | `fill-mask` |
| Zero-Shot Classification | 제로샷 분류 | `zero-shot-classification` |

#### Vision Tasks
| Task | 설명 | Pipeline 이름 |
|------|------|---------------|
| Image Classification | 이미지 분류 | `image-classification` |
| Object Detection | 객체 탐지 | `object-detection` |
| Image Segmentation | 이미지 분할 | `image-segmentation` |
| Depth Estimation | 깊이 추정 | `depth-estimation` |

#### Audio Tasks
| Task | 설명 | Pipeline 이름 |
|------|------|---------------|
| ASR | 음성 인식 | `automatic-speech-recognition` |
| Audio Classification | 오디오 분류 | `audio-classification` |
| Text-to-Speech | 음성 합성 | `text-to-speech` |

#### Multimodal Tasks
| Task | 설명 | Pipeline 이름 |
|------|------|---------------|
| VQA | Visual Question Answering | `visual-question-answering` |
| Image-to-Text | 이미지 캡셔닝 | `image-to-text` |
| Document QA | 문서 질의응답 | `document-question-answering` |

### 2.4 커스텀 모델로 Pipeline 생성

```python
from transformers import pipeline

# 특정 모델 지정
classifier = pipeline(
    task="text-classification",
    model="nlptown/bert-base-multilingual-uncased-sentiment"
)

# 다국어 감성 분석
result = classifier("이 영화 정말 재미있어요!")
print(result)
# [{'label': '5 stars', 'score': 0.6823}]
```

### 2.5 주요 Task 실습

#### Text Generation (LLM)
```python
from transformers import pipeline

generator = pipeline(
    task="text-generation",
    model="gpt2"
)

output = generator(
    "The future of AI is",
    max_length=50,
    num_return_sequences=2,
    do_sample=True,
    temperature=0.7
)

for seq in output:
    print(seq["generated_text"])
    print("-" * 50)
```

#### Question Answering
```python
qa_pipeline = pipeline(
    task="question-answering",
    model="deepset/roberta-base-squad2"
)

context = """
Hugging Face is a company that develops tools for building 
machine learning applications. It was founded in 2016 and 
is headquartered in New York City.
"""

result = qa_pipeline(
    question="Where is Hugging Face located?",
    context=context
)
print(result)
# {'answer': 'New York City', 'score': 0.95, 'start': 142, 'end': 156}
```

#### Named Entity Recognition (NER)
```python
ner_pipeline = pipeline(
    task="ner",
    model="dslim/bert-base-NER",
    aggregation_strategy="simple"  # 토큰 병합
)

text = "Apple CEO Tim Cook announced new products in Cupertino."
entities = ner_pipeline(text)

for entity in entities:
    print(f"{entity['word']:15} | {entity['entity_group']:10} | {entity['score']:.4f}")
```

**출력:**
```
Apple           | ORG        | 0.9987
Tim Cook        | PER        | 0.9992
Cupertino       | LOC        | 0.9856
```

#### Summarization
```python
summarizer = pipeline(
    task="summarization",
    model="facebook/bart-large-cnn"
)

article = """
The Mars Orbiter Mission, also called Mangalyaan, is a space probe 
orbiting Mars since 24 September 2014. It was launched on 5 November 
2013 by the Indian Space Research Organisation (ISRO). It is India's 
first interplanetary mission and it made India the fourth country to 
achieve Mars orbit.
"""

summary = summarizer(
    article,
    max_length=50,
    min_length=20
)
print(summary[0]["summary_text"])
```

#### Image Classification
```python
from transformers import pipeline

classifier = pipeline(
    task="image-classification",
    model="google/vit-base-patch16-224"
)

# URL 또는 로컬 파일 경로
result = classifier("https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg")

for pred in result[:3]:
    print(f"{pred['label']:30} | {pred['score']:.4f}")
```

#### Visual Question Answering (VQA)
```python
vqa_pipeline = pipeline(
    task="visual-question-answering",
    model="Salesforce/blip-vqa-base"
)

result = vqa_pipeline(
    image="https://example.com/image.jpg",
    question="What color is the car?"
)
print(result)
# [{'answer': 'red'}]
```

### 2.6 Pipeline 성능 최적화

```python
from transformers import pipeline
import torch

# GPU 사용
pipe = pipeline(
    task="text-generation",
    model="gpt2",
    device=0  # GPU 0번, -1은 CPU
)

# 또는 device_map="auto" (자동 분배)
pipe = pipeline(
    task="text-generation",
    model="meta-llama/Llama-3.2-1B",
    device_map="auto",
    torch_dtype=torch.bfloat16  # 메모리 절약
)
```

---

## 3. AutoClass 패턴

### 3.1 AutoClass란?

- `from_pretrained()`으로 모델 아키텍처 자동 감지
- 동일한 코드로 다양한 모델 로드 가능
- Task별 전용 클래스 제공

### 3.2 주요 AutoClass 종류

#### 기본 클래스
```python
from transformers import (
    AutoConfig,          # 설정 자동 로드
    AutoTokenizer,       # 토크나이저 자동 로드
    AutoModel,           # 기본 모델 (헤드 없음)
    AutoProcessor,       # 멀티모달 프로세서
    AutoFeatureExtractor,# 오디오/이미지 전처리
    AutoImageProcessor,  # 이미지 전처리
)
```

#### Task별 AutoModel
```python
from transformers import (
    # NLP
    AutoModelForSequenceClassification,   # 분류
    AutoModelForTokenClassification,      # NER, POS
    AutoModelForQuestionAnswering,        # QA
    AutoModelForCausalLM,                 # GPT 계열 생성
    AutoModelForSeq2SeqLM,                # T5, BART 계열
    AutoModelForMaskedLM,                 # BERT 계열 MLM
    
    # Vision
    AutoModelForImageClassification,      # 이미지 분류
    AutoModelForObjectDetection,          # 객체 탐지
    AutoModelForDepthEstimation,          # 깊이 추정
    
    # Multimodal
    AutoModelForVision2Seq,               # 이미지 → 텍스트
)
```

### 3.3 AutoTokenizer 사용법

```python
from transformers import AutoTokenizer

# Hub에서 로드
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# 텍스트 토큰화
text = "Hello, how are you?"
encoded = tokenizer(text)
print(encoded)
# {'input_ids': [101, 7592, 1010, ...], 'attention_mask': [1, 1, 1, ...]}

# 배치 처리 + 패딩 + 자르기
texts = ["Short text", "This is a much longer sentence that needs truncation"]
batch = tokenizer(
    texts,
    padding=True,          # 짧은 문장에 패딩
    truncation=True,       # 긴 문장 자르기
    max_length=128,
    return_tensors="pt"    # PyTorch 텐서 반환
)
print(batch.keys())  # dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])
print(batch["input_ids"].shape)  # torch.Size([2, 128])

# 디코딩
decoded = tokenizer.decode(batch["input_ids"][0], skip_special_tokens=True)
print(decoded)
```

### 3.4 AutoModel 사용법

```python
from transformers import AutoModel, AutoTokenizer

model_name = "bert-base-uncased"

# 토크나이저와 모델 로드
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# 추론
text = "Hello, world!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

# 출력 확인
print(outputs.last_hidden_state.shape)  
# torch.Size([1, 5, 768])  # [batch, seq_len, hidden_dim]
```

### 3.5 Task별 AutoModel 실습

#### 텍스트 분류
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "distilbert-base-uncased-finetuned-sst-2-english"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 추론
text = "I really enjoyed this movie!"
inputs = tokenizer(text, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    probabilities = torch.softmax(logits, dim=-1)
    
predicted_class = torch.argmax(probabilities, dim=-1).item()
labels = model.config.id2label
print(f"Prediction: {labels[predicted_class]}")
print(f"Probabilities: {probabilities}")
```

#### 텍스트 생성 (Causal LM)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "gpt2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 텍스트 생성
prompt = "The future of artificial intelligence"
inputs = tokenizer(prompt, return_tensors="pt")

# generate() 메서드 사용
output_ids = model.generate(
    **inputs,
    max_new_tokens=50,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    pad_token_id=tokenizer.eos_token_id
)

generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(generated_text)
```

#### Chat 형식 (LLM)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "meta-llama/Llama-3.2-1B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Chat 형식
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is machine learning?"}
]

# Chat template 적용
input_text = tokenizer.apply_chat_template(
    messages, 
    tokenize=False, 
    add_generation_prompt=True
)
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

# 생성
output_ids = model.generate(
    **inputs,
    max_new_tokens=200,
    do_sample=True,
    temperature=0.7
)

response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(response)
```

### 3.6 Vision 모델 (AutoImageProcessor)

```python
from transformers import AutoImageProcessor, AutoModelForImageClassification
from PIL import Image
import requests
import torch

model_name = "google/vit-base-patch16-224"

# 이미지 프로세서 & 모델 로드
processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForImageClassification.from_pretrained(model_name)

# 이미지 로드
url = "https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 전처리
inputs = processor(images=image, return_tensors="pt")

# 추론
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    
predicted_idx = logits.argmax(-1).item()
print(f"Predicted: {model.config.id2label[predicted_idx]}")
```

### 3.7 Multimodal (AutoProcessor)

```python
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image
import requests
import torch

model_name = "Salesforce/blip-image-captioning-base"

processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForVision2Seq.from_pretrained(model_name)

# 이미지 로드
url = "https://example.com/image.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 전처리 (이미지 + 텍스트 프롬프트)
inputs = processor(images=image, return_tensors="pt")

# 캡션 생성
output_ids = model.generate(**inputs, max_new_tokens=50)
caption = processor.decode(output_ids[0], skip_special_tokens=True)
print(f"Caption: {caption}")
```

---

## 4. from_pretrained / push_to_hub 워크플로우

### 4.1 from_pretrained 옵션

```python
from transformers import AutoModel, AutoTokenizer

# 기본 로드
model = AutoModel.from_pretrained("bert-base-uncased")

# 특정 revision (버전/브랜치)
model = AutoModel.from_pretrained(
    "bert-base-uncased",
    revision="v1.0"  # 또는 commit hash, branch name
)

# 양자화 로드 (메모리 절약)
model = AutoModel.from_pretrained(
    "meta-llama/Llama-3.2-1B",
    load_in_8bit=True,
    device_map="auto"
)

# 4-bit 양자화
model = AutoModel.from_pretrained(
    "meta-llama/Llama-3.2-1B",
    load_in_4bit=True,
    device_map="auto"
)

# dtype 지정
import torch
model = AutoModel.from_pretrained(
    "bert-base-uncased",
    torch_dtype=torch.float16
)

# 로컬 디렉토리에서 로드
model = AutoModel.from_pretrained("./my_saved_model")
```

### 4.2 save_pretrained - 로컬 저장

```python
from transformers import AutoModel, AutoTokenizer

model_name = "bert-base-uncased"
save_dir = "./my_model"

# 모델 & 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# 로컬에 저장
tokenizer.save_pretrained(save_dir)
model.save_pretrained(save_dir)

# 저장된 파일 확인
import os
print(os.listdir(save_dir))
# ['config.json', 'model.safetensors', 'tokenizer.json', 
#  'tokenizer_config.json', 'vocab.txt', 'special_tokens_map.json']
```

### 4.3 push_to_hub - Hub에 업로드

```python
from transformers import AutoModel, AutoTokenizer
from huggingface_hub import login

# 로그인 (토큰 필요)
login()

model_name = "bert-base-uncased"
repo_id = "my-username/my-bert-model"

# 모델 & 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Hub에 업로드
tokenizer.push_to_hub(repo_id)
model.push_to_hub(repo_id)

print(f"Uploaded to: https://huggingface.co/{repo_id}")
```

### 4.4 Private 저장소로 업로드

```python
# Private 저장소로 업로드
model.push_to_hub(
    repo_id="my-username/my-private-model",
    private=True
)
```

### 4.5 PR로 업로드 (협업)

```python
# Pull Request 생성
model.push_to_hub(
    repo_id="organization/model-name",
    create_pr=True,
    commit_message="Add fine-tuned weights"
)
```

### 4.6 전체 워크플로우 예시

```python
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
from huggingface_hub import login

# 1. 로그인
login()

# 2. 데이터 로드
dataset = load_dataset("imdb")

# 3. 모델 & 토크나이저 로드
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, 
    num_labels=2
)

# 4. 데이터 전처리
def tokenize_fn(examples):
    return tokenizer(
        examples["text"], 
        truncation=True, 
        padding="max_length",
        max_length=256
    )

tokenized_dataset = dataset.map(tokenize_fn, batched=True)

# 5. 학습 설정
repo_id = "my-username/distilbert-imdb"

training_args = TrainingArguments(
    output_dir=repo_id,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    push_to_hub=True,           # 자동 업로드
    hub_model_id=repo_id,
)

# 6. Trainer로 학습
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"].select(range(1000)),
    eval_dataset=tokenized_dataset["test"].select(range(200)),
)

trainer.train()

# 7. 최종 업로드
trainer.push_to_hub()
tokenizer.push_to_hub(repo_id)
```

---

## 5. 실용 팁

### 5.1 캐시 관리

```python
import os

# 캐시 디렉토리 확인/변경
print(os.environ.get("HF_HOME", "~/.cache/huggingface"))

# 환경변수로 캐시 경로 변경
os.environ["HF_HOME"] = "/path/to/custom/cache"

# 캐시 비활성화 (다운로드만)
from transformers import AutoModel
model = AutoModel.from_pretrained(
    "bert-base-uncased",
    cache_dir=None,
    local_files_only=False
)
```

### 5.2 오프라인 사용

```python
# 사전에 다운로드
from huggingface_hub import snapshot_download
snapshot_download("bert-base-uncased", local_dir="./bert-offline")

# 오프라인 환경에서 사용
os.environ["HF_HUB_OFFLINE"] = "1"
model = AutoModel.from_pretrained("./bert-offline")
```

### 5.3 Trust Remote Code

일부 모델은 커스텀 코드 실행 필요:

```python
# 커스텀 코드 신뢰 (주의해서 사용)
model = AutoModel.from_pretrained(
    "microsoft/phi-2",
    trust_remote_code=True
)
```

### 5.4 Attention Output 접근

```python
from transformers import AutoModel

model = AutoModel.from_pretrained(
    "bert-base-uncased",
    output_attentions=True,
    output_hidden_states=True
)

outputs = model(**inputs)
print(outputs.attentions)        # 각 레이어 attention weights
print(outputs.hidden_states)     # 각 레이어 hidden states
```

---

## 6. 실습 과제

### 과제 1: 멀티태스크 Pipeline 비교
```python
"""
목표: 동일 텍스트에 대해 여러 NLP task 수행
요구사항:
1. 감성 분석
2. NER
3. 요약 (긴 텍스트인 경우)
4. 결과 비교 분석
"""

def analyze_text(text: str):
    # TODO: 구현하기
    pass

text = """
Apple CEO Tim Cook announced new iPhone models at the company's 
headquarters in Cupertino, California. The event was well received 
by investors and customers alike.
"""

analyze_text(text)
```

### 과제 2: 커스텀 분류 모델 배포
```python
"""
목표: 학습된 모델을 Hub에 배포하고 다시 로드
요구사항:
1. 사전학습 모델 로드
2. 커스텀 라벨로 설정 변경
3. Hub에 업로드
4. 새로 로드하여 추론
"""

def deploy_custom_classifier(
    base_model: str,
    labels: list[str],
    repo_id: str
):
    # TODO: 구현하기
    pass
```

---

## 7. 참고 링크

### 공식 문서
- [Transformers Quick Tour](https://huggingface.co/docs/transformers/quicktour)
- [Pipeline Tutorial](https://huggingface.co/docs/transformers/pipeline_tutorial)
- [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)
- [Tokenizer Guide](https://huggingface.co/docs/transformers/main_classes/tokenizer)
- [Model Guide](https://huggingface.co/docs/transformers/main_classes/model)
- [Pipelines API Reference](https://huggingface.co/docs/transformers/main_classes/pipelines)

### GitHub
- [Transformers Repository](https://github.com/huggingface/transformers)

### 추가 학습
- [NLP Course](https://huggingface.co/learn/nlp-course)
- [Tasks Overview](https://huggingface.co/tasks)

---

## 8. 핵심 요약

| 목적 | 방법 | 예시 |
|------|------|------|
| 빠른 프로토타이핑 | `pipeline()` | `pipeline("sentiment-analysis")` |
| 토크나이저 로드 | `AutoTokenizer.from_pretrained()` | `AutoTokenizer.from_pretrained("bert-base-uncased")` |
| 모델 로드 | `AutoModelForXxx.from_pretrained()` | `AutoModelForSequenceClassification.from_pretrained(...)` |
| 로컬 저장 | `save_pretrained()` | `model.save_pretrained("./my_model")` |
| Hub 업로드 | `push_to_hub()` | `model.push_to_hub("username/model")` |
| 이미지 전처리 | `AutoImageProcessor` | `AutoImageProcessor.from_pretrained("vit-base")` |
| 멀티모달 | `AutoProcessor` | `AutoProcessor.from_pretrained("blip-base")` |

### Pipeline vs AutoClass 선택 기준

| 상황 | 추천 |
|------|------|
| 빠른 테스트/데모 | `pipeline()` |
| 커스텀 전처리 필요 | AutoTokenizer + AutoModel |
| 파인튜닝 | AutoModelForXxx |
| 배치 추론 최적화 | AutoModel + 직접 구현 |
| 프로덕션 배포 | AutoModel + 최적화 |