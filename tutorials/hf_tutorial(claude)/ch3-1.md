# Part 3.1: Small LLM ì„ íƒê³¼ ì–‘ìí™”

---

## í•™ìŠµ ëª©í‘œ
- ì£¼ìš” Small LLM ëª¨ë¸ ì´í•´ ë° ì„ íƒ ê¸°ì¤€
- ì–‘ìí™”(Quantization) ê°œë…ê³¼ BitsAndBytes ì‚¬ìš©ë²•
- ëª¨ë¸ ë¡œë”© ìµœì í™” ê¸°ë²•
- QLoRA ê¸°ë°˜ íš¨ìœ¨ì  íŒŒì¸íŠœë‹

---

## 1. Small Language Model (SLM) ê°œìš”

### 1.1 SLMì´ë€?

**Small Language Model (SLM)**: 1B~10B íŒŒë¼ë¯¸í„° ë²”ìœ„ì˜ ê²½ëŸ‰í™”ëœ ì–¸ì–´ ëª¨ë¸

```
LLM (70B+)  â†’  ê³ ì„±ëŠ¥, ê³ ë¹„ìš©, ì„œë²„ í•„ìˆ˜
SLM (1B-10B) â†’  íš¨ìœ¨ì , ì €ë¹„ìš©, ì—£ì§€ ë°°í¬ ê°€ëŠ¥
```

### 1.2 SLM í™œìš© ì‹œë‚˜ë¦¬ì˜¤

| ì‹œë‚˜ë¦¬ì˜¤ | ì¶”ì²œ ëª¨ë¸ í¬ê¸° | ì´ìœ  |
|---------|--------------|------|
| ëª¨ë°”ì¼/ì—£ì§€ ë°°í¬ | 0.5B - 3B | ë©”ëª¨ë¦¬ ì œí•œ, ë¹ ë¥¸ ì‘ë‹µ |
| ê°œì¸ GPU (16GB) | 3B - 8B | RTX 3090/4090 |
| ê°œë°œ/í”„ë¡œí† íƒ€ì´í•‘ | 1B - 7B | ë¹ ë¥¸ ì‹¤í—˜ ë°˜ë³µ |
| RAG íŒŒì´í”„ë¼ì¸ | 3B - 8B | ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ íš¨ìœ¨ |
| Function Calling | 7B - 14B | Tool use ëŠ¥ë ¥ |

---

## 2. ì£¼ìš” Small LLM ëª¨ë¸ ë¹„êµ

### 2.1 ëª¨ë¸ íŒ¨ë°€ë¦¬ ê°œìš”

| ëª¨ë¸ | ê°œë°œì‚¬ | í¬ê¸° ë²”ìœ„ | ë¼ì´ì„ ìŠ¤ | íŠ¹ì§• |
|------|--------|----------|---------|------|
| **Llama 3.2** | Meta | 1B, 3B | Llama 3.2 License | ë²”ìš©ì„±, ì»¤ë®¤ë‹ˆí‹° ì§€ì› |
| **Qwen 2.5** | Alibaba | 0.5B~72B | Apache 2.0 | ë‹¤êµ­ì–´, ê¸´ ì»¨í…ìŠ¤íŠ¸, ì½”ë”© |
| **Gemma 2** | Google | 2B, 9B, 27B | Gemma License | ì¶”ë¡  ì†ë„, ì•ˆì •ì„± |
| **Phi-3/4** | Microsoft | 3.8B, 7B, 14B | MIT | ì¶”ë¡ /ìˆ˜í•™, ì—£ì§€ ìµœì í™” |
| **SmolLM** | HuggingFace | 135M~1.7B | Apache 2.0 | ì´ˆê²½ëŸ‰, ì™„ì „ ì˜¤í”ˆ |
| **Mistral** | Mistral AI | 7B | Apache 2.0 | ì„±ëŠ¥/íš¨ìœ¨ ê· í˜• |

### 2.2 ìš©ë„ë³„ ì¶”ì²œ ëª¨ë¸

```python
# ìš©ë„ë³„ ì¶”ì²œ ëª¨ë¸ (2024-2025 ê¸°ì¤€)

recommendations = {
    # ë²”ìš© ëŒ€í™”
    "general_chat": [
        "Qwen/Qwen2.5-7B-Instruct",
        "meta-llama/Llama-3.2-3B-Instruct",
        "google/gemma-2-9b-it",
    ],
    
    # ì½”ë”©
    "coding": [
        "Qwen/Qwen2.5-Coder-7B-Instruct",
        "deepseek-ai/deepseek-coder-6.7b-instruct",
        "codellama/CodeLlama-7b-Instruct-hf",
    ],
    
    # ìˆ˜í•™/ì¶”ë¡ 
    "reasoning": [
        "microsoft/Phi-3-mini-4k-instruct",
        "Qwen/Qwen2.5-Math-7B-Instruct",
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    ],
    
    # ë‹¤êµ­ì–´
    "multilingual": [
        "Qwen/Qwen2.5-7B-Instruct",  # 29+ languages
        "google/gemma-2-9b-it",       # 140+ languages
    ],
    
    # ì´ˆê²½ëŸ‰ (ì—£ì§€/ëª¨ë°”ì¼)
    "edge": [
        "HuggingFaceTB/SmolLM2-1.7B-Instruct",
        "Qwen/Qwen2.5-0.5B-Instruct",
        "meta-llama/Llama-3.2-1B-Instruct",
    ],
}
```

### 2.3 ëª¨ë¸ ì¹´ë“œ ë¹„êµ ë¡œë“œ

```python
from transformers import AutoConfig
from huggingface_hub import HfApi

def compare_models(model_ids: list[str]):
    """ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„¤ì • ë¹„êµ"""
    api = HfApi()
    
    for model_id in model_ids:
        print(f"\n{'='*50}")
        print(f"Model: {model_id}")
        print('='*50)
        
        # ì„¤ì • ë¡œë“œ
        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"Hidden size: {config.hidden_size}")
        print(f"Num layers: {config.num_hidden_layers}")
        print(f"Num attention heads: {config.num_attention_heads}")
        print(f"Vocab size: {config.vocab_size}")
        
        # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        model_info = api.model_info(model_id)
        print(f"Downloads (last month): {model_info.downloads:,}")
        print(f"Likes: {model_info.likes}")
        print(f"License: {model_info.card_data.license if model_info.card_data else 'N/A'}")

# ë¹„êµ ì‹¤í–‰
compare_models([
    "Qwen/Qwen2.5-3B-Instruct",
    "meta-llama/Llama-3.2-3B-Instruct",
    "microsoft/Phi-3-mini-4k-instruct",
])
```

---

## 3. ê¸°ë³¸ ëª¨ë¸ ë¡œë”©

### 3.1 AutoModelForCausalLM ê¸°ë³¸ ì‚¬ìš©

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = "Qwen/Qwen2.5-3B-Instruct"

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_id)

# ëª¨ë¸ ë¡œë“œ (ê¸°ë³¸: fp32)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",  # GPU ìë™ ë°°ì¹˜
    torch_dtype=torch.bfloat16,  # ë©”ëª¨ë¦¬ ì ˆì•½
)

print(f"Model dtype: {model.dtype}")
print(f"Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB")
```

### 3.2 Chat Template ì ìš©

```python
# ëŒ€í™” í˜•ì‹ ì •ì˜
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Hello! How are you?"}
]

# Chat template ì ìš©
prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

print("Formatted prompt:")
print(prompt)

# í† í°í™” ë° ì¶”ë¡ 
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"\nResponse:\n{response}")
```

### 3.3 Pipeline ì‚¬ìš© (ê°„í¸)

```python
from transformers import pipeline
import torch

pipe = pipeline(
    "text-generation",
    model="Qwen/Qwen2.5-3B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

messages = [
    {"role": "user", "content": "Pythonìœ¼ë¡œ í€µì†ŒíŠ¸ êµ¬í˜„í•´ì¤˜"}
]

output = pipe(messages, max_new_tokens=500)
print(output[0]["generated_text"][-1]["content"])
```

---

## 4. ì–‘ìí™” (Quantization)

### 4.1 ì–‘ìí™”ë€?

```
ì›ë³¸ (fp32)  â†’  ì–‘ìí™”  â†’  ì €ì •ë°€ë„ (int8/int4)
 32 bits         â†“         8/4 bits
 í° ë©”ëª¨ë¦¬       â†“         ì‘ì€ ë©”ëª¨ë¦¬ (1/4 ~ 1/8)
```

| ì–‘ìí™” ë°©ì‹ | ë©”ëª¨ë¦¬ ì ˆì•½ | ì •í™•ë„ ì†ì‹¤ | ì†ë„ |
|------------|-----------|-----------|------|
| fp16/bf16 | 2x | ê±°ì˜ ì—†ìŒ | ë¹ ë¦„ |
| int8 | 4x | ì•½ê°„ | ì¤‘ê°„ |
| int4 (NF4) | 8x | ìˆìŒ | ëŠë¦¼ |
| GPTQ/AWQ | 8x | ì ìŒ | ë¹ ë¦„ |

### 4.2 BitsAndBytes 8-bit ì–‘ìí™”

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

model_id = "meta-llama/Llama-3.2-3B-Instruct"

# 8-bit ì–‘ìí™” ì„¤ì •
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto",
)

# ë©”ëª¨ë¦¬ í™•ì¸
print(f"Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB")
# ì•½ 3GB (ì›ë³¸ 6GB ëŒ€ë¹„ ~50% ì ˆì•½)
```

### 4.3 BitsAndBytes 4-bit ì–‘ìí™” (NF4)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

model_id = "Qwen/Qwen2.5-7B-Instruct"

# 4-bit NF4 ì–‘ìí™” ì„¤ì • (QLoRAìš© ìµœì )
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # NF4 (ì •ê·œë¶„í¬ ìµœì í™”)
    bnb_4bit_compute_dtype=torch.bfloat16,  # ê³„ì‚°ì€ bf16
    bnb_4bit_use_double_quant=True,  # ì´ì¤‘ ì–‘ìí™” (ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½)
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto",
)

# ë©”ëª¨ë¦¬ í™•ì¸
print(f"Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB")
# ì•½ 4GB (ì›ë³¸ 14GB ëŒ€ë¹„ ~70% ì ˆì•½)
```

### 4.4 ì–‘ìí™” ì˜µì…˜ ìƒì„¸ ì„¤ëª…

```python
from transformers import BitsAndBytesConfig
import torch

# ì™„ì „í•œ QLoRA ì–‘ìí™” ì„¤ì •
quantization_config = BitsAndBytesConfig(
    # ê¸°ë³¸ ì„¤ì •
    load_in_4bit=True,  # 4-bit ì–‘ìí™” í™œì„±í™”
    
    # ì–‘ìí™” íƒ€ì…
    bnb_4bit_quant_type="nf4",  
    # "nf4": Normal Float 4 (ì •ê·œë¶„í¬ ê°€ì •, ì¶”ì²œ)
    # "fp4": Float Point 4
    
    # ê³„ì‚° ë°ì´í„° íƒ€ì…
    bnb_4bit_compute_dtype=torch.bfloat16,
    # ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•´ ê³„ì‚°í•  ë•Œì˜ dtype
    # bf16: ë” ë¹ ë¥¸ ê³„ì‚°, ì•½ê°„ ë‚®ì€ ì •ë°€ë„
    # fp16: í‘œì¤€ ë°˜ì •ë°€ë„
    # fp32: ìµœê³  ì •ë°€ë„, ëŠë¦¼
    
    # ì´ì¤‘ ì–‘ìí™”
    bnb_4bit_use_double_quant=True,
    # ì–‘ìí™” ìƒìˆ˜ë„ ì–‘ìí™” â†’ íŒŒë¼ë¯¸í„°ë‹¹ ~0.4bit ì¶”ê°€ ì ˆì•½
    
    # 8-bit ì „ìš© ì˜µì…˜
    # llm_int8_threshold=6.0,  # ì´ìƒì¹˜ ê°ì§€ ì„ê³„ê°’
    # llm_int8_skip_modules=["lm_head"],  # ì–‘ìí™” ì œì™¸ ëª¨ë“ˆ
    # llm_int8_enable_fp32_cpu_offload=True,  # CPU ì˜¤í”„ë¡œë“œ
)
```

### 4.5 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

def load_model_with_memory_check(model_id: str, quant_type: str = None):
    """ë‹¤ì–‘í•œ ì–‘ìí™” ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ë° ë©”ëª¨ë¦¬ í™•ì¸"""
    
    torch.cuda.empty_cache()
    
    if quant_type == "4bit":
        config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
    elif quant_type == "8bit":
        config = BitsAndBytesConfig(load_in_8bit=True)
    else:
        config = None
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=config,
        device_map="auto",
        torch_dtype=torch.bfloat16 if config is None else None,
    )
    
    memory = model.get_memory_footprint() / 1e9
    print(f"{quant_type or 'bf16':>6}: {memory:.2f} GB")
    
    del model
    torch.cuda.empty_cache()
    return memory

# ë¹„êµ ì‹¤í–‰
model_id = "Qwen/Qwen2.5-7B-Instruct"
print(f"\n{model_id} Memory Comparison:")
print("-" * 30)

# ì£¼ì˜: ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ ì •í™•í•œ ë©”ëª¨ë¦¬ ì¸¡ì • ê°€ëŠ¥
# load_model_with_memory_check(model_id, None)    # bf16
# load_model_with_memory_check(model_id, "8bit")  # int8
# load_model_with_memory_check(model_id, "4bit")  # int4
```

---

## 5. PEFTì™€ QLoRA íŒŒì¸íŠœë‹

### 5.1 LoRA ê°œë…

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LoRA ê°œë…                     â”‚
â”‚                                                 â”‚
â”‚  ì›ë³¸ ê°€ì¤‘ì¹˜ W (frozen)  +  Î”W = A Ã— B         â”‚
â”‚     [d Ã— d]                 [d Ã— r] Ã— [r Ã— d]  â”‚
â”‚                                                 â”‚
â”‚  r << d  â†’  í•™ìŠµ íŒŒë¼ë¯¸í„° ëŒ€í­ ê°ì†Œ             â”‚
â”‚  ì˜ˆ: d=4096, r=16 â†’ 99.6% íŒŒë¼ë¯¸í„° ì ˆì•½        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 LoraConfig ì„¤ì •

```python
from peft import LoraConfig, TaskType

# LoRA ì„¤ì •
lora_config = LoraConfig(
    # í•µì‹¬ íŒŒë¼ë¯¸í„°
    r=16,  # LoRA rank (8~64 ì¶”ì²œ, ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥â†‘)
    lora_alpha=32,  # ìŠ¤ì¼€ì¼ë§ íŒ©í„° (ë³´í†µ rì˜ 2ë°°)
    lora_dropout=0.05,  # ë“œë¡­ì•„ì›ƒ
    
    # íƒ€ê²Ÿ ëª¨ë“ˆ
    target_modules="all-linear",  # ëª¨ë“  Linear ë ˆì´ì–´
    # ë˜ëŠ” ëª…ì‹œì ìœ¼ë¡œ:
    # target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    
    # íƒœìŠ¤í¬ íƒ€ì…
    task_type=TaskType.CAUSAL_LM,
    
    # ì¶”ê°€ í•™ìŠµ ëª¨ë“ˆ (ì„ë² ë”© í¬í•¨ ì‹œ)
    modules_to_save=["lm_head", "embed_tokens"],
    
    # ê¸°íƒ€
    bias="none",  # bias í•™ìŠµ ì—¬ë¶€ ("none", "all", "lora_only")
)
```

### 5.3 QLoRA ì „ì²´ íŒŒì¸íŠœë‹ ì˜ˆì œ

```python
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, SFTConfig

# 1. ëª¨ë¸ ID
model_id = "Qwen/Qwen2.5-3B-Instruct"

# 2. 4-bit ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# 4. ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™”)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
)

# 5. LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules="all-linear",
    task_type="CAUSAL_LM",
    modules_to_save=["lm_head", "embed_tokens"],
)

# 6. ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("trl-lib/Capybara", split="train[:1000]")

# 7. SFTConfig ì„¤ì •
training_args = SFTConfig(
    output_dir="./qwen-qlora-finetuned",
    max_seq_length=512,
    packing=True,  # ì§§ì€ ìƒ˜í”Œë“¤ì„ ë¬¶ì–´ì„œ íš¨ìœ¨ì  í•™ìŠµ
    
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
    
    # ì˜µí‹°ë§ˆì´ì €
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    optim="adamw_torch_fused",
    
    # ì •ë°€ë„
    bf16=True,
    
    # ë¡œê¹…
    logging_steps=10,
    save_strategy="epoch",
    
    # Hub ì—…ë¡œë“œ (ì„ íƒ)
    # push_to_hub=True,
    # hub_model_id="username/model-name",
)

# 8. SFTTrainer ìƒì„±
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    peft_config=lora_config,
    processing_class=tokenizer,
)

# 9. í•™ìŠµ ì‹¤í–‰
trainer.train()

# 10. ì €ì¥
trainer.save_model()
```

### 5.4 í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ë¡œë“œ ë° ì¶”ë¡ 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
base_model_id = "Qwen/Qwen2.5-3B-Instruct"
adapter_path = "./qwen-qlora-finetuned"

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

# LoRA ì–´ëŒ‘í„° ë¡œë“œ
model = PeftModel.from_pretrained(model, adapter_path)

# ì¶”ë¡ 
messages = [{"role": "user", "content": "Hello!"}]
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=100)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### 5.5 LoRA ì–´ëŒ‘í„° ë³‘í•© (Merge)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (fp16)
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
)

# LoRA ì–´ëŒ‘í„° ë¡œë“œ
model = PeftModel.from_pretrained(base_model, "./qwen-qlora-finetuned")

# ë³‘í•©
merged_model = model.merge_and_unload()

# ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
merged_model.save_pretrained("./qwen-merged")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
tokenizer.save_pretrained("./qwen-merged")

# Hubì— ì—…ë¡œë“œ
# merged_model.push_to_hub("username/qwen-merged")
# tokenizer.push_to_hub("username/qwen-merged")
```

---

## 6. ì¶”ë¡  ìµœì í™”

### 6.1 Flash Attention 2

```python
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="flash_attention_2",  # Flash Attention 2 ì‚¬ìš©
)
```

### 6.2 SDPA (Scaled Dot-Product Attention)

```python
# PyTorch 2.0+ ê¸°ë³¸ ì œê³µ
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa",  # PyTorch native
)
```

### 6.3 ì¶”ë¡  ì†ë„ ë¹„êµ ìœ í‹¸ë¦¬í‹°

```python
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

def benchmark_inference(model_id, quantization=None, attn_impl="sdpa", num_runs=5):
    """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation=attn_impl,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # ì…ë ¥ ì¤€ë¹„
    prompt = "Write a short story about a robot learning to paint."
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ì›Œë°ì—…
    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=50)
    
    # ë²¤ì¹˜ë§ˆí¬
    times = []
    total_tokens = 0
    
    for _ in range(num_runs):
        torch.cuda.synchronize()
        start = time.time()
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=100)
        
        torch.cuda.synchronize()
        end = time.time()
        
        times.append(end - start)
        total_tokens += outputs.shape[1] - inputs["input_ids"].shape[1]
    
    avg_time = sum(times) / len(times)
    tokens_per_sec = total_tokens / sum(times)
    
    print(f"Model: {model_id}")
    print(f"Attention: {attn_impl}")
    print(f"Average time: {avg_time:.3f}s")
    print(f"Tokens/sec: {tokens_per_sec:.1f}")
    
    return tokens_per_sec
```

---

## 7. ì‹¤ìŠµ ê³¼ì œ

### ê³¼ì œ 1: ëª¨ë¸ ë¹„êµ ë¶„ì„

```python
"""
ëª©í‘œ: 3ê°œ ì´ìƒì˜ Small LLM ë¹„êµ ë¶„ì„
ìš”êµ¬ì‚¬í•­:
1. ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ í’ˆì§ˆ ë¹„êµ
2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
3. ì¶”ë¡  ì†ë„ ë¹„êµ
4. ê²°ê³¼ë¥¼ í‘œë¡œ ì •ë¦¬
"""

class ModelComparator:
    def __init__(self, model_ids: list[str]):
        self.model_ids = model_ids
        self.results = {}
    
    def compare_quality(self, prompt: str):
        """ì‘ë‹µ í’ˆì§ˆ ë¹„êµ"""
        pass
    
    def compare_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ"""
        pass
    
    def compare_speed(self, num_runs: int = 5):
        """ì¶”ë¡  ì†ë„ ë¹„êµ"""
        pass
    
    def generate_report(self) -> str:
        """ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""
        pass
```

### ê³¼ì œ 2: QLoRA ì»¤ìŠ¤í…€ íŒŒì¸íŠœë‹

```python
"""
ëª©í‘œ: íŠ¹ì • ë„ë©”ì¸ì— Small LLM íŒŒì¸íŠœë‹
ìš”êµ¬ì‚¬í•­:
1. ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°ì…‹ ì¤€ë¹„ (ì˜ˆ: í•œêµ­ì–´ Q&A)
2. QLoRA íŒŒì¸íŠœë‹ ì‹¤í–‰
3. íŒŒì¸íŠœë‹ ì „í›„ ì„±ëŠ¥ ë¹„êµ
4. ì–´ëŒ‘í„° Hubì— ì—…ë¡œë“œ
"""

def prepare_korean_dataset():
    """í•œêµ­ì–´ ë°ì´í„°ì…‹ ì¤€ë¹„"""
    pass

def run_qlora_finetuning(dataset, model_id):
    """QLoRA íŒŒì¸íŠœë‹ ì‹¤í–‰"""
    pass

def evaluate_model(model, test_data):
    """ëª¨ë¸ í‰ê°€"""
    pass
```

---

## 8. ì°¸ê³  ë§í¬

### ê³µì‹ ë¬¸ì„œ
- [Transformers Quantization Guide](https://huggingface.co/docs/transformers/en/quantization/overview)
- [BitsAndBytes Documentation](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [TRL SFTTrainer](https://huggingface.co/docs/trl/en/sft_trainer)

### ëª¨ë¸ ì»¬ë ‰ì…˜
- [Qwen Collection](https://huggingface.co/collections/Qwen)
- [Llama Collection](https://huggingface.co/collections/meta-llama)
- [Gemma Collection](https://huggingface.co/collections/google/gemma-2-release)
- [Phi Collection](https://huggingface.co/collections/microsoft/phi-4)

### ìœ ìš©í•œ ë¸”ë¡œê·¸/íŠœí† ë¦¬ì–¼
- [Making LLMs accessible with 4-bit quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
- [Fine-tune LLMs in 2025](https://www.philschmid.de/fine-tune-llms-in-2025)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)

### Leaderboard
- [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)

---

## 9. í•µì‹¬ ìš”ì•½

### Small LLM ì„ íƒ í”Œë¡œìš°ì°¨íŠ¸

```
ì‹œì‘
  â”‚
  â”œâ”€ ë©”ëª¨ë¦¬ < 8GB?
  â”‚    â”œâ”€ Yes â†’ SmolLM, Qwen-0.5B/1.5B, Llama-1B
  â”‚    â””â”€ No â†“
  â”‚
  â”œâ”€ ìš©ë„?
  â”‚    â”œâ”€ ì½”ë”© â†’ Qwen-Coder, DeepSeek-Coder, CodeLlama
  â”‚    â”œâ”€ ìˆ˜í•™/ì¶”ë¡  â†’ Phi-3/4, Qwen-Math, DeepSeek-R1
  â”‚    â”œâ”€ ë‹¤êµ­ì–´ â†’ Qwen2.5, Gemma 2
  â”‚    â””â”€ ë²”ìš© â†’ Llama 3.2, Qwen2.5, Gemma 2
  â”‚
  â”œâ”€ ì–‘ìí™” í•„ìš”?
  â”‚    â”œâ”€ ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ 4-bit (NF4)
  â”‚    â”œâ”€ ê· í˜• â†’ 8-bit
  â”‚    â””â”€ ìµœê³  í’ˆì§ˆ â†’ bf16
  â”‚
  â””â”€ íŒŒì¸íŠœë‹ í•„ìš”?
       â”œâ”€ Yes â†’ QLoRA + SFTTrainer
       â””â”€ No â†’ ë°”ë¡œ ì‚¬ìš©
```

### ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê°€ì´ë“œ

| ëª¨ë¸ í¬ê¸° | fp16 | 8-bit | 4-bit |
|----------|------|-------|-------|
| 1B | ~2 GB | ~1 GB | ~0.5 GB |
| 3B | ~6 GB | ~3 GB | ~1.5 GB |
| 7B | ~14 GB | ~7 GB | ~4 GB |
| 13B | ~26 GB | ~13 GB | ~7 GB |

> ğŸ’¡ **Tip**: QLoRA í•™ìŠµ ì‹œ 4-bit ëª¨ë¸ + LoRA ì–´ëŒ‘í„°ë¡œ 7B ëª¨ë¸ë„ 16GB GPUì—ì„œ í•™ìŠµ ê°€ëŠ¥!