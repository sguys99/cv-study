# Hugging Face 튜토리얼 Part 6.1: Model Optimization

## 학습 목표
- Quantization (양자화) 기법 이해 및 적용: GPTQ, AWQ, bitsandbytes
- ONNX 변환 및 ONNX Runtime 최적화
- vLLM 서빙: PagedAttention, Continuous Batching
- 추론 최적화 전략: KV Cache, FlashAttention, Speculative Decoding

---

## 1. Quantization (양자화) 개요

### 1.1 양자화란?

```
┌─────────────────────────────────────────────────────────────┐
│                    Quantization 개념                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   FP32 (32-bit)  ──▶  INT8 (8-bit)  ──▶  INT4 (4-bit)     │
│   ═══════════════      ═══════════      ═══════════        │
│   4 bytes/param        1 byte/param     0.5 bytes/param    │
│   기준 정확도          ~1% 손실         ~2-3% 손실          │
│                                                             │
│   메모리 절감:                                              │
│   • 8-bit: 4x 감소                                         │
│   • 4-bit: 8x 감소                                         │
│                                                             │
│   속도 향상:                                                │
│   • 메모리 대역폭 감소 → 추론 속도 향상                     │
│   • INT 연산이 FP 연산보다 빠름                            │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 양자화 방법 비교

| 방법 | 특징 | 장점 | 단점 | 사용 시점 |
|------|------|------|------|-----------|
| **bitsandbytes** | 동적 양자화, 캘리브레이션 불필요 | 간편, QLoRA 지원 | 속도 느림 | 빠른 프로토타이핑, 파인튜닝 |
| **GPTQ** | PTQ, 캘리브레이션 필요 | 빠른 추론 | 양자화 시간 필요 | GPU 프로덕션 배포 |
| **AWQ** | 활성화 인식, 중요 가중치 보존 | 가장 빠른 추론, 정확도 유지 | 양자화 시간 필요 | 고성능 프로덕션 |
| **GGUF** | llama.cpp 포맷, CPU 친화적 | CPU에서 실행 가능 | GPU 최적화 부족 | CPU/Edge 배포 |
| **HQQ** | 캘리브레이션 불필요, 빠른 양자화 | 빠른 양자화, 좋은 정확도 | 비교적 신규 | 빠른 양자화 필요 시 |

### 1.3 메모리 요구량 비교 (7B 모델 기준)

| 정밀도 | 메모리 | 설명 |
|--------|--------|------|
| FP32 | ~28 GB | 기준 |
| FP16/BF16 | ~14 GB | 2x 절감 |
| INT8 | ~7 GB | 4x 절감 |
| INT4 | ~3.5 GB | 8x 절감 |

---

## 2. bitsandbytes 양자화

### 2.1 설치

```bash
pip install bitsandbytes accelerate transformers
```

### 2.2 8-bit 양자화 (LLM.int8())

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "meta-llama/Llama-3.1-8B-Instruct"

# 8-bit 설정
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,  # Outlier 임계값
)

# 모델 로드
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto",
)

tokenizer = AutoTokenizer.from_pretrained(model_id)

# 추론
inputs = tokenizer("Hello, how are you?", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### 2.3 4-bit 양자화 (NF4/FP4)

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "meta-llama/Llama-3.1-8B-Instruct"

# 4-bit 설정 (권장: NF4 + Double Quantization)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",           # "nf4" 또는 "fp4"
    bnb_4bit_use_double_quant=True,      # 이중 양자화 (메모리 추가 절감)
    bnb_4bit_compute_dtype=torch.bfloat16,  # 연산 정밀도
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto",
)

# 메모리 사용량 확인
print(f"메모리 사용량: {model.get_memory_footprint() / 1e9:.2f} GB")
```

### 2.4 QLoRA용 4-bit 설정

```python
from transformers import BitsAndBytesConfig
import torch

# QLoRA 최적 설정
qlora_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# PEFT와 함께 사용
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=qlora_config,
    device_map="auto",
)

# k-bit 학습 준비
model = prepare_model_for_kbit_training(model)

# LoRA 설정
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
```

---

## 3. GPTQ 양자화

### 3.1 GPTQ 개념

```
┌─────────────────────────────────────────────────────────────┐
│                      GPTQ 알고리즘                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   1. 캘리브레이션 데이터셋으로 활성화 수집                  │
│   2. 레이어별 양자화 (Inverse Hessian 기반)                 │
│   3. 중요 가중치 오류 최소화                                │
│   4. 추론 시 INT4 → FP16 동적 역양자화                      │
│                                                             │
│   특징:                                                     │
│   • Post-Training Quantization (PTQ)                        │
│   • 레이어별 순차 처리                                      │
│   • GPU 추론에 최적화                                       │
│   • Marlin 커널로 추가 가속                                 │
└─────────────────────────────────────────────────────────────┘
```

### 3.2 설치

```bash
pip install gptqmodel optimum accelerate
# 또는
pip install auto-gptq  # Legacy (deprecated)
```

### 3.3 GPTQ 양자화 수행

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "meta-llama/Llama-3.1-8B-Instruct"

# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(model_id)

# GPTQ 설정
gptq_config = GPTQConfig(
    bits=4,                      # 4-bit 양자화
    group_size=128,              # 그룹 크기
    dataset="c4",                # 캘리브레이션 데이터셋
    desc_act=True,               # 활성화 기반 정렬
)

# 양자화 수행 (시간 소요)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=gptq_config,
    device_map="auto",
)

# 저장
model.save_pretrained("llama-8b-gptq")
tokenizer.save_pretrained("llama-8b-gptq")
```

### 3.4 사전 양자화된 GPTQ 모델 사용

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

# Hub에서 GPTQ 모델 로드
model_id = "TheBloke/Llama-2-7B-GPTQ"

# Marlin 백엔드 사용 (추가 가속)
gptq_config = GPTQConfig(bits=4, backend="marlin")

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=gptq_config,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)

# 추론
inputs = tokenizer("The capital of France is", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0]))
```

### 3.5 GPTQModel 직접 사용

```python
from gptqmodel import GPTQModel, QuantizeConfig

model_id = "meta-llama/Llama-3.1-8B-Instruct"

# 양자화 설정
quantize_config = QuantizeConfig(
    bits=4,
    group_size=128,
)

# 모델 로드 및 양자화
model = GPTQModel.load(model_id, quantize_config)

# 캘리브레이션 데이터 준비
calibration_data = [
    "This is a sample text for calibration.",
    "Another example sentence for the model.",
    # ... 더 많은 샘플
]

# 양자화 실행
model.quantize(calibration_data)

# 저장
model.save("llama-gptq-4bit")
```

---

## 4. AWQ 양자화

### 4.1 AWQ 개념

```
┌─────────────────────────────────────────────────────────────┐
│              AWQ (Activation-aware Weight Quantization)     │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   핵심 아이디어:                                            │
│   "모든 가중치가 동등하게 중요하지 않다"                    │
│                                                             │
│   1. 활성화 값 관찰 → 중요 가중치 식별                     │
│   2. 중요 가중치(~1%)는 더 높은 정밀도 유지                │
│   3. 나머지 가중치는 공격적 양자화                         │
│                                                             │
│   장점:                                                     │
│   • GPTQ 대비 빠른 추론 속도                               │
│   • 유사하거나 더 좋은 정확도                              │
│   • vLLM, TGI 등과 호환                                    │
└─────────────────────────────────────────────────────────────┘
```

### 4.2 설치

```bash
pip install autoawq
```

### 4.3 AWQ 양자화 수행

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_id = "meta-llama/Llama-3.1-8B-Instruct"

# 모델 로드
model = AutoAWQForCausalLM.from_pretrained(
    model_id,
    safetensors=True,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 양자화 설정
quant_config = {
    "w_bit": 4,
    "q_group_size": 128,
    "zero_point": True,
    "version": "GEMM",  # "GEMM" 또는 "GEMV"
}

# 양자화 실행
model.quantize(
    tokenizer,
    quant_config=quant_config,
)

# 저장
model.save_quantized("llama-8b-awq")
tokenizer.save_pretrained("llama-8b-awq")
```

### 4.4 사전 양자화된 AWQ 모델 사용

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig

model_id = "TheBloke/Llama-2-7B-AWQ"

# AWQ 설정
awq_config = AwqConfig(
    bits=4,
    version="GEMM",
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=awq_config,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)

# 추론
inputs = tokenizer("Hello!", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))
```

### 4.5 vLLM에서 AWQ 사용

```python
from vllm import LLM, SamplingParams

# AWQ 모델 로드
llm = LLM(
    model="TheBloke/Llama-2-7B-AWQ",
    quantization="awq",
)

# 샘플링 파라미터
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=256,
)

# 추론
outputs = llm.generate(["What is machine learning?"], sampling_params)
print(outputs[0].outputs[0].text)
```

---

## 5. ONNX 변환 및 최적화

### 5.1 ONNX 개요

```
┌─────────────────────────────────────────────────────────────┐
│                    ONNX 워크플로우                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   [PyTorch Model] ──▶ [ONNX Export] ──▶ [ONNX Model]       │
│                             │                               │
│                             ▼                               │
│                    [Graph Optimization]                     │
│                    • Operator Fusion                        │
│                    • Constant Folding                       │
│                    • Memory Planning                        │
│                             │                               │
│                             ▼                               │
│                   [ONNX Runtime 추론]                       │
│                   • CPU / GPU / TensorRT                    │
│                   • 1.5-3x 속도 향상                        │
└─────────────────────────────────────────────────────────────┘
```

### 5.2 설치

```bash
pip install optimum[onnx] onnxruntime-gpu
# 또는 CPU만 사용
pip install optimum[onnx] onnxruntime
```

### 5.3 CLI로 ONNX 변환

```bash
# 기본 변환
optimum-cli export onnx \
    --model meta-llama/Llama-3.2-1B \
    onnx_llama/

# 최적화 포함 변환
optimum-cli export onnx \
    --model distilbert-base-uncased \
    --optimize O2 \
    onnx_distilbert/

# FP16 변환
optimum-cli export onnx \
    --model bert-base-uncased \
    --dtype fp16 \
    --device cuda \
    onnx_bert_fp16/
```

**최적화 레벨:**
- `O1`: 기본 일반 최적화
- `O2`: 기본 + 확장 최적화, Transformer Fusion
- `O3`: O2 + GELU 근사
- `O4`: O3 + Mixed Precision (FP16, GPU 전용)

### 5.4 Python으로 ONNX 변환

```python
from optimum.onnxruntime import ORTModelForCausalLM
from transformers import AutoTokenizer

model_id = "meta-llama/Llama-3.2-1B"

# 변환 및 로드 (export=True)
model = ORTModelForCausalLM.from_pretrained(
    model_id,
    export=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)

# 저장
model.save_pretrained("onnx_llama/")
tokenizer.save_pretrained("onnx_llama/")

# 추론
inputs = tokenizer("Hello, world!", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0]))
```

### 5.5 ONNX 모델 로드 및 추론

```python
from optimum.onnxruntime import ORTModelForCausalLM, ORTModelForSequenceClassification
from transformers import AutoTokenizer, pipeline

# 사전 변환된 ONNX 모델 로드
model = ORTModelForCausalLM.from_pretrained(
    "onnx-community/Llama-3.2-1B",
    subfolder="onnx",
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")

# Pipeline 사용
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)

result = pipe("The future of AI is", max_new_tokens=50)
print(result[0]["generated_text"])
```

### 5.6 ONNX Runtime 양자화

```python
from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"

# 모델 로드 및 ONNX 변환
model = ORTModelForSequenceClassification.from_pretrained(model_id, export=True)

# Quantizer 생성
quantizer = ORTQuantizer.from_pretrained(model)

# 동적 양자화 설정
qconfig = AutoQuantizationConfig.avx512_vnni(
    is_static=False,
    per_channel=True,
)

# 양자화 실행
quantizer.quantize(
    save_dir="onnx_quantized/",
    quantization_config=qconfig,
)

# 양자화된 모델 로드
quantized_model = ORTModelForSequenceClassification.from_pretrained("onnx_quantized/")
```

### 5.7 ONNX 그래프 최적화

```python
from optimum.onnxruntime import ORTOptimizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import OptimizationConfig

# 모델 로드
model = ORTModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    export=True,
)

# Optimizer 생성
optimizer = ORTOptimizer.from_pretrained(model)

# 최적화 설정
optimization_config = OptimizationConfig(
    optimization_level=2,  # O2 레벨
    enable_transformers_specific_optimizations=True,
)

# 최적화 실행
optimizer.optimize(
    save_dir="onnx_optimized/",
    optimization_config=optimization_config,
)
```

---

## 6. vLLM 서빙

### 6.1 vLLM 개요

```
┌─────────────────────────────────────────────────────────────┐
│                      vLLM 핵심 기술                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   1. PagedAttention                                         │
│      • KV Cache를 OS 가상 메모리처럼 관리                  │
│      • 비연속적 메모리 블록 할당                            │
│      • 메모리 낭비 90% 이상 감소                           │
│      • 2-4x 더 큰 배치 크기 가능                           │
│                                                             │
│   2. Continuous Batching                                    │
│      • 요청 완료 즉시 새 요청 추가                         │
│      • 정적 배칭 대비 2-10x 처리량 향상                    │
│      • GPU 유휴 시간 최소화                                │
│                                                             │
│   3. 추가 최적화                                            │
│      • CUDA Graph: 커널 실행 오버헤드 감소                 │
│      • FlashAttention: 메모리 효율적 어텐션                │
│      • Prefix Caching: 공통 프리픽스 재사용                │
│      • Chunked Prefill: 긴 프롬프트 분할 처리              │
└─────────────────────────────────────────────────────────────┘
```

### 6.2 설치

```bash
# pip 설치 (권장)
pip install vllm

# 특정 CUDA 버전
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121
```

### 6.3 기본 사용법

```python
from vllm import LLM, SamplingParams

# 모델 로드
llm = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    tensor_parallel_size=1,     # GPU 수
    gpu_memory_utilization=0.9,  # GPU 메모리 사용률
)

# 샘플링 파라미터
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    max_tokens=256,
    stop=["<|eot_id|>"],
)

# 단일 추론
prompts = ["What is the capital of France?"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
```

### 6.4 배치 추론

```python
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-3.1-8B-Instruct")

# 여러 프롬프트 배치 처리
prompts = [
    "Explain quantum computing in simple terms.",
    "Write a haiku about programming.",
    "What are the benefits of exercise?",
    "Describe the water cycle.",
]

sampling_params = SamplingParams(
    temperature=0.8,
    max_tokens=100,
)

# 배치 추론 (자동 Continuous Batching)
outputs = llm.generate(prompts, sampling_params)

for i, output in enumerate(outputs):
    print(f"\n[{i+1}] {output.outputs[0].text}")
```

### 6.5 OpenAI 호환 API 서버

```bash
# 서버 시작
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --api-key your-api-key

# 또는 Python 코드로
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000
```

**클라이언트 사용:**

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="your-api-key",
)

response = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"},
    ],
    temperature=0.7,
    max_tokens=100,
)

print(response.choices[0].message.content)
```

### 6.6 양자화 모델 사용

```python
from vllm import LLM, SamplingParams

# GPTQ 모델
llm_gptq = LLM(
    model="TheBloke/Llama-2-7B-GPTQ",
    quantization="gptq",
)

# AWQ 모델
llm_awq = LLM(
    model="TheBloke/Llama-2-7B-AWQ",
    quantization="awq",
)

# INT8 양자화
llm_int8 = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    quantization="int8",
)

# FP8 양자화 (H100+)
llm_fp8 = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    quantization="fp8",
)
```

### 6.7 고급 설정

```python
from vllm import LLM, SamplingParams
from vllm.engine.arg_utils import EngineArgs

# 상세 설정
llm = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    
    # 메모리 설정
    gpu_memory_utilization=0.9,
    max_model_len=8192,           # 최대 컨텍스트 길이
    
    # 병렬화
    tensor_parallel_size=2,        # Tensor 병렬 (GPU 수)
    pipeline_parallel_size=1,      # Pipeline 병렬
    
    # 성능 최적화
    enable_prefix_caching=True,    # Prefix Caching 활성화
    enable_chunked_prefill=True,   # Chunked Prefill 활성화
    max_num_batched_tokens=8192,   # 배치당 최대 토큰
    max_num_seqs=256,              # 최대 동시 시퀀스
    
    # 양자화
    quantization="awq",
    
    # KV Cache
    kv_cache_dtype="auto",         # "auto", "fp8", "fp8_e5m2"
)
```

### 6.8 CLI 벤치마크

```bash
# 지연시간 벤치마크
vllm bench latency \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --input-tokens 32 \
    --output-tokens 128 \
    --batch-size 8

# 처리량 벤치마크
vllm bench throughput \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --num-prompts 1000

# 서빙 벤치마크
vllm bench serve \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --dataset-name sharegpt \
    --request-rate 10
```

---

## 7. 추론 최적화 전략

### 7.1 KV Cache 최적화

```
┌─────────────────────────────────────────────────────────────┐
│                    KV Cache 개념                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Without KV Cache:                                         │
│   매 토큰 생성마다 전체 시퀀스 재계산 → O(n²)              │
│                                                             │
│   With KV Cache:                                            │
│   이전 토큰의 K, V 저장 → 새 토큰만 계산 → O(n)            │
│                                                             │
│   [Token 1] ──▶ [K₁, V₁] ──┐                               │
│   [Token 2] ──▶ [K₂, V₂] ──┼──▶ [Cache] ──▶ [새 토큰]     │
│   [Token 3] ──▶ [K₃, V₃] ──┘        ▲                      │
│                                      │                      │
│                               [재사용]                      │
│                                                             │
│   메모리 사용량:                                            │
│   • 2 × num_layers × hidden_dim × seq_len × batch_size     │
│   • 8B 모델, 4K 컨텍스트: ~2GB per request                 │
└─────────────────────────────────────────────────────────────┘
```

### 7.2 KV Cache 압축 기법

```python
# HuggingFace에서 KV Cache 설정
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    attn_implementation="sdpa",  # Scaled Dot Product Attention
)

# 생성 시 use_cache 활성화
outputs = model.generate(
    inputs,
    max_new_tokens=100,
    use_cache=True,  # KV Cache 사용
)
```

### 7.3 FlashAttention

```
┌─────────────────────────────────────────────────────────────┐
│                   FlashAttention 원리                       │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   기존 Attention:                                           │
│   Q, K, V → GPU HBM ↔ SRAM 왕복 다수 → 느림                │
│                                                             │
│   FlashAttention:                                           │
│   • Tiling: 작은 블록으로 나눠 SRAM에서 처리               │
│   • Kernel Fusion: 여러 연산을 하나로 통합                 │
│   • Memory I/O 최소화                                      │
│                                                             │
│   성능:                                                     │
│   • 1.5-3x 속도 향상                                       │
│   • 메모리 사용량 감소                                     │
│   • 수학적으로 정확 (근사 아님)                            │
└─────────────────────────────────────────────────────────────┘
```

**FlashAttention 사용:**

```python
from transformers import AutoModelForCausalLM
import torch

# FlashAttention-2 활성화
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",  # FA2 사용
    device_map="auto",
)

# 또는 SDPA (Scaled Dot Product Attention)
model_sdpa = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype=torch.bfloat16,
    attn_implementation="sdpa",  # PyTorch 내장
    device_map="auto",
)
```

### 7.4 Speculative Decoding

```
┌─────────────────────────────────────────────────────────────┐
│                 Speculative Decoding                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   일반 추론:                                                │
│   [LLM] → 1 token → [LLM] → 1 token → ...                  │
│   (매 토큰마다 전체 모델 실행)                              │
│                                                             │
│   Speculative Decoding:                                     │
│   [Draft Model] → k tokens 제안                            │
│        ↓                                                    │
│   [Target Model] → 한 번에 검증                            │
│        ↓                                                    │
│   수락된 토큰만 사용                                        │
│                                                             │
│   조건:                                                     │
│   • Draft 모델 품질이 높을수록 효과적                      │
│   • 수락률(acceptance rate)이 중요                         │
│   • 2-3x 속도 향상 가능                                    │
└─────────────────────────────────────────────────────────────┘
```

**vLLM에서 Speculative Decoding:**

```python
from vllm import LLM, SamplingParams

# Draft 모델과 함께 로드
llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",  # Target (큰 모델)
    speculative_model="meta-llama/Llama-3.1-8B-Instruct",  # Draft (작은 모델)
    num_speculative_tokens=5,  # Draft가 생성할 토큰 수
    use_v2_block_manager=True,
)

sampling_params = SamplingParams(
    temperature=0.0,  # Greedy가 효과적
    max_tokens=256,
)

outputs = llm.generate(["Explain quantum computing"], sampling_params)
```

**HuggingFace에서 Assisted Generation:**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Target 모델 (큰 모델)
target_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-70B-Instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

# Assistant 모델 (작은 모델)
assistant_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-70B-Instruct")

inputs = tokenizer("The future of AI is", return_tensors="pt").to("cuda")

# Assisted Generation
outputs = target_model.generate(
    **inputs,
    assistant_model=assistant_model,
    max_new_tokens=100,
    do_sample=False,  # Greedy
)

print(tokenizer.decode(outputs[0]))
```

### 7.5 Grouped-Query Attention (GQA)

```
┌─────────────────────────────────────────────────────────────┐
│                    Attention 변형                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   MHA (Multi-Head Attention):                               │
│   Q heads = K heads = V heads = N                          │
│   KV Cache: 2 × N × d × seq_len                            │
│                                                             │
│   MQA (Multi-Query Attention):                              │
│   Q heads = N, K heads = V heads = 1                       │
│   KV Cache: 2 × 1 × d × seq_len (N배 감소!)                │
│   단점: 품질 저하 가능                                     │
│                                                             │
│   GQA (Grouped-Query Attention):                            │
│   Q heads = N, K heads = V heads = G (1 < G < N)           │
│   KV Cache: 2 × G × d × seq_len                            │
│   장점: MHA 품질 + MQA 효율의 균형                         │
│                                                             │
│   예: Llama 3.1 70B                                        │
│   • 64 Q heads, 8 KV heads (8개 그룹)                      │
│   • KV Cache 8x 감소                                       │
└─────────────────────────────────────────────────────────────┘
```

---

## 8. 벤치마크 및 비교

### 8.1 양자화 방법 성능 비교 (Llama 3.1 8B)

| 방법 | VRAM | 처리량 (tok/s) | 정확도 (MMLU) | 양자화 시간 |
|------|------|----------------|---------------|-------------|
| FP16 (baseline) | 16 GB | 50 | 68.5% | - |
| bitsandbytes 8-bit | 9 GB | 40 | 68.2% | 즉시 |
| bitsandbytes 4-bit | 5 GB | 35 | 67.0% | 즉시 |
| GPTQ 4-bit | 5 GB | 70 | 67.5% | ~20분 |
| AWQ 4-bit | 5 GB | 80 | 67.8% | ~10분 |

### 8.2 추론 프레임워크 비교

| 프레임워크 | 장점 | 단점 | 사용 사례 |
|------------|------|------|-----------|
| **HuggingFace** | 간편함, 광범위한 모델 지원 | 느린 추론 | 개발/프로토타입 |
| **vLLM** | 최고 처리량, PagedAttention | 메모리 사용량 | 프로덕션 서빙 |
| **TGI** | HF 통합, 쉬운 배포 | vLLM 대비 느림 | HF 에코시스템 |
| **ONNX Runtime** | 크로스플랫폼, CPU 최적화 | LLM 지원 제한적 | CPU 배포, Edge |
| **TensorRT-LLM** | NVIDIA 최적화, 최고 성능 | 복잡한 설정, NVIDIA 전용 | 대규모 프로덕션 |

### 8.3 벤치마크 코드

```python
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def benchmark_inference(model, tokenizer, prompt, num_runs=10, max_new_tokens=100):
    """추론 벤치마크"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Warmup
    for _ in range(3):
        model.generate(**inputs, max_new_tokens=max_new_tokens)
    
    # 벤치마크
    times = []
    for _ in range(num_runs):
        torch.cuda.synchronize()
        start = time.perf_counter()
        
        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
        
        torch.cuda.synchronize()
        end = time.perf_counter()
        times.append(end - start)
    
    avg_time = sum(times) / len(times)
    tokens_per_sec = max_new_tokens / avg_time
    
    print(f"평균 시간: {avg_time:.3f}s")
    print(f"처리량: {tokens_per_sec:.1f} tokens/sec")
    print(f"메모리: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")
    
    return avg_time, tokens_per_sec


# 사용 예시
model_id = "meta-llama/Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

benchmark_inference(model, tokenizer, "Explain machine learning")
```

---

## 9. 실습 과제

### 과제 1: 양자화 비교
1. 동일 모델에 bitsandbytes, GPTQ, AWQ 적용
2. 메모리 사용량, 추론 속도, 출력 품질 비교
3. 결과 표로 정리

### 과제 2: ONNX 최적화 파이프라인
1. BERT 모델 ONNX 변환
2. 그래프 최적화 적용
3. 동적 양자화 적용
4. PyTorch 대비 속도 비교

### 과제 3: vLLM 서빙
1. vLLM으로 OpenAI 호환 서버 구축
2. 배치 추론 성능 측정
3. Prefix Caching 효과 비교

### 과제 4: 최적화 조합
1. AWQ + vLLM + FlashAttention 조합
2. 기본 HuggingFace 대비 성능 비교
3. 비용 효율성 분석

---

## 10. 요약 체크리스트

### Quantization
- [ ] bitsandbytes: 8-bit, 4-bit NF4 설정
- [ ] GPTQ: GPTQConfig, Marlin 백엔드
- [ ] AWQ: autoawq, vLLM 연동
- [ ] 사전 양자화 모델 로드 (TheBloke 등)

### ONNX
- [ ] optimum-cli로 ONNX 변환
- [ ] ORTModel 클래스 사용
- [ ] 그래프 최적화 (O1-O4)
- [ ] ONNX Runtime 양자화

### vLLM
- [ ] PagedAttention 이해
- [ ] Continuous Batching 개념
- [ ] OpenAI 호환 API 서버
- [ ] 양자화 모델 서빙 (GPTQ, AWQ, FP8)

### 추론 최적화
- [ ] KV Cache 원리 및 관리
- [ ] FlashAttention 활성화
- [ ] Speculative Decoding 설정
- [ ] GQA/MQA 이해

---

## 참고 자료

### 공식 문서
- **HF Quantization**: https://huggingface.co/docs/transformers/quantization
- **GPTQ**: https://huggingface.co/docs/transformers/quantization/gptq
- **AWQ**: https://huggingface.co/docs/transformers/quantization/awq
- **bitsandbytes**: https://huggingface.co/docs/transformers/quantization/bitsandbytes
- **Optimum ONNX**: https://huggingface.co/docs/optimum-onnx
- **vLLM**: https://docs.vllm.ai/

### 논문 및 블로그
- **FlashAttention**: https://arxiv.org/abs/2205.14135
- **PagedAttention**: https://arxiv.org/abs/2309.06180
- **GPTQ**: https://arxiv.org/abs/2210.17323
- **AWQ**: https://arxiv.org/abs/2306.00978
- **NVIDIA Inference Optimization**: https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/

### 도구
- **TheBloke (양자화 모델)**: https://huggingface.co/TheBloke
- **ONNX Community**: https://huggingface.co/onnx-community

---

## 다음 단계

**Part 6.2: MLOps & Deployment**에서 다룰 내용:
- HuggingFace Inference Endpoints
- Text Generation Inference (TGI)
- Docker 컨테이너화
- Kubernetes 배포
- CI/CD 파이프라인
- 모니터링 및 로깅