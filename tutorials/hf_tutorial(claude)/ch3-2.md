# Hugging Face 튜토리얼 Part 3.2: Small LLM 파인튜닝 심화 및 배포

## 학습 목표
- 데이터셋 포맷 및 전처리 마스터 (Alpaca, ShareGPT, Conversational)
- SFTTrainer 고급 기능 활용 (Packing, Completion-only, NEFTune)
- GGUF 변환 및 llama.cpp 배포
- vLLM / TGI를 활용한 프로덕션 서빙
- LM Evaluation Harness로 모델 벤치마킹

---

## 1. 데이터셋 포맷 및 전처리

### 1.1 주요 데이터셋 포맷

파인튜닝에 사용되는 데이터셋은 크게 세 가지 형식으로 분류됩니다.

#### Alpaca 포맷 (Instruction-Response)
단일 턴 instruction-following 태스크에 적합합니다.

```python
# Alpaca 포맷 예시
{
    "instruction": "다음 문장을 영어로 번역하세요.",
    "input": "오늘 날씨가 좋습니다.",          # 선택적
    "output": "The weather is nice today."
}
```

#### ShareGPT 포맷 (Multi-turn Conversation)
멀티턴 대화 학습에 적합합니다.

```python
# ShareGPT 포맷 예시
{
    "conversations": [
        {"from": "human", "value": "Python에서 리스트를 정렬하는 방법은?"},
        {"from": "gpt", "value": "Python에서 리스트 정렬은 두 가지 방법이 있습니다..."},
        {"from": "human", "value": "역순 정렬은 어떻게 하나요?"},
        {"from": "gpt", "value": "reverse=True 파라미터를 사용하면 됩니다..."}
    ]
}
```

#### TRL Conversational 포맷 (권장)
TRL의 SFTTrainer가 네이티브로 지원하는 최신 포맷입니다.

```python
# TRL messages 포맷 (OpenAI 스타일)
{
    "messages": [
        {"role": "system", "content": "당신은 친절한 AI 어시스턴트입니다."},
        {"role": "user", "content": "Python에서 리스트를 정렬하는 방법은?"},
        {"role": "assistant", "content": "Python에서 리스트 정렬은..."}
    ]
}
```

### 1.2 데이터셋 변환 실습

```python
from datasets import load_dataset, Dataset

# Alpaca → TRL messages 변환
def alpaca_to_messages(example):
    messages = []
    
    # instruction + input 결합
    user_content = example["instruction"]
    if example.get("input"):
        user_content += f"\n\n{example['input']}"
    
    messages.append({"role": "user", "content": user_content})
    messages.append({"role": "assistant", "content": example["output"]})
    
    return {"messages": messages}

# 예시: Alpaca 데이터셋 변환
alpaca_dataset = load_dataset("tatsu-lab/alpaca", split="train")
converted_dataset = alpaca_dataset.map(alpaca_to_messages, remove_columns=alpaca_dataset.column_names)

print(converted_dataset[0])
# {'messages': [{'role': 'user', 'content': '...'}, {'role': 'assistant', 'content': '...'}]}
```

```python
# ShareGPT → TRL messages 변환
def sharegpt_to_messages(example):
    messages = []
    role_map = {"human": "user", "gpt": "assistant", "system": "system"}
    
    for turn in example["conversations"]:
        role = role_map.get(turn["from"], turn["from"])
        messages.append({"role": role, "content": turn["value"]})
    
    return {"messages": messages}

# 예시: ShareGPT 스타일 데이터셋 변환
sharegpt_dataset = load_dataset("anon8231489123/ShareGPT_Vicuna_unfiltered", split="train")
converted_sharegpt = sharegpt_dataset.map(sharegpt_to_messages, remove_columns=sharegpt_dataset.column_names)
```

### 1.3 Chat Template 적용

모델별로 다른 chat template을 사용합니다. SFTTrainer는 자동으로 적용합니다.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi there!"}
]

# Chat template 적용
formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
print(formatted)
```

**주요 모델별 Chat Template:**

| 모델 | Template 스타일 |
|------|----------------|
| Llama 3 | `<\|begin_of_text\|><\|start_header_id\|>system<\|end_header_id\|>...` |
| Qwen 2.5 | `<\|im_start\|>system\n...<\|im_end\|>` (ChatML) |
| Gemma 2 | `<start_of_turn>user\n...<end_of_turn>` |
| Mistral | `[INST] ... [/INST]` |

---

## 2. SFTTrainer 고급 기능

### 2.1 기본 SFT 학습 설정

```python
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, TaskType
from trl import SFTTrainer, SFTConfig
import torch

# 모델 로드 (4-bit 양자화)
model_id = "Qwen/Qwen2.5-3B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    attn_implementation="flash_attention_2",  # 또는 "sdpa"
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# 데이터셋 로드 (TRL 네이티브 포맷)
dataset = load_dataset("trl-lib/Capybara", split="train")

# LoRA 설정
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules="all-linear",
    task_type=TaskType.CAUSAL_LM,
)

# SFT 설정
training_args = SFTConfig(
    output_dir="./qwen-sft-advanced",
    max_length=2048,
    
    # 학습 파라미터
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    gradient_checkpointing=True,
    
    # Optimizer
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    weight_decay=0.01,
    
    # Mixed precision
    bf16=True,
    
    # Logging
    logging_steps=10,
    save_strategy="steps",
    save_steps=100,
    
    # 고급 옵션 (아래에서 상세 설명)
    packing=True,
    # dataset_text_field="text",  # 또는 messages 포맷 사용
)

# Trainer 생성 및 학습
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    peft_config=lora_config,
    processing_class=tokenizer,
)

trainer.train()
```

### 2.2 Packing: 효율적인 배치 구성

Packing은 짧은 샘플들을 하나의 시퀀스로 묶어 GPU 활용도를 극대화합니다.

```python
# Packing 활성화
training_args = SFTConfig(
    output_dir="./output",
    max_length=2048,
    packing=True,                    # Packing 활성화
    packing_strategy="bfd",          # "bfd" (Best-Fit Decreasing) 또는 "wrapped"
)

# Packing 동작 방식:
# - 여러 짧은 샘플을 max_length까지 연결
# - EOS 토큰으로 샘플 구분
# - position_ids를 통해 샘플 간 attention 격리
```

**Packing의 장단점:**

| 장점 | 단점 |
|------|------|
| 패딩 낭비 제거 | 샘플 간 경계 처리 복잡 |
| 학습 처리량 증가 (2-3배) | 일부 모델에서 품질 저하 가능 |
| GPU 메모리 효율 향상 | completion_only와 함께 사용 불가 |

### 2.3 Completion-Only Training (Assistant-Only Loss)

프롬프트(user 메시지)는 무시하고 응답(assistant 메시지)에만 loss를 계산합니다.

```python
# 방법 1: SFTConfig 설정 (권장)
training_args = SFTConfig(
    output_dir="./output",
    max_length=2048,
    packing=False,  # completion-only와 packing은 함께 사용 불가
    
    # Assistant 응답에만 loss 계산
    # messages 포맷 데이터셋 필요
    # 모델의 chat_template이 {% generation %} 키워드 지원 필요
)

# 방법 2: prompt-completion 포맷 사용
# 이 포맷을 사용하면 자동으로 completion에만 loss 계산
dataset = Dataset.from_dict({
    "prompt": ["질문1", "질문2"],
    "completion": ["답변1", "답변2"]
})

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
)
```

### 2.4 NEFTune: 노이즈 임베딩으로 성능 향상

학습 중 임베딩에 노이즈를 추가하여 일반화 성능을 향상시킵니다.

```python
training_args = SFTConfig(
    output_dir="./output",
    max_length=2048,
    
    # NEFTune 활성화
    neftune_noise_alpha=5,  # 권장: 5-15 사이 값
)

# NEFTune 효과:
# - AlpacaEval: 29.79% → 64.69% (Llama-2-7B)
# - 일반화 성능 향상
# - 학습 후 자동 비활성화
```

### 2.5 Padding-Free Training (Flash Attention 필수)

패딩 없이 시퀀스를 연결하여 메모리 효율을 극대화합니다.

```python
training_args = SFTConfig(
    output_dir="./output",
    max_length=2048,
    
    # Padding-free 모드
    padding_free=True,  # Flash Attention 2/3 필수
)

# 모델 로드 시 Flash Attention 활성화 필요
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    attn_implementation="flash_attention_2",
    ...
)
```

### 2.6 학습 완료 후 모델 저장

```python
# 학습 완료 후
trainer.save_model("./final_model")

# LoRA 어댑터만 저장됨
# 전체 모델이 필요하면 merge 수행

from peft import PeftModel

# Base 모델 + LoRA 병합
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
merged_model = PeftModel.from_pretrained(base_model, "./final_model")
merged_model = merged_model.merge_and_unload()

# 병합된 모델 저장
merged_model.save_pretrained("./merged_model")
tokenizer.save_pretrained("./merged_model")
```

---

## 3. GGUF 변환 및 llama.cpp 배포

### 3.1 GGUF 포맷 이해

GGUF(GPT-Generated Unified Format)는 llama.cpp에서 사용하는 최적화된 모델 포맷입니다.

**GGUF의 장점:**
- CPU/GPU 하이브리드 추론 지원
- 다양한 양자화 레벨 (1-bit ~ 8-bit)
- 단일 파일에 모든 메타데이터 포함
- llama.cpp, Ollama, LM Studio 등과 호환

### 3.2 llama.cpp 설치 및 변환

```bash
# llama.cpp 클론 및 빌드
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp

# Python 의존성 설치
pip install -r requirements.txt

# 빌드 (CPU만 사용 시)
cmake -B build
cmake --build build --config Release

# CUDA 지원 빌드
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

### 3.3 HuggingFace → GGUF 변환

```bash
# 1. HF 모델을 FP16 GGUF로 변환
python llama.cpp/convert_hf_to_gguf.py ./merged_model \
    --outfile model-f16.gguf \
    --outtype f16

# BF16 모델의 경우
python llama.cpp/convert_hf_to_gguf.py ./merged_model \
    --outfile model-bf16.gguf \
    --outtype bf16
```

### 3.4 GGUF 양자화

```bash
# 2. 양자화 수행
./build/bin/llama-quantize model-f16.gguf model-Q4_K_M.gguf Q4_K_M

# 사용 가능한 양자화 타입
./build/bin/llama-quantize --help
```

**주요 양자화 타입:**

| 타입 | 크기 (7B 기준) | 품질 | 용도 |
|------|---------------|------|------|
| Q2_K | ~2.9 GB | 낮음 | 극저사양 |
| Q3_K_M | ~3.7 GB | 보통 | 모바일/엣지 |
| Q4_K_M | ~4.1 GB | 좋음 | **일반 권장** |
| Q5_K_M | ~4.8 GB | 매우 좋음 | 품질 중시 |
| Q6_K | ~5.5 GB | 우수 | 높은 품질 필요 시 |
| Q8_0 | ~7.0 GB | 최상 | 품질 최우선 |

### 3.5 Importance Matrix를 활용한 고품질 양자화

저비트 양자화 시 품질 향상을 위해 importance matrix를 사용합니다.

```bash
# 1. Importance matrix 계산 (calibration 데이터 필요)
./build/bin/llama-imatrix \
    -m model-f16.gguf \
    -f calibration_data.txt \
    --chunk 512 \
    -o model-imatrix.dat \
    -ngl 99  # GPU 레이어 수

# 2. Importance matrix를 사용한 양자화
./build/bin/llama-quantize \
    --imatrix model-imatrix.dat \
    model-f16.gguf \
    model-Q4_K_M-imat.gguf \
    Q4_K_M
```

### 3.6 llama.cpp로 추론

```bash
# CLI 추론
./build/bin/llama-cli \
    -m model-Q4_K_M.gguf \
    -p "Hello, how are you?" \
    -n 100 \
    -ngl 99  # GPU에 올릴 레이어 수

# 대화 모드
./build/bin/llama-cli \
    -m model-Q4_K_M.gguf \
    -cnv \
    --chat-template chatml

# OpenAI 호환 서버 실행
./build/bin/llama-server \
    -m model-Q4_K_M.gguf \
    --port 8080 \
    -ngl 99
```

### 3.7 Python에서 llama-cpp-python 사용

```python
# 설치: pip install llama-cpp-python
# CUDA: CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

from llama_cpp import Llama

# 모델 로드
llm = Llama(
    model_path="./model-Q4_K_M.gguf",
    n_ctx=4096,       # Context length
    n_gpu_layers=99,  # GPU 레이어 (-1 = 전체)
    verbose=False,
)

# 추론
output = llm(
    "Q: What is the capital of France?\nA:",
    max_tokens=100,
    stop=["Q:", "\n\n"],
    echo=False,
)

print(output["choices"][0]["text"])

# Chat 모드
output = llm.create_chat_completion(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"},
    ],
    max_tokens=100,
)

print(output["choices"][0]["message"]["content"])
```

---

## 4. vLLM / TGI 프로덕션 서빙

### 4.1 서빙 솔루션 비교

| 특성 | vLLM | TGI | llama.cpp |
|------|------|-----|-----------|
| 최적화 | PagedAttention | Flash Attention | GGML/CPU 최적화 |
| 처리량 | 매우 높음 | 높음 | 보통 |
| 메모리 효율 | 매우 높음 | 높음 | 높음 |
| 지원 하드웨어 | NVIDIA GPU | NVIDIA/AMD/Intel | CPU/GPU 모두 |
| 양자화 | AWQ, GPTQ, FP8 | BitsAndBytes, GPTQ | GGUF (다양한 비트) |
| 프로덕션 기능 | 기본 | 풍부 (모니터링 등) | 기본 |
| 설정 복잡도 | 낮음 | 낮음 | 매우 낮음 |

### 4.2 vLLM 설치 및 사용

```bash
# 설치
pip install vllm
```

```python
from vllm import LLM, SamplingParams

# 모델 로드
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,    # GPU 수
    gpu_memory_utilization=0.9,
    max_model_len=4096,
    # quantization="awq",      # AWQ 양자화 모델 사용 시
)

# Sampling 파라미터
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512,
)

# 배치 추론
prompts = [
    "What is machine learning?",
    "Explain quantum computing.",
    "What is the meaning of life?",
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}\n")
```

**vLLM OpenAI 호환 서버:**

```bash
# 서버 실행
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --port 8000 \
    --tensor-parallel-size 1

# API 호출
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-7B-Instruct",
        "messages": [
            {"role": "user", "content": "Hello!"}
        ],
        "max_tokens": 100
    }'
```

### 4.3 TGI (Text Generation Inference)

```bash
# Docker로 실행 (권장)
model=Qwen/Qwen2.5-7B-Instruct
volume=$PWD/data

docker run --gpus all --shm-size 1g \
    -p 8080:80 \
    -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id $model \
    --max-input-tokens 2048 \
    --max-total-tokens 4096 \
    --quantize bitsandbytes-nf4  # 선택적 양자화
```

**TGI API 사용:**

```python
from huggingface_hub import InferenceClient

client = InferenceClient(model="http://localhost:8080")

# 스트리밍 생성
for token in client.text_generation(
    "What is deep learning?",
    max_new_tokens=100,
    stream=True
):
    print(token, end="")

# Chat completion (OpenAI 호환)
response = client.chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    max_tokens=100,
)
print(response.choices[0].message.content)
```

```bash
# cURL로 직접 호출
curl http://localhost:8080/v1/chat/completions \
    -X POST \
    -H "Content-Type: application/json" \
    -d '{
        "model": "tgi",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is deep learning?"}
        ],
        "max_tokens": 100,
        "stream": true
    }'
```

### 4.4 Hugging Face Inference Endpoints

클라우드 배포를 위한 관리형 서비스입니다.

```python
from huggingface_hub import InferenceEndpoint, create_inference_endpoint

# Endpoint 생성
endpoint = create_inference_endpoint(
    name="my-qwen-endpoint",
    repository="Qwen/Qwen2.5-7B-Instruct",
    framework="text-generation-inference",
    task="text-generation",
    accelerator="gpu",
    instance_type="nvidia-l4",
    instance_size="x1",
    region="us-east-1",
)

# 배포 대기
endpoint.wait()

# 추론
client = endpoint.client
response = client.text_generation("Hello, how are you?", max_new_tokens=100)
print(response)

# 종료
endpoint.delete()
```

---

## 5. LM Evaluation Harness로 벤치마킹

### 5.1 설치

```bash
# 설치
git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .

# HuggingFace 모델 백엔드
pip install -e ".[hf]"

# vLLM 백엔드
pip install -e ".[vllm]"
```

### 5.2 기본 평가 실행

```bash
# HuggingFace 모델 평가
lm_eval --model hf \
    --model_args pretrained=Qwen/Qwen2.5-7B-Instruct \
    --tasks mmlu,gsm8k,hellaswag \
    --batch_size auto \
    --output_path ./results

# 특정 MMLU 서브태스크만
lm_eval --model hf \
    --model_args pretrained=./my_finetuned_model \
    --tasks mmlu_abstract_algebra,mmlu_anatomy \
    --num_fewshot 5 \
    --batch_size 8

# 양자화 모델 평가
lm_eval --model hf \
    --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,load_in_4bit=True \
    --tasks gsm8k \
    --batch_size 4
```

### 5.3 주요 벤치마크

| 벤치마크 | 설명 | 평가 방식 |
|---------|------|----------|
| MMLU | 57개 과목 지식 테스트 | 5-shot, 선택형 |
| GSM8K | 초등 수학 문제 | 8-shot, 생성형 |
| HellaSwag | 상식 추론 | 0-shot, 선택형 |
| ARC-Challenge | 과학 추론 | 25-shot, 선택형 |
| TruthfulQA | 진실성 평가 | 0-shot, 선택형 |
| HumanEval | 코드 생성 | 0-shot, 생성형 |
| GPQA | 대학원 수준 Q&A | 0-shot, 선택형 |

### 5.4 vLLM 백엔드로 빠른 평가

```bash
# vLLM으로 평가 (더 빠름)
lm_eval --model vllm \
    --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,tensor_parallel_size=1 \
    --tasks mmlu \
    --batch_size auto

# SGLang 백엔드
lm_eval --model sglang \
    --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,tp_size=1 \
    --tasks gsm8k_cot \
    --batch_size auto
```

### 5.5 로컬 서버 평가 (OpenAI API 호환)

```bash
# vLLM/TGI 서버가 실행 중일 때
lm_eval --model local-completions \
    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1/completions \
    --tasks gsm8k \
    --batch_size 16
```

### 5.6 LoRA 어댑터 직접 평가

```bash
# PEFT 어댑터 평가
lm_eval --model hf \
    --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,peft=./my_lora_adapter \
    --tasks mmlu \
    --batch_size 8
```

### 5.7 결과 분석

```python
import json

# 결과 파일 로드
with open("./results/results.json") as f:
    results = json.load(f)

# 태스크별 점수 출력
for task, metrics in results["results"].items():
    if "acc" in metrics:
        print(f"{task}: {metrics['acc']:.4f} (±{metrics.get('acc_stderr', 0):.4f})")
    elif "exact_match" in metrics:
        print(f"{task}: {metrics['exact_match']:.4f}")
```

### 5.8 커스텀 벤치마크 추가

```yaml
# my_task.yaml
task: my_custom_task
dataset_path: my_dataset  # HuggingFace 데이터셋
dataset_name: null
output_type: generate_until
training_split: train
test_split: test
doc_to_text: "Question: {{question}}\nAnswer:"
doc_to_target: "{{answer}}"
generation_kwargs:
  max_gen_toks: 100
  stop_sequences: ["\n\n"]
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
```

```bash
# 커스텀 태스크 실행
lm_eval --model hf \
    --model_args pretrained=my_model \
    --tasks my_custom_task \
    --include_path ./my_tasks/
```

---

## 6. 실습 과제

### 과제 1: End-to-End 파인튜닝 파이프라인
1. 한국어 instruction 데이터셋을 선택 (예: 한국어 Alpaca, KoAlpaca)
2. TRL messages 포맷으로 변환
3. QLoRA로 3B 모델 파인튜닝 (NEFTune 적용)
4. LoRA 어댑터 병합 후 GGUF Q4_K_M으로 변환
5. llama.cpp 서버로 배포

### 과제 2: 벤치마크 비교 분석
1. 베이스 모델과 파인튜닝 모델을 LM Evaluation Harness로 평가
2. MMLU 한국어 서브셋, GSM8K 등에서 성능 비교
3. 양자화 전/후 성능 차이 분석
4. 결과를 표와 그래프로 정리

### 과제 3: 프로덕션 배포 실습
1. vLLM 또는 TGI로 모델 서빙
2. OpenAI 호환 API로 클라이언트 구현
3. 스트리밍 응답 처리
4. 간단한 벤치마크 (처리량, 지연시간) 측정

---

## 7. 요약 체크리스트

### 데이터셋 준비
- [ ] 데이터셋 포맷 이해 (Alpaca, ShareGPT, TRL messages)
- [ ] 포맷 변환 함수 구현
- [ ] Chat template 적용 확인

### SFTTrainer 고급 기능
- [ ] Packing으로 학습 효율화
- [ ] Completion-only training 설정
- [ ] NEFTune으로 일반화 성능 향상
- [ ] LoRA 어댑터 저장 및 병합

### GGUF 변환 및 배포
- [ ] llama.cpp 빌드
- [ ] HF → GGUF 변환
- [ ] 적절한 양자화 타입 선택 (Q4_K_M 권장)
- [ ] llama-cpp-python으로 추론

### 프로덕션 서빙
- [ ] vLLM/TGI 중 선택
- [ ] OpenAI 호환 API 서버 실행
- [ ] 클라이언트 통합

### 모델 평가
- [ ] LM Evaluation Harness 설치
- [ ] 주요 벤치마크 실행 (MMLU, GSM8K)
- [ ] 베이스 모델 대비 성능 비교

---

## 다음 단계

이제 Small LLM의 파인튜닝부터 배포까지 전체 파이프라인을 마스터했습니다.

**Part 4에서는 RAG 시스템 구축**을 다룹니다:
- Sentence Transformers로 임베딩 생성
- Vector Database 연동 (FAISS, Chroma)
- Retrieval 파이프라인 구축
- RAG 성능 최적화