# Hugging Face 튜토리얼 Part 4.2: RAG Pipeline 구현

## 학습 목표
- Document Chunking 전략 이해 및 구현
- Retriever + Generator 연결 파이프라인 구축
- LangChain + Hugging Face 통합 RAG 시스템 구현
- 로컬 LLM을 활용한 완전 오픈소스 RAG 구축

---

## 1. RAG 시스템 개요

### 1.1 RAG란?

- **Retrieval-Augmented Generation**
- LLM의 한계 (Hallucination, 최신 정보 부재) 극복
- 외부 지식 베이스에서 관련 문서 검색 → LLM에 컨텍스트로 제공

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Query     │────▶│  Retriever  │────▶│  Retrieved  │
│  (질문)     │     │  (검색기)   │     │   Docs      │
└─────────────┘     └─────────────┘     └──────┬──────┘
                                               │
                                               ▼
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Answer    │◀────│  Generator  │◀────│   Prompt    │
│  (답변)     │     │   (LLM)     │     │ Query+Docs  │
└─────────────┘     └─────────────┘     └─────────────┘
```

### 1.2 RAG 파이프라인 구성 요소

| 단계 | 구성 요소 | 역할 |
|------|----------|------|
| 1. 문서 로드 | Document Loader | 다양한 형식 (PDF, TXT, HTML) 로드 |
| 2. 청킹 | Text Splitter | 문서를 작은 청크로 분할 |
| 3. 임베딩 | Embedding Model | 청크를 벡터로 변환 |
| 4. 인덱싱 | Vector Store | 벡터 저장 및 검색 |
| 5. 검색 | Retriever | 쿼리와 유사한 청크 검색 |
| 6. 생성 | LLM (Generator) | 컨텍스트 기반 답변 생성 |

---

## 2. Document Chunking 전략

### 2.1 Chunking의 중요성

- 너무 작은 청크: 문맥 손실, 의미 불완전
- 너무 큰 청크: 검색 정밀도 저하, 노이즈 증가
- **권장**: 400-512 토큰, 10-20% overlap

### 2.2 설치

```bash
pip install langchain langchain-community langchain-huggingface langchain-text-splitters
pip install sentence-transformers faiss-cpu transformers
```

### 2.3 Character Text Splitter

```python
from langchain_text_splitters import CharacterTextSplitter

text = """
Python은 인기 있는 프로그래밍 언어입니다.
배우기 쉽고 다양한 분야에서 활용됩니다.

머신러닝과 데이터 분석에 널리 사용됩니다.
TensorFlow, PyTorch 같은 프레임워크가 있습니다.

웹 개발에도 Django, Flask가 인기입니다.
"""

# 단순 문자 기반 분할
splitter = CharacterTextSplitter(
    separator="\n\n",  # 분할 기준
    chunk_size=100,     # 청크 최대 크기 (문자 수)
    chunk_overlap=20,   # 청크 간 중복
)

chunks = splitter.split_text(text)
for i, chunk in enumerate(chunks):
    print(f"[Chunk {i}] ({len(chunk)} chars): {chunk[:50]}...")
```

### 2.4 Recursive Character Text Splitter (권장)

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 계층적 분할: 문단 → 줄 → 문장 → 단어 순서로 시도
splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ".", "?", "!", " ", ""],
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
)

text = """
인공지능(AI)은 인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현한 것입니다.

머신러닝은 AI의 하위 분야로, 데이터에서 패턴을 학습합니다. 지도학습, 비지도학습, 강화학습으로 나뉩니다.

딥러닝은 머신러닝의 한 종류로, 인공신경망을 사용합니다. CNN, RNN, Transformer 등의 아키텍처가 있습니다.

자연어 처리(NLP)는 컴퓨터가 인간 언어를 이해하고 생성하는 기술입니다. 번역, 요약, 질의응답 등에 활용됩니다.
"""

chunks = splitter.split_text(text)
print(f"총 {len(chunks)}개 청크 생성\n")

for i, chunk in enumerate(chunks):
    print(f"[Chunk {i}] ({len(chunk)} chars)")
    print(chunk)
    print("-" * 50)
```

### 2.5 토큰 기반 분할

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# tiktoken 인코더 사용 (OpenAI 토크나이저)
splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base",
    chunk_size=100,  # 토큰 수 기준
    chunk_overlap=10,
)

# 또는 HuggingFace 토크나이저 사용
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-m3")

splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer,
    chunk_size=256,
    chunk_overlap=30,
)
```

### 2.6 문서 타입별 분할

```python
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,
    Language,
    MarkdownHeaderTextSplitter,
)

# Python 코드 분할
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=200,
    chunk_overlap=20,
)

python_code = """
class DataProcessor:
    def __init__(self, data):
        self.data = data
    
    def process(self):
        return [x * 2 for x in self.data]

def main():
    processor = DataProcessor([1, 2, 3])
    result = processor.process()
    print(result)
"""

chunks = python_splitter.split_text(python_code)
for chunk in chunks:
    print(chunk)
    print("---")

# Markdown 헤더 기반 분할
md_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
)
```

### 2.7 Chunking 전략 비교

| 전략 | 장점 | 단점 | 용도 |
|------|------|------|------|
| Fixed Size | 간단, 빠름 | 문맥 무시 | 단순 텍스트 |
| Recursive | 구조 보존 | 설정 필요 | **일반 권장** |
| Token-based | LLM 토큰 최적화 | 토크나이저 필요 | LLM 연동 |
| Semantic | 의미 단위 보존 | 느림, 비용 | 고품질 필요 시 |
| Document-specific | 구조 완벽 보존 | 문서별 설정 | 코드, Markdown |

### 2.8 청킹 모범 사례

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

def create_optimized_splitter(
    chunk_size: int = 500,
    overlap_ratio: float = 0.1,
) -> RecursiveCharacterTextSplitter:
    """최적화된 텍스트 분할기 생성"""
    return RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ".", "?", "!", ";", ",", " ", ""],
        chunk_size=chunk_size,
        chunk_overlap=int(chunk_size * overlap_ratio),
        length_function=len,
        is_separator_regex=False,
    )

# 사용
splitter = create_optimized_splitter(chunk_size=400, overlap_ratio=0.15)

# Document 객체로 분할 (메타데이터 유지)
documents = [
    Document(page_content="긴 텍스트...", metadata={"source": "doc1.txt"}),
]
chunks = splitter.split_documents(documents)
```

---

## 3. LangChain + Hugging Face 통합

### 3.1 langchain-huggingface 패키지

- **공식 파트너 패키지**: LangChain + Hugging Face 공동 유지보수
- **URL**: https://huggingface.co/blog/langchain

```bash
pip install langchain-huggingface
```

### 3.2 HuggingFaceEmbeddings

```python
from langchain_huggingface import HuggingFaceEmbeddings

# 임베딩 모델 초기화
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",  # 또는 한국어: "nlpai-lab/KURE-v1"
    model_kwargs={"device": "cuda"},  # GPU 사용
    encode_kwargs={"normalize_embeddings": True},  # 코사인 유사도용
)

# 단일 텍스트 임베딩
query_embedding = embeddings.embed_query("인공지능이란 무엇인가요?")
print(f"Embedding dim: {len(query_embedding)}")

# 여러 문서 임베딩
doc_embeddings = embeddings.embed_documents([
    "AI는 인간 지능을 모방합니다.",
    "머신러닝은 데이터에서 학습합니다.",
])
print(f"Number of docs: {len(doc_embeddings)}")
```

### 3.3 HuggingFacePipeline (로컬 LLM)

```python
from langchain_huggingface import HuggingFacePipeline

# 방법 1: from_model_id로 직접 로드
llm = HuggingFacePipeline.from_model_id(
    model_id="Qwen/Qwen2.5-1.5B-Instruct",
    task="text-generation",
    pipeline_kwargs={
        "max_new_tokens": 256,
        "temperature": 0.7,
        "do_sample": True,
    },
)

response = llm.invoke("인공지능의 미래에 대해 설명해주세요.")
print(response)
```

### 3.4 4-bit 양자화 LLM

```python
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch

model_id = "Qwen/Qwen2.5-7B-Instruct"

# 4-bit 양자화 설정
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# 모델 및 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
)

# Pipeline 생성
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.7,
    do_sample=True,
    return_full_text=False,
)

# LangChain에 연결
llm = HuggingFacePipeline(pipeline=pipe)
```

### 3.5 ChatHuggingFace (대화형)

```python
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline

# 기존 HuggingFacePipeline을 Chat 모델로 래핑
llm = HuggingFacePipeline.from_model_id(
    model_id="Qwen/Qwen2.5-3B-Instruct",
    task="text-generation",
    pipeline_kwargs={"max_new_tokens": 256},
)

chat_model = ChatHuggingFace(llm=llm)

# 메시지 형식으로 호출
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="당신은 친절한 AI 어시스턴트입니다."),
    HumanMessage(content="RAG란 무엇인가요?"),
]

response = chat_model.invoke(messages)
print(response.content)
```

---

## 4. RAG Pipeline 구현

### 4.1 FAISS Vector Store 설정

```python
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# 1. 임베딩 모델 초기화
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={"device": "cuda"},
    encode_kwargs={"normalize_embeddings": True},
)

# 2. 문서 준비
documents = [
    Document(page_content="RAG는 Retrieval-Augmented Generation의 약자입니다.", metadata={"source": "rag_intro.txt"}),
    Document(page_content="RAG는 LLM의 할루시네이션을 줄이는 데 효과적입니다.", metadata={"source": "rag_benefits.txt"}),
    Document(page_content="벡터 데이터베이스는 임베딩을 저장하고 검색합니다.", metadata={"source": "vectordb.txt"}),
    Document(page_content="FAISS는 Facebook에서 개발한 벡터 검색 라이브러리입니다.", metadata={"source": "faiss.txt"}),
    Document(page_content="LangChain은 LLM 애플리케이션 개발 프레임워크입니다.", metadata={"source": "langchain.txt"}),
]

# 3. FAISS 벡터 스토어 생성
vectorstore = FAISS.from_documents(
    documents,
    embeddings,
)

# 4. 검색 테스트
query = "RAG의 장점은 무엇인가요?"
results = vectorstore.similarity_search(query, k=3)

print("검색 결과:")
for doc in results:
    print(f"  - {doc.page_content[:50]}... [source: {doc.metadata['source']}]")
```

### 4.2 Retriever 설정

```python
# 기본 Retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4},
)

# 검색 실행
docs = retriever.invoke("벡터 데이터베이스란?")
for doc in docs:
    print(doc.page_content)

# MMR (Maximal Marginal Relevance) - 다양성 확보
retriever_mmr = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 4,
        "fetch_k": 10,  # 초기 후보 수
        "lambda_mult": 0.5,  # 다양성 가중치 (0: 다양성, 1: 유사도)
    },
)

# Score Threshold - 품질 필터링
retriever_threshold = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.5,
        "k": 4,
    },
)
```

### 4.3 기본 RAG Chain 구현

```python
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# 1. LLM 설정
llm = HuggingFacePipeline.from_model_id(
    model_id="Qwen/Qwen2.5-3B-Instruct",
    task="text-generation",
    pipeline_kwargs={
        "max_new_tokens": 256,
        "temperature": 0.7,
        "do_sample": True,
        "return_full_text": False,
    },
)

# 2. 프롬프트 템플릿
prompt_template = """다음 문맥을 참고하여 질문에 답변하세요.
문맥에 없는 내용은 "정보가 없습니다"라고 답하세요.

문맥:
{context}

질문: {question}

답변:"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"],
)

# 3. 문서 포맷팅 함수
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# 4. RAG Chain 구성 (LCEL 사용)
rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough(),
    }
    | prompt
    | llm
    | StrOutputParser()
)

# 5. 실행
question = "RAG란 무엇인가요?"
answer = rag_chain.invoke(question)
print(f"Q: {question}")
print(f"A: {answer}")
```

### 4.4 RetrievalQA Chain

```python
from langchain.chains import RetrievalQA

# RetrievalQA 체인 생성
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # 모든 문서를 한 번에 전달
    retriever=retriever,
    return_source_documents=True,  # 출처 문서 반환
    chain_type_kwargs={
        "prompt": prompt,
    },
)

# 실행
result = qa_chain.invoke({"query": "FAISS는 무엇인가요?"})
print(f"Answer: {result['result']}")
print(f"\nSources:")
for doc in result['source_documents']:
    print(f"  - {doc.metadata['source']}")
```

---

## 5. 완전한 RAG 시스템 구현

### 5.1 전체 파이프라인 클래스

```python
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from typing import List, Dict, Any
import os


class HuggingFaceRAG:
    """Hugging Face 기반 RAG 시스템"""
    
    def __init__(
        self,
        embedding_model: str = "BAAI/bge-m3",
        llm_model: str = "Qwen/Qwen2.5-3B-Instruct",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        device: str = "cuda",
    ):
        self.device = device
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # 임베딩 모델 초기화
        print(f"Loading embedding model: {embedding_model}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )
        
        # LLM 초기화
        print(f"Loading LLM: {llm_model}")
        self.llm = HuggingFacePipeline.from_model_id(
            model_id=llm_model,
            task="text-generation",
            device=0 if device == "cuda" else -1,
            pipeline_kwargs={
                "max_new_tokens": 512,
                "temperature": 0.7,
                "do_sample": True,
                "return_full_text": False,
            },
        )
        
        # Text Splitter 초기화
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "?", "!", " ", ""],
        )
        
        self.vectorstore = None
        self.retriever = None
        
    def load_documents(self, documents: List[Document]) -> None:
        """문서 로드 및 인덱싱"""
        print(f"Processing {len(documents)} documents...")
        
        # 청킹
        chunks = self.splitter.split_documents(documents)
        print(f"Created {len(chunks)} chunks")
        
        # 벡터 스토어 생성
        self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4},
        )
        print("Vector store created!")
    
    def load_texts(self, texts: List[str], metadatas: List[Dict] = None) -> None:
        """텍스트 직접 로드"""
        if metadatas is None:
            metadatas = [{"source": f"text_{i}"} for i in range(len(texts))]
        
        documents = [
            Document(page_content=text, metadata=meta)
            for text, meta in zip(texts, metadatas)
        ]
        self.load_documents(documents)
    
    def _format_docs(self, docs: List[Document]) -> str:
        """검색된 문서 포맷팅"""
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("source", "unknown")
            formatted.append(f"[문서 {i}] (출처: {source})\n{doc.page_content}")
        return "\n\n".join(formatted)
    
    def query(
        self,
        question: str,
        return_sources: bool = False,
    ) -> Dict[str, Any]:
        """질문에 대한 답변 생성"""
        if self.retriever is None:
            raise ValueError("먼저 load_documents() 또는 load_texts()를 호출하세요.")
        
        # 프롬프트 템플릿
        prompt_template = """아래 문맥을 참고하여 질문에 답변하세요.
문맥에 없는 내용은 추측하지 말고, 모르면 "해당 정보를 찾을 수 없습니다"라고 답하세요.

문맥:
{context}

질문: {question}

답변:"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"],
        )
        
        # 검색
        retrieved_docs = self.retriever.invoke(question)
        context = self._format_docs(retrieved_docs)
        
        # 프롬프트 생성 및 답변
        formatted_prompt = prompt.format(context=context, question=question)
        answer = self.llm.invoke(formatted_prompt)
        
        result = {
            "question": question,
            "answer": answer,
        }
        
        if return_sources:
            result["sources"] = [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                }
                for doc in retrieved_docs
            ]
        
        return result
    
    def save_index(self, path: str) -> None:
        """벡터 인덱스 저장"""
        if self.vectorstore is None:
            raise ValueError("저장할 벡터 스토어가 없습니다.")
        self.vectorstore.save_local(path)
        print(f"Index saved to {path}")
    
    def load_index(self, path: str) -> None:
        """벡터 인덱스 로드"""
        self.vectorstore = FAISS.load_local(
            path,
            self.embeddings,
            allow_dangerous_deserialization=True,
        )
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4},
        )
        print(f"Index loaded from {path}")


# 사용 예시
if __name__ == "__main__":
    # RAG 시스템 초기화
    rag = HuggingFaceRAG(
        embedding_model="BAAI/bge-m3",
        llm_model="Qwen/Qwen2.5-1.5B-Instruct",  # 가벼운 모델
        device="cuda",
    )
    
    # 샘플 문서
    sample_docs = [
        "RAG(Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다. LLM이 외부 지식 베이스를 참조하여 더 정확한 답변을 생성합니다.",
        "FAISS는 Facebook AI Research에서 개발한 벡터 유사도 검색 라이브러리입니다. 수십억 개의 벡터에서 밀리초 단위로 검색할 수 있습니다.",
        "LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다. 체인, 에이전트, 메모리 등의 기능을 제공합니다.",
        "Hugging Face는 ML 모델과 데이터셋을 공유하는 플랫폼입니다. Transformers, Datasets 등의 라이브러리를 제공합니다.",
        "임베딩 모델은 텍스트를 벡터로 변환합니다. BGE, E5, Sentence Transformers 등이 인기 있습니다.",
    ]
    
    # 문서 로드
    rag.load_texts(
        sample_docs,
        metadatas=[{"source": f"doc_{i}.txt"} for i in range(len(sample_docs))],
    )
    
    # 질문
    questions = [
        "RAG란 무엇인가요?",
        "FAISS의 특징은?",
        "임베딩 모델의 역할은?",
    ]
    
    for q in questions:
        result = rag.query(q, return_sources=True)
        print(f"\n{'='*60}")
        print(f"Q: {result['question']}")
        print(f"A: {result['answer']}")
        print(f"\n출처:")
        for src in result['sources'][:2]:
            print(f"  - {src['metadata']['source']}: {src['content'][:50]}...")
```

### 5.2 PDF 문서 로드 RAG

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# PDF 로드
loader = PyPDFLoader("document.pdf")
documents = loader.load()

print(f"Loaded {len(documents)} pages")

# 청킹
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
)
chunks = splitter.split_documents(documents)

print(f"Created {len(chunks)} chunks")

# RAG에 로드
rag.load_documents(chunks)
```

### 5.3 스트리밍 응답

```python
from langchain_core.callbacks import StreamingStdOutCallbackHandler

# 스트리밍 LLM 설정
llm_streaming = HuggingFacePipeline.from_model_id(
    model_id="Qwen/Qwen2.5-3B-Instruct",
    task="text-generation",
    pipeline_kwargs={
        "max_new_tokens": 256,
        "temperature": 0.7,
        "do_sample": True,
        "return_full_text": False,
    },
    callbacks=[StreamingStdOutCallbackHandler()],
)

# 스트리밍으로 RAG Chain 구성
rag_chain_streaming = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough(),
    }
    | prompt
    | llm_streaming
    | StrOutputParser()
)

# 스트리밍 실행
for chunk in rag_chain_streaming.stream("RAG의 장점은?"):
    print(chunk, end="", flush=True)
```

---

## 6. 벡터 스토어 관리

### 6.1 인덱스 저장 및 로드

```python
# 저장
vectorstore.save_local("./faiss_index")

# 로드
loaded_vectorstore = FAISS.load_local(
    "./faiss_index",
    embeddings,
    allow_dangerous_deserialization=True,  # 신뢰할 수 있는 소스에서만
)
```

### 6.2 문서 추가 및 삭제

```python
# 문서 추가
new_docs = [
    Document(page_content="새로운 문서입니다.", metadata={"source": "new.txt"}),
]
vectorstore.add_documents(new_docs)

# 텍스트 직접 추가
vectorstore.add_texts(
    texts=["추가 텍스트 1", "추가 텍스트 2"],
    metadatas=[{"source": "added_1"}, {"source": "added_2"}],
)

# 문서 ID로 삭제 (FAISS는 직접 삭제 미지원, 재구성 필요)
```

### 6.3 벡터 스토어 병합

```python
# 두 벡터 스토어 병합
vectorstore_1 = FAISS.from_documents(docs_1, embeddings)
vectorstore_2 = FAISS.from_documents(docs_2, embeddings)

vectorstore_1.merge_from(vectorstore_2)
```

---

## 7. 실전 예제: 한국어 문서 RAG

```python
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document


def build_korean_rag_system():
    """한국어 RAG 시스템 구축"""
    
    # 1. 한국어 임베딩 모델 (KURE-v1 또는 BGE-M3-ko)
    embeddings = HuggingFaceEmbeddings(
        model_name="nlpai-lab/KURE-v1",  # 한국어 특화
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True},
    )
    
    # 2. 한국어 LLM (Qwen이 한국어 성능 좋음)
    llm = HuggingFacePipeline.from_model_id(
        model_id="Qwen/Qwen2.5-3B-Instruct",
        task="text-generation",
        device=0,
        pipeline_kwargs={
            "max_new_tokens": 512,
            "temperature": 0.7,
            "do_sample": True,
            "return_full_text": False,
        },
    )
    
    # 3. 한국어 문서
    korean_docs = [
        Document(
            page_content="대한민국은 동아시아에 위치한 국가입니다. 수도는 서울이며, 인구는 약 5천만 명입니다.",
            metadata={"source": "korea_info.txt", "topic": "기본정보"},
        ),
        Document(
            page_content="한국의 GDP는 세계 10위권이며, IT, 자동차, 조선 등의 산업이 발달했습니다.",
            metadata={"source": "korea_economy.txt", "topic": "경제"},
        ),
        Document(
            page_content="한글은 세종대왕이 1443년에 창제한 문자입니다. 과학적이고 배우기 쉬운 문자로 평가받습니다.",
            metadata={"source": "hangul.txt", "topic": "문화"},
        ),
        Document(
            page_content="김치는 한국의 대표적인 발효 음식입니다. 배추, 고춧가루, 젓갈 등으로 만듭니다.",
            metadata={"source": "kimchi.txt", "topic": "음식"},
        ),
    ]
    
    # 4. 벡터 스토어 생성
    vectorstore = FAISS.from_documents(korean_docs, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
    
    # 5. 한국어 프롬프트
    prompt_template = """당신은 한국에 대해 잘 아는 AI 어시스턴트입니다.
아래 문맥을 참고하여 질문에 친절하게 답변하세요.

문맥:
{context}

질문: {question}

답변:"""
    
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"],
    )
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    # 6. RAG Chain
    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain


# 사용
rag = build_korean_rag_system()

questions = [
    "한국의 수도는 어디인가요?",
    "한글은 누가 만들었나요?",
    "김치는 어떻게 만드나요?",
]

for q in questions:
    print(f"\nQ: {q}")
    answer = rag.invoke(q)
    print(f"A: {answer}")
```

---

## 8. 실습 과제

### 과제 1: PDF RAG 시스템
1. PyPDFLoader로 PDF 문서 로드
2. RecursiveCharacterTextSplitter로 청킹 (400 토큰, 10% overlap)
3. KURE-v1 임베딩으로 FAISS 인덱스 생성
4. Qwen2.5-3B-Instruct로 RAG 시스템 구축
5. 다양한 질문으로 테스트

### 과제 2: Chunking 전략 비교
1. 동일 문서에 대해 3가지 청킹 전략 적용:
   - Fixed size (500자)
   - Recursive (500자, separators 설정)
   - Token-based (256 토큰)
2. 각 전략별 청크 수, 평균 길이 비교
3. 동일 질문에 대한 검색 결과 품질 비교

### 과제 3: 멀티 소스 RAG
1. PDF, TXT, Markdown 파일 로드
2. 통합 벡터 스토어 구축
3. 메타데이터 기반 필터링 검색 구현
4. 출처별 응답 생성

---

## 9. 요약 체크리스트

### Document Chunking
- [ ] RecursiveCharacterTextSplitter 사용법
- [ ] chunk_size, chunk_overlap 설정
- [ ] 문서 타입별 분할 전략

### LangChain + Hugging Face
- [ ] HuggingFaceEmbeddings 설정
- [ ] HuggingFacePipeline 로컬 LLM 사용
- [ ] ChatHuggingFace 대화형 모델

### RAG Pipeline
- [ ] FAISS 벡터 스토어 생성/저장/로드
- [ ] Retriever 설정 (similarity, MMR)
- [ ] LCEL 기반 RAG Chain 구성
- [ ] RetrievalQA Chain 사용

---

## 참고 자료

- **LangChain RAG 문서**: https://python.langchain.com/docs/tutorials/rag/
- **LangChain-HuggingFace**: https://huggingface.co/blog/langchain
- **HuggingFace Cookbook - Advanced RAG**: https://huggingface.co/learn/cookbook/en/advanced_rag
- **Chunking 전략 가이드**: https://www.pinecone.io/learn/chunking-strategies/
- **FAISS 문서**: https://faiss.ai/

---

## 다음 단계

**Part 4.3: Advanced RAG**에서 다룰 내용:
- Reranking (Cross-Encoder)
- Hybrid Search (Dense + Sparse)
- Self-RAG, Corrective RAG
- Query Transformation