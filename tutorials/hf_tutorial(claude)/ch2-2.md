# Part 2.2: Vision-Language Models (VLM) 실습

---

## 학습 목표
- VLM 아키텍처 이해 (CLIP, BLIP, LLaVA)
- Zero-shot Image Classification
- Image Captioning & Visual Question Answering
- 대화형 멀티모달 추론

---

## 1. Vision-Language Model 개요

### 1.1 VLM이란?

**Vision-Language Model (VLM)**: 이미지와 텍스트를 동시에 처리하는 멀티모달 모델

```
이미지 + 텍스트 → VLM → 텍스트 출력
```

### 1.2 주요 VLM 아키텍처 비교

| 모델 | 구조 | 학습 방식 | 주요 용도 |
|------|------|----------|----------|
| **CLIP** | Image Encoder + Text Encoder | Contrastive Learning | Zero-shot 분류, 이미지-텍스트 매칭 |
| **BLIP** | Vision Encoder + Text Encoder/Decoder | Bootstrapped Pre-training | 캡셔닝, VQA, 검색 |
| **BLIP-2** | Frozen ViT + Q-Former + Frozen LLM | Q-Former 학습 | 캡셔닝, VQA, 대화 |
| **LLaVA** | CLIP ViT + MLP Projector + LLM | Visual Instruction Tuning | 멀티모달 대화 |

### 1.3 아키텍처 패턴

```
┌──────────────────────────────────────────────────────────┐
│                      CLIP 패턴                           │
│  Image → Vision Encoder → [CLS] ↘                       │
│                                   → Similarity Score    │
│  Text  → Text Encoder   → [CLS] ↗                       │
└──────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────┐
│                   LLaVA 패턴                             │
│  Image → Vision Encoder → Projector ↘                   │
│                                       → LLM → Text      │
│  Text  → Tokenizer → Embeddings     ↗                   │
└──────────────────────────────────────────────────────────┘
```

---

## 2. CLIP - Zero-shot Image Classification

### 2.1 Pipeline 사용법

```python
from transformers import pipeline
from PIL import Image
import requests

# Zero-shot 이미지 분류 파이프라인
classifier = pipeline(
    task="zero-shot-image-classification",
    model="openai/clip-vit-large-patch14"
)

# 이미지 로드
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 후보 라벨 정의 (학습 없이 자유롭게 정의 가능!)
candidate_labels = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]

# 분류
results = classifier(image, candidate_labels=candidate_labels)

for result in results:
    print(f"{result['label']:30} | Score: {result['score']:.4f}")
```

**출력 예시:**
```
a photo of a cat               | Score: 0.9892
a photo of a bird              | Score: 0.0078
a photo of a dog               | Score: 0.0030
```

### 2.2 AutoModel로 직접 구현

```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import requests
import torch

model_name = "openai/clip-vit-base-patch32"

# 모델 & 프로세서 로드
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# 이미지 로드
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 후보 라벨
texts = ["a photo of a cat", "a photo of a dog", "a photo of a car"]

# 전처리
inputs = processor(
    text=texts,
    images=image,
    return_tensors="pt",
    padding=True
)

# 추론
with torch.no_grad():
    outputs = model(**inputs)
    
# 유사도 계산
logits_per_image = outputs.logits_per_image  # (1, num_texts)
probs = logits_per_image.softmax(dim=1)

print("=== Image-Text Similarity ===")
for text, prob in zip(texts, probs[0]):
    print(f"{text:30} | {prob.item():.4f}")
```

### 2.3 이미지-텍스트 임베딩 추출

```python
from transformers import CLIPProcessor, CLIPModel
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 이미지 임베딩
image_inputs = processor(images=image, return_tensors="pt")
with torch.no_grad():
    image_features = model.get_image_features(**image_inputs)
    
# 텍스트 임베딩
text_inputs = processor(text=["a cat", "a dog"], return_tensors="pt", padding=True)
with torch.no_grad():
    text_features = model.get_text_features(**text_inputs)

# 정규화
image_features = image_features / image_features.norm(dim=-1, keepdim=True)
text_features = text_features / text_features.norm(dim=-1, keepdim=True)

print(f"Image features shape: {image_features.shape}")  # (1, 512)
print(f"Text features shape: {text_features.shape}")    # (2, 512)

# 유사도 (코사인 유사도)
similarity = (image_features @ text_features.T)
print(f"Similarity: {similarity}")
```

### 2.4 SigLIP (개선된 CLIP)

```python
from transformers import AutoProcessor, AutoModel
import torch

# SigLIP은 CLIP보다 개선된 성능
model = AutoModel.from_pretrained("google/siglip-base-patch16-224")
processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224")

inputs = processor(
    text=["a photo of a cat", "a photo of a dog"],
    images=image,
    return_tensors="pt",
    padding=True
)

with torch.no_grad():
    outputs = model(**inputs)
    
logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image)  # SigLIP은 sigmoid 사용
print(f"Probabilities: {probs}")
```

---

## 3. BLIP - Image Captioning & VQA

### 3.1 Image Captioning

```python
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import requests
import torch

model_name = "Salesforce/blip-image-captioning-base"

processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name)

# 이미지 로드
url = "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# Unconditional Captioning (프롬프트 없이)
inputs = processor(images=image, return_tensors="pt")

with torch.no_grad():
    output_ids = model.generate(**inputs, max_length=50)
    
caption = processor.decode(output_ids[0], skip_special_tokens=True)
print(f"Caption: {caption}")
# >>> "a woman sitting on the beach with her dog"

# Conditional Captioning (프롬프트 제공)
text = "a photography of"
inputs = processor(images=image, text=text, return_tensors="pt")

with torch.no_grad():
    output_ids = model.generate(**inputs, max_length=50)

caption = processor.decode(output_ids[0], skip_special_tokens=True)
print(f"Conditional Caption: {caption}")
# >>> "a photography of a woman and her dog"
```

### 3.2 Visual Question Answering (VQA)

```python
from transformers import BlipProcessor, BlipForQuestionAnswering
from PIL import Image
import requests
import torch

model_name = "Salesforce/blip-vqa-base"

processor = BlipProcessor.from_pretrained(model_name)
model = BlipForQuestionAnswering.from_pretrained(model_name)

# 이미지 로드
url = "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# 질문
question = "How many dogs are in the picture?"

inputs = processor(images=image, text=question, return_tensors="pt")

with torch.no_grad():
    output_ids = model.generate(**inputs, max_length=20)

answer = processor.decode(output_ids[0], skip_special_tokens=True)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

### 3.3 Pipeline으로 VQA

```python
from transformers import pipeline
import torch

vqa = pipeline(
    task="visual-question-answering",
    model="Salesforce/blip-vqa-base",
    torch_dtype=torch.float16,
    device=0 if torch.cuda.is_available() else -1
)

url = "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg"

# 여러 질문
questions = [
    "What is the woman doing?",
    "What color is the dog?",
    "Where is this scene?",
]

for q in questions:
    result = vqa(question=q, image=url)
    print(f"Q: {q}")
    print(f"A: {result[0]['answer']}\n")
```

---

## 4. BLIP-2 - 대형 언어 모델 연결

### 4.1 BLIP-2 구조

```
┌─────────────────────────────────────────────────────────────┐
│                       BLIP-2                                │
│                                                             │
│  Image → [Frozen ViT] → Q-Former → [Frozen LLM] → Text     │
│                            ↑                                │
│                   Learnable Queries                         │
│                                                             │
│  * ViT: 이미지 인코딩 (frozen)                              │
│  * Q-Former: 비전-언어 정렬 (trainable)                     │
│  * LLM: 텍스트 생성 (frozen, OPT/Flan-T5)                  │
└─────────────────────────────────────────────────────────────┘
```

### 4.2 BLIP-2 추론

```python
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import requests
import torch

model_name = "Salesforce/blip2-opt-2.7b"

processor = Blip2Processor.from_pretrained(model_name)
model = Blip2ForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 이미지 로드
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# Image Captioning
inputs = processor(images=image, return_tensors="pt").to(model.device, torch.float16)

with torch.no_grad():
    output_ids = model.generate(**inputs, max_new_tokens=50)

caption = processor.decode(output_ids[0], skip_special_tokens=True)
print(f"Caption: {caption}")
```

### 4.3 BLIP-2 VQA

```python
# Visual Question Answering
question = "Question: How many cats are there? Answer:"

inputs = processor(
    images=image, 
    text=question, 
    return_tensors="pt"
).to(model.device, torch.float16)

with torch.no_grad():
    output_ids = model.generate(**inputs, max_new_tokens=20)

answer = processor.decode(output_ids[0], skip_special_tokens=True)
print(f"Q: How many cats are there?")
print(f"A: {answer}")
```

### 4.4 BLIP-2 대화형 프롬프트

```python
# 대화형 프롬프트
prompt = """
Question: What is this image about?
Answer: This image shows two cats lying on a couch.

Question: What are they doing?
Answer: They appear to be sleeping or resting.

Question: What color are the cats?
Answer:"""

inputs = processor(
    images=image,
    text=prompt,
    return_tensors="pt"
).to(model.device, torch.float16)

with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=30,
        do_sample=True,
        temperature=0.7
    )

response = processor.decode(output_ids[0], skip_special_tokens=True)
print(response)
```

---

## 5. LLaVA - 대화형 멀티모달 AI

### 5.1 LLaVA 구조

```
┌─────────────────────────────────────────────────────────────┐
│                        LLaVA                                │
│                                                             │
│  Image → CLIP ViT → MLP Projector → ┐                      │
│                                      ├→ LLM → Response     │
│  Text  → Tokenizer → Embeddings    → ┘                     │
│                                                             │
│  * Vision Encoder: CLIP ViT-L/14 (frozen)                  │
│  * Projector: 2-layer MLP (trainable)                      │
│  * LLM: Vicuna/LLaMA (trainable)                           │
└─────────────────────────────────────────────────────────────┘
```

### 5.2 LLaVA 1.5 추론

```python
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image
import requests
import torch

model_name = "llava-hf/llava-1.5-7b-hf"

processor = AutoProcessor.from_pretrained(model_name)
model = LlavaForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 이미지 로드
url = "https://www.ilankelman.org/stopsigns/australia.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 대화 형식 구성
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": url},
            {"type": "text", "text": "What is shown in this image?"}
        ]
    }
]

# Chat template 적용
inputs = processor.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device, torch.float16)

# 생성
with torch.no_grad():
    output_ids = model.generate(**inputs, max_new_tokens=100)

response = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
print(response)
```

### 5.3 LLaVA-NeXT (개선 버전)

```python
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
from PIL import Image
import requests
import torch

model_name = "llava-hf/llava-v1.6-mistral-7b-hf"

processor = LlavaNextProcessor.from_pretrained(model_name)
model = LlavaNextForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 이미지 로드
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 대화 형식
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Describe this diagram in detail."}
        ]
    }
]

# 프롬프트 생성
prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

# 입력 처리
inputs = processor(images=image, text=prompt, return_tensors="pt").to(model.device)

# 생성
with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7
    )

response = processor.decode(output_ids[0], skip_special_tokens=True)
print(response)
```

### 5.4 LLaVA Pipeline (간단 사용)

```python
from transformers import pipeline

pipe = pipeline(
    task="image-text-to-text",
    model="llava-hf/llama3-llava-next-8b-hf"
)

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "https://example.com/image.jpg"},
            {"type": "text", "text": "What is in this image?"}
        ]
    }
]

output = pipe(text=messages, max_new_tokens=100)
print(output[0]["generated_text"])
```

### 5.5 멀티 이미지 추론

```python
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
from PIL import Image
import requests
import torch

processor = LlavaNextProcessor.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf")
model = LlavaNextForConditionalGeneration.from_pretrained(
    "llava-hf/llava-v1.6-mistral-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 여러 이미지 로드
url1 = "https://example.com/image1.jpg"
url2 = "https://example.com/image2.jpg"
image1 = Image.open(requests.get(url1, stream=True).raw)
image2 = Image.open(requests.get(url2, stream=True).raw)

# 멀티 이미지 대화
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "image"},
            {"type": "text", "text": "Compare these two images and describe the differences."}
        ]
    }
]

prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
inputs = processor(images=[image1, image2], text=prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    output_ids = model.generate(**inputs, max_new_tokens=300)

response = processor.decode(output_ids[0], skip_special_tokens=True)
print(response)
```

---

## 6. 4-bit 양자화로 메모리 절약

```python
from transformers import (
    AutoProcessor,
    AutoModelForImageTextToText,
    BitsAndBytesConfig
)
import torch

# 4-bit 양자화 설정
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)

model_name = "llava-hf/llava-v1.6-mistral-7b-hf"

processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForImageTextToText.from_pretrained(
    model_name,
    quantization_config=quant_config,
    device_map="auto"
)

# 메모리 사용량 확인
print(f"Model memory: {model.get_memory_footprint() / 1e9:.2f} GB")
```

---

## 7. 성능 비교

### 7.1 모델 선택 가이드

| 모델 | 파라미터 | 메모리 | 속도 | 정확도 | 주요 용도 |
|------|----------|--------|------|--------|----------|
| CLIP ViT-B/32 | 150M | ~600MB | 빠름 | 중 | Zero-shot 분류 |
| CLIP ViT-L/14 | 430M | ~1.7GB | 중간 | 높음 | 이미지 검색, 분류 |
| BLIP-base | 380M | ~1.5GB | 빠름 | 중 | 캡셔닝, VQA |
| BLIP-2 OPT-2.7B | 3.8B | ~8GB | 느림 | 높음 | 대화형 VQA |
| LLaVA-1.5-7B | 7B | ~14GB | 느림 | 매우 높음 | 멀티모달 대화 |
| LLaVA-NeXT-8B | 8B | ~16GB | 느림 | 최고 | 복잡한 추론 |

### 7.2 Task별 추천 모델

| Task | 추천 모델 | 이유 |
|------|----------|------|
| Zero-shot 분류 | CLIP, SigLIP | 빠름, 유연한 라벨 |
| 간단한 캡셔닝 | BLIP-base | 가벼움, 빠름 |
| 상세 캡셔닝 | BLIP-2, LLaVA | 더 자세한 설명 |
| VQA (단답형) | BLIP-VQA | 최적화된 구조 |
| VQA (대화형) | LLaVA-NeXT | 자연스러운 대화 |
| 이미지 검색 | CLIP | 임베딩 기반 검색 |
| 문서 OCR | LLaVA-NeXT | OCR 강화 학습 |

---

## 8. 실습 과제

### 과제 1: 이미지 검색 시스템
```python
"""
목표: CLIP을 이용한 이미지 검색 시스템 구축
요구사항:
1. 이미지 데이터셋 임베딩 생성
2. 텍스트 쿼리로 유사 이미지 검색
3. Top-K 결과 반환
"""

class CLIPImageSearch:
    def __init__(self, model_name="openai/clip-vit-base-patch32"):
        # TODO: 구현하기
        pass
    
    def index_images(self, images: list):
        """이미지 임베딩 인덱싱"""
        pass
    
    def search(self, query: str, top_k: int = 5):
        """텍스트 쿼리로 이미지 검색"""
        pass
```

### 과제 2: 멀티모달 챗봇
```python
"""
목표: LLaVA 기반 대화형 이미지 분석 챗봇
요구사항:
1. 이미지 업로드
2. 멀티턴 대화 지원
3. 대화 히스토리 유지
"""

class MultimodalChatbot:
    def __init__(self, model_name="llava-hf/llava-1.5-7b-hf"):
        # TODO: 구현하기
        pass
    
    def set_image(self, image_path: str):
        """분석할 이미지 설정"""
        pass
    
    def chat(self, message: str) -> str:
        """대화 턴 처리"""
        pass
    
    def clear_history(self):
        """대화 히스토리 초기화"""
        pass
```

---

## 9. 참고 링크

### 공식 문서
- [CLIP Documentation](https://huggingface.co/docs/transformers/model_doc/clip)
- [BLIP Documentation](https://huggingface.co/docs/transformers/model_doc/blip)
- [BLIP-2 Documentation](https://huggingface.co/docs/transformers/model_doc/blip-2)
- [LLaVA Documentation](https://huggingface.co/docs/transformers/model_doc/llava)
- [LLaVA-NeXT Documentation](https://huggingface.co/docs/transformers/model_doc/llava_next)

### HuggingFace Blog
- [Vision Language Models Explained](https://huggingface.co/blog/vlms)
- [Zero-shot Image-to-Text with BLIP-2](https://huggingface.co/blog/blip-2)

### 모델 컬렉션
- [BLIP Collection](https://huggingface.co/collections/Salesforce/blip-models-65237fbc10e469e59e01d1e0)
- [LLaVA Collection](https://huggingface.co/collections/llava-hf/llava-15-65f762d5b6941db5c2ba07e0)
- [LLaVA-NeXT Collection](https://huggingface.co/collections/llava-hf/llava-next-65f75c4afac77fd37dbbe6cf)

### 평가 벤치마크
- [Open VLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard)
- [Vision Arena](https://huggingface.co/spaces/WildVision/vision-arena)

---

## 10. 핵심 요약

| Task | Pipeline | AutoModel |
|------|----------|-----------|
| Zero-shot 분류 | `zero-shot-image-classification` | `CLIPModel` |
| VQA | `visual-question-answering` | `BlipForQuestionAnswering` |
| 캡셔닝 | `image-to-text` | `BlipForConditionalGeneration` |
| 멀티모달 대화 | `image-text-to-text` | `LlavaForConditionalGeneration` |

### VLM 선택 플로우차트

```
시작
  │
  ├─ Zero-shot 분류? → CLIP / SigLIP
  │
  ├─ 단순 캡셔닝/VQA? → BLIP
  │
  ├─ 고품질 캡셔닝? → BLIP-2
  │
  └─ 대화형 멀티모달?
       │
       ├─ 메모리 제한? → LLaVA-1.5 (7B) + 4bit
       │
       └─ 최고 성능? → LLaVA-NeXT
```