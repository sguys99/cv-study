# Hugging Face 튜토리얼 Part 4.1: Embedding & Vector Search

## 학습 목표
- Sentence Transformers로 텍스트 임베딩 생성
- MTEB 리더보드 이해 및 모델 선택 기준 학습
- 한국어 임베딩 모델 선택 가이드
- HuggingFace `datasets` FAISS 인덱싱으로 벡터 검색 구현

---

## 1. 텍스트 임베딩 기초

### 1.1 임베딩이란?

- **정의**: 텍스트를 고정 길이의 벡터(숫자 배열)로 변환
- **목적**: 의미적으로 유사한 텍스트 → 가까운 벡터
- **활용**: 시맨틱 검색, 클러스터링, 분류, RAG 등

```
"오늘 날씨가 좋다" → [0.12, -0.34, 0.56, ..., 0.78]  (384차원)
"날씨가 화창하다"  → [0.11, -0.33, 0.55, ..., 0.77]  (유사한 벡터)
"주식이 올랐다"    → [-0.45, 0.22, -0.11, ..., 0.33] (다른 벡터)
```

### 1.2 Sentence Transformers 소개

- **공식 사이트**: https://www.sbert.net/
- **HuggingFace**: https://huggingface.co/sentence-transformers
- **특징**:
  - BERT 기반 문장/문단 임베딩 생성
  - 15,000+ 사전학습 모델 제공
  - 다국어 지원 (100+ 언어)
  - 빠른 추론 속도

---

## 2. Sentence Transformers 실습

### 2.1 설치

```bash
pip install sentence-transformers
```

### 2.2 기본 사용법

```python
from sentence_transformers import SentenceTransformer

# 모델 로드
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# 문장 리스트
sentences = [
    "오늘 날씨가 정말 좋습니다.",
    "날씨가 화창하고 맑네요.",
    "주식 시장이 상승했습니다.",
]

# 임베딩 생성
embeddings = model.encode(sentences)

print(f"Shape: {embeddings.shape}")  # (3, 384)
print(f"Type: {type(embeddings)}")   # numpy.ndarray
```

### 2.3 유사도 계산

```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# 쿼리와 문서
query = "인공지능의 미래는?"
documents = [
    "AI 기술은 빠르게 발전하고 있습니다.",
    "오늘 점심은 파스타를 먹었습니다.",
    "머신러닝이 산업을 변화시키고 있습니다.",
]

# 임베딩 생성
query_embedding = model.encode(query)
doc_embeddings = model.encode(documents)

# 코사인 유사도 계산
similarities = util.cos_sim(query_embedding, doc_embeddings)

print("유사도 점수:")
for i, doc in enumerate(documents):
    print(f"  [{similarities[0][i]:.4f}] {doc}")

# 출력 예시:
# 유사도 점수:
#   [0.4521] AI 기술은 빠르게 발전하고 있습니다.
#   [0.0234] 오늘 점심은 파스타를 먹었습니다.
#   [0.3892] 머신러닝이 산업을 변화시키고 있습니다.
```

### 2.4 배치 인코딩 (대용량 처리)

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# 대용량 문서 리스트
documents = ["문서 " + str(i) for i in range(10000)]

# 배치 인코딩 (GPU 활용, 진행률 표시)
embeddings = model.encode(
    documents,
    batch_size=64,
    show_progress_bar=True,
    convert_to_numpy=True,  # numpy 배열 반환 (기본값)
    normalize_embeddings=True,  # L2 정규화 (코사인 유사도용)
)

print(f"Shape: {embeddings.shape}")  # (10000, 384)
```

### 2.5 GPU 사용

```python
from sentence_transformers import SentenceTransformer

# GPU 지정
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", device="cuda")

# 또는 자동 감지
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
print(f"Device: {model.device}")  # cuda:0 또는 cpu
```

---

## 3. 임베딩 모델 선택 가이드

### 3.1 MTEB 리더보드

- **MTEB**: Massive Text Embedding Benchmark
- **URL**: https://huggingface.co/spaces/mteb/leaderboard
- **평가 태스크 (8가지)**:
  - Retrieval (검색)
  - Classification (분류)
  - Clustering (클러스터링)
  - Semantic Textual Similarity (STS)
  - Pair Classification
  - Reranking
  - Summarization
  - Bitext Mining

### 3.2 주요 영어 임베딩 모델 (2024-2025)

| 모델 | 차원 | 파라미터 | MTEB 점수 | 특징 |
|------|------|----------|-----------|------|
| `NV-Embed-v2` | 4096 | 7.9B | ~72 | NVIDIA, SOTA |
| `gte-Qwen2-7B-instruct` | 3584 | 7B | ~70 | Alibaba, 다국어 |
| `BAAI/bge-large-en-v1.5` | 1024 | 335M | ~64 | 범용, 가성비 |
| `sentence-transformers/all-MiniLM-L6-v2` | 384 | 22M | ~56 | 경량, 빠름 |
| `intfloat/e5-large-v2` | 1024 | 335M | ~62 | Microsoft, 범용 |
| `BAAI/bge-m3` | 1024 | 568M | ~68 | 다국어, 8K 컨텍스트 |

### 3.3 한국어 임베딩 모델 선택

| 모델 | 기반 | 차원 | 특징 |
|------|------|------|------|
| `BAAI/bge-m3` | XLM-RoBERTa | 1024 | 100+ 언어, 8K 토큰, Dense/Sparse/Multi-vector |
| `dragonkue/BGE-m3-ko` | bge-m3 | 1024 | 한국어 파인튜닝, 장문에 강함 |
| `nlpai-lab/KURE-v1` | bge-m3 | 1024 | 고려대, 한국어 검색 특화 |
| `nlpai-lab/KoE5` | multilingual-e5-large | 1024 | 한국어 파인튜닝, prefix 필요 |
| `dragonkue/multilingual-e5-small-ko` | e5-small | 384 | 경량, 데모용 |
| `intfloat/multilingual-e5-large` | XLM-RoBERTa | 1024 | 범용 다국어 |

### 3.4 모델 선택 기준

```
선택 흐름도:

1. 언어는?
   ├─ 영어만 → bge-large-en-v1.5, e5-large-v2
   ├─ 한국어 중심 → KURE-v1, BGE-m3-ko, KoE5
   └─ 다국어 → bge-m3, multilingual-e5-large

2. 리소스는?
   ├─ GPU 충분 → 7B+ 모델 (NV-Embed, gte-Qwen2)
   ├─ GPU 제한 → 300M급 (bge-large, e5-large)
   └─ CPU만 → 22M급 (all-MiniLM-L6-v2)

3. 문서 길이는?
   ├─ 짧은 문장 (< 512 토큰) → 대부분 모델 가능
   └─ 긴 문서 (> 512 토큰) → bge-m3 (8K), BGE-m3-ko

4. 용도는?
   ├─ 검색/RAG → Retrieval 점수 높은 모델
   ├─ 분류 → Classification 점수 높은 모델
   └─ 범용 → 전체 평균 높은 모델
```

---

## 4. 한국어 임베딩 모델 실습

### 4.1 BGE-M3 (다국어, 범용)

```python
from sentence_transformers import SentenceTransformer

# BGE-M3 로드
model = SentenceTransformer("BAAI/bge-m3")

# 한국어 문장
sentences = [
    "오늘 서울의 날씨는 맑고 화창합니다.",
    "서울 하늘이 맑아서 기분이 좋네요.",
    "삼성전자 주가가 오늘 3% 상승했습니다.",
]

embeddings = model.encode(sentences, normalize_embeddings=True)
print(f"Shape: {embeddings.shape}")  # (3, 1024)

# 유사도 확인
from sentence_transformers import util
similarities = util.cos_sim(embeddings, embeddings)
print(similarities)
```

### 4.2 KURE-v1 (한국어 검색 특화)

```python
from sentence_transformers import SentenceTransformer

# KURE-v1 로드 (고려대 NLP 연구실)
model = SentenceTransformer("nlpai-lab/KURE-v1")

query = "헌법과 법원조직법의 관계는?"
passages = [
    "우리 헌법과 법원조직법은 대법원 구성을 다양화하여 기본권 보장에 기여한다.",
    "오늘 점심 메뉴는 김치찌개입니다.",
    "대한민국 헌법은 1948년에 제정되었습니다.",
]

query_emb = model.encode(query, normalize_embeddings=True)
passage_embs = model.encode(passages, normalize_embeddings=True)

# 유사도 계산
from sentence_transformers import util
scores = util.cos_sim(query_emb, passage_embs)[0]

for i, (score, passage) in enumerate(zip(scores, passages)):
    print(f"[{score:.4f}] {passage[:50]}...")
```

### 4.3 KoE5 (Prefix 필요)

```python
from sentence_transformers import SentenceTransformer

# KoE5 로드
model = SentenceTransformer("nlpai-lab/KoE5")

# ⚠️ KoE5는 prefix가 필요함
# - 쿼리: "query: {텍스트}"
# - 문서: "passage: {텍스트}"

query = "query: 한국의 수도는 어디인가요?"
passages = [
    "passage: 서울은 대한민국의 수도이며 정치, 경제의 중심지입니다.",
    "passage: 부산은 한국 제2의 도시로 항구 도시입니다.",
]

query_emb = model.encode(query, normalize_embeddings=True)
passage_embs = model.encode(passages, normalize_embeddings=True)

from sentence_transformers import util
scores = util.cos_sim(query_emb, passage_embs)[0]
print(scores)  # tensor([0.8xxx, 0.5xxx])
```

### 4.4 성능 비교 실험

```python
from sentence_transformers import SentenceTransformer, util
import time

# 테스트할 모델 목록
models_to_test = [
    "BAAI/bge-m3",
    "nlpai-lab/KURE-v1",
    "intfloat/multilingual-e5-large",
]

# 테스트 데이터
query = "인공지능이 의료 분야에 미치는 영향"
documents = [
    "AI 기술은 의료 진단의 정확도를 높이고 있습니다.",
    "딥러닝은 의료 영상 분석에 혁신을 가져왔습니다.",
    "오늘 날씨가 좋아서 산책을 했습니다.",
    "한국의 의료 시스템은 세계적으로 우수합니다.",
]

for model_name in models_to_test:
    print(f"\n{'='*50}")
    print(f"Model: {model_name}")
    
    # 모델 로드
    start = time.time()
    model = SentenceTransformer(model_name)
    load_time = time.time() - start
    print(f"Load time: {load_time:.2f}s")
    
    # 인코딩
    start = time.time()
    query_emb = model.encode(query, normalize_embeddings=True)
    doc_embs = model.encode(documents, normalize_embeddings=True)
    encode_time = time.time() - start
    print(f"Encode time: {encode_time:.4f}s")
    
    # 유사도
    scores = util.cos_sim(query_emb, doc_embs)[0]
    print(f"Dimension: {query_emb.shape[0]}")
    print("Scores:")
    for score, doc in zip(scores, documents):
        print(f"  [{score:.4f}] {doc[:40]}...")
```

---

## 5. HuggingFace Datasets FAISS 인덱싱

### 5.1 FAISS란?

- **Facebook AI Similarity Search**
- 대규모 벡터 유사도 검색 라이브러리
- 수십억 개 벡터에서 밀리초 단위 검색
- HuggingFace `datasets`와 통합 지원

### 5.2 설치

```bash
pip install datasets faiss-cpu
# GPU 사용 시: pip install faiss-gpu
```

### 5.3 기본 워크플로우

```python
from datasets import Dataset
from sentence_transformers import SentenceTransformer

# 1. 데이터 준비
documents = [
    {"id": 1, "text": "Python은 인기 있는 프로그래밍 언어입니다."},
    {"id": 2, "text": "JavaScript는 웹 개발에 필수적입니다."},
    {"id": 3, "text": "머신러닝은 AI의 핵심 분야입니다."},
    {"id": 4, "text": "딥러닝은 신경망을 사용합니다."},
    {"id": 5, "text": "자연어 처리는 텍스트를 분석합니다."},
]

dataset = Dataset.from_list(documents)
print(dataset)
# Dataset({
#     features: ['id', 'text'],
#     num_rows: 5
# })
```

### 5.4 임베딩 생성 및 추가

```python
from sentence_transformers import SentenceTransformer

# 모델 로드
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# 임베딩 생성 함수
def add_embeddings(batch):
    embeddings = model.encode(batch["text"])
    return {"embeddings": embeddings}

# 임베딩 추가 (배치 처리)
dataset = dataset.map(add_embeddings, batched=True, batch_size=32)

print(dataset)
# Dataset({
#     features: ['id', 'text', 'embeddings'],
#     num_rows: 5
# })
print(f"Embedding shape: {len(dataset[0]['embeddings'])}")  # 384
```

### 5.5 FAISS 인덱스 생성 및 검색

```python
# FAISS 인덱스 추가
dataset.add_faiss_index(column="embeddings")

# 쿼리 임베딩 생성
query = "AI와 인공지능에 대해 알려주세요"
query_embedding = model.encode(query)

# 검색 (k=3: 상위 3개)
scores, retrieved = dataset.get_nearest_examples(
    "embeddings", 
    query_embedding, 
    k=3
)

print("검색 결과:")
for score, text in zip(scores, retrieved["text"]):
    print(f"  [{score:.4f}] {text}")

# 출력 예시:
# 검색 결과:
#   [0.8234] 머신러닝은 AI의 핵심 분야입니다.
#   [0.7123] 딥러닝은 신경망을 사용합니다.
#   [0.6543] 자연어 처리는 텍스트를 분석합니다.
```

### 5.6 인덱스 저장 및 로드

```python
# 인덱스 저장
dataset.save_faiss_index("embeddings", "my_index.faiss")

# 나중에 로드
from datasets import load_dataset

# 데이터셋 로드 (임베딩 포함)
dataset = load_dataset("my_dataset", split="train")

# FAISS 인덱스 로드
dataset.load_faiss_index("embeddings", "my_index.faiss")

# 검색 가능
scores, retrieved = dataset.get_nearest_examples("embeddings", query_embedding, k=3)
```

### 5.7 대용량 데이터셋 처리 예제

```python
from datasets import load_dataset, Dataset
from sentence_transformers import SentenceTransformer
import numpy as np

# 1. 실제 데이터셋 로드 (예: Wikipedia)
# dataset = load_dataset("wikipedia", "20220301.simple", split="train[:10000]")

# 예시용 더미 데이터
texts = [f"문서 {i}: 이것은 테스트 문서입니다. 내용은 {i}번째입니다." for i in range(1000)]
dataset = Dataset.from_dict({"text": texts, "doc_id": list(range(1000))})

# 2. 모델 로드
model = SentenceTransformer("BAAI/bge-m3")

# 3. 배치 임베딩 생성
def embed_batch(batch):
    embeddings = model.encode(
        batch["text"],
        batch_size=32,
        show_progress_bar=False,
        normalize_embeddings=True,
    )
    return {"embeddings": [emb.tolist() for emb in embeddings]}

# GPU 활용, 멀티프로세싱
dataset = dataset.map(
    embed_batch,
    batched=True,
    batch_size=100,
)

# 4. FAISS 인덱스 생성
dataset.add_faiss_index(column="embeddings")

# 5. 검색 테스트
query = "테스트 문서 500번"
query_emb = model.encode(query, normalize_embeddings=True)

scores, results = dataset.get_nearest_examples("embeddings", query_emb, k=5)

print("Top 5 결과:")
for score, doc_id, text in zip(scores, results["doc_id"], results["text"]):
    print(f"  [Score: {score:.4f}] Doc {doc_id}: {text[:50]}...")
```

### 5.8 외부 numpy 배열로 인덱스 생성

```python
import numpy as np
from datasets import Dataset

# 이미 임베딩이 numpy 배열로 있는 경우
texts = ["문서1", "문서2", "문서3"]
embeddings = np.random.rand(3, 384).astype(np.float32)  # 예시

dataset = Dataset.from_dict({"text": texts})

# 외부 배열로 FAISS 인덱스 추가
dataset.add_faiss_index_from_external_arrays(
    external_arrays=embeddings,
    index_name="embeddings",
)

# 검색
query_emb = np.random.rand(384).astype(np.float32)
scores, results = dataset.get_nearest_examples("embeddings", query_emb, k=2)
```

---

## 6. 고급 FAISS 설정

### 6.1 커스텀 FAISS 인덱스

```python
import faiss
from datasets import Dataset

# 데이터 준비
dataset = Dataset.from_dict({
    "text": ["doc1", "doc2", "doc3"],
    "embeddings": [[0.1]*384, [0.2]*384, [0.3]*384],
})

# 커스텀 FAISS 인덱스 (IVF: 대용량용)
dimension = 384
nlist = 100  # 클러스터 수

# IVF 인덱스 생성
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist)

# 커스텀 인덱스로 추가
dataset.add_faiss_index(
    column="embeddings",
    custom_index=index,
    train_size=len(dataset),  # IVF는 학습 필요
)
```

### 6.2 인덱스 타입별 특징

| 인덱스 타입 | 용도 | 검색 속도 | 정확도 | 메모리 |
|------------|------|----------|--------|--------|
| `IndexFlatL2` | 소규모 (< 100K) | 느림 | 100% | 높음 |
| `IndexFlatIP` | 코사인 유사도 | 느림 | 100% | 높음 |
| `IndexIVFFlat` | 중규모 (100K-1M) | 빠름 | 95%+ | 중간 |
| `IndexHNSWFlat` | 대규모 | 매우 빠름 | 95%+ | 높음 |
| `IndexIVFPQ` | 초대규모 (1B+) | 빠름 | 90%+ | 낮음 |

---

## 7. 실전 예제: 시맨틱 검색 시스템

### 7.1 전체 파이프라인

```python
from datasets import Dataset, load_dataset
from sentence_transformers import SentenceTransformer, util
import json

class SemanticSearchEngine:
    def __init__(self, model_name="BAAI/bge-m3"):
        self.model = SentenceTransformer(model_name)
        self.dataset = None
    
    def index_documents(self, documents: list[dict], text_field: str = "text"):
        """문서 인덱싱"""
        # Dataset 생성
        self.dataset = Dataset.from_list(documents)
        self.text_field = text_field
        
        # 임베딩 생성
        def add_embeddings(batch):
            embeddings = self.model.encode(
                batch[text_field],
                normalize_embeddings=True,
                show_progress_bar=True,
            )
            return {"embeddings": embeddings}
        
        self.dataset = self.dataset.map(add_embeddings, batched=True, batch_size=32)
        
        # FAISS 인덱스 생성
        self.dataset.add_faiss_index(column="embeddings")
        
        print(f"Indexed {len(self.dataset)} documents")
    
    def search(self, query: str, k: int = 5) -> list[dict]:
        """검색 수행"""
        query_embedding = self.model.encode(query, normalize_embeddings=True)
        scores, results = self.dataset.get_nearest_examples("embeddings", query_embedding, k=k)
        
        # 결과 포맷팅
        search_results = []
        for i in range(len(scores)):
            result = {col: results[col][i] for col in results.keys() if col != "embeddings"}
            result["score"] = float(scores[i])
            search_results.append(result)
        
        return search_results
    
    def save(self, path: str):
        """인덱스 저장"""
        self.dataset.save_faiss_index("embeddings", f"{path}/index.faiss")
        self.dataset.save_to_disk(f"{path}/dataset")
    
    def load(self, path: str):
        """인덱스 로드"""
        from datasets import load_from_disk
        self.dataset = load_from_disk(f"{path}/dataset")
        self.dataset.load_faiss_index("embeddings", f"{path}/index.faiss")


# 사용 예시
if __name__ == "__main__":
    # 문서 준비
    documents = [
        {"id": 1, "title": "Python 기초", "text": "Python은 배우기 쉬운 프로그래밍 언어입니다."},
        {"id": 2, "title": "JavaScript 입문", "text": "JavaScript는 웹 개발의 핵심 언어입니다."},
        {"id": 3, "title": "머신러닝 개요", "text": "머신러닝은 데이터에서 패턴을 학습합니다."},
        {"id": 4, "title": "딥러닝 소개", "text": "딥러닝은 인공 신경망을 사용한 기계 학습입니다."},
        {"id": 5, "title": "자연어 처리", "text": "NLP는 컴퓨터가 인간의 언어를 이해하게 합니다."},
    ]
    
    # 검색 엔진 초기화 및 인덱싱
    engine = SemanticSearchEngine(model_name="sentence-transformers/all-MiniLM-L6-v2")
    engine.index_documents(documents)
    
    # 검색
    results = engine.search("AI와 인공지능", k=3)
    
    print("\n검색 결과:")
    for r in results:
        print(f"  [{r['score']:.4f}] {r['title']}: {r['text'][:50]}...")
```

---

## 8. 실습 과제

### 과제 1: 한국어 임베딩 모델 비교
1. BGE-M3, KURE-v1, KoE5 세 모델 로드
2. 동일한 한국어 쿼리-문서 세트로 검색 수행
3. 각 모델의 검색 결과 순위 비교
4. 인코딩 속도 측정

### 과제 2: 뉴스 기사 검색 시스템
1. 한국어 뉴스 데이터셋 로드 (예: `klue`, `kor_nli`)
2. 1000개 이상의 기사에 임베딩 생성
3. FAISS 인덱스 구축
4. 쿼리로 관련 기사 검색
5. 검색 결과를 파일로 저장

### 과제 3: 임베딩 시각화
1. 다양한 주제의 문장 50개 준비
2. 임베딩 생성
3. t-SNE 또는 UMAP으로 2D 시각화
4. 주제별 클러스터 형성 확인

---

## 9. 요약 체크리스트

### Sentence Transformers
- [ ] 기본 사용법 (encode, similarity)
- [ ] 배치 인코딩 옵션
- [ ] GPU 활용

### 모델 선택
- [ ] MTEB 리더보드 이해
- [ ] 용도별 모델 선택 기준
- [ ] 한국어 모델 옵션 파악

### FAISS 인덱싱
- [ ] Dataset에 임베딩 추가
- [ ] add_faiss_index() 사용
- [ ] get_nearest_examples()로 검색
- [ ] 인덱스 저장/로드

---

## 참고 자료

- **Sentence Transformers 문서**: https://www.sbert.net/
- **MTEB 리더보드**: https://huggingface.co/spaces/mteb/leaderboard
- **HuggingFace Datasets FAISS**: https://huggingface.co/docs/datasets/faiss_es
- **FAISS 공식 Wiki**: https://github.com/facebookresearch/faiss/wiki
- **한국어 임베딩 모델**:
  - KURE: https://github.com/nlpai-lab/KURE
  - BGE-M3: https://huggingface.co/BAAI/bge-m3
  - KoE5: https://huggingface.co/nlpai-lab/KoE5

---

## 다음 단계

**Part 4.2: RAG Pipeline 구현**에서 다룰 내용:
- Document Chunking 전략
- Retriever + Generator 연결
- LangChain + Hugging Face 통합